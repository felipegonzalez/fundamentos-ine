# Análisis exploratorio {#analisis-1}

> "Exploratory data analysis can never be the whole story, but nothing
else can serve as the foundation stone --as the first step." --- John Tukey

```{r, message = FALSE, echo = FALSE, include = FALSE}
ggplot2::theme_set(ggplot2::theme_light())
```

Muchas veces se le llama **análisis exploratorio** a una combinación
de resúmenes, gráficas y tablas cuyos propósitos pueden englobarse en:

1. **Validación de datos**. Verificar si en los datos 
existen o no valores imposibles o "extraños" para algunas mediciones,
si existen datos faltantes, o existen otros posibles "defectos", 
irregularidades o sorpresas.

2. **Resúmenes compactos**. Mostrar algunas resúmenes útiles y compactos 
de las mediciones
que tenemos: cuáles son valores o rangos de valores comunes, cuáles son los
extremos de las mediciones y así sucesivamente.

3. **Patrones**. Buscamos gráficas o resúmenes que muestren patrones 
más importantes y sustanciales,
como la relación entre distintas partes de los datos, qué partes de la variación en 
los datos son tentativamente atribuibles a ciertos factores y qué partes no.


Esta fase del análisis de datos es fundamental, como
la cita de Tukey explica arriba, y se caracteríza por un *enfoque de detective*:
quizá tenemos algunas preguntas, algunas sospechas, y en esta fase acumulamos indicios
que nos indiquen caminos prometedores de investigación.

En contraste, tenemos el *análisis confirmatorio*, que busca validar hipótesis
o dar respuestas correctamente cuantificadas en cuanto a su incertidumbre o grado 
de error. En esta parte somos más *jueces* que detectives, y utilizamos más maquinaria
matemática (teoría de probabilidad) para especificar con claridad nuestros supuestos y 
poder hacer cálculos cuidadosos, generalmente basados en algún tipo de aleatorización.

Ninguno de los dos tipos de análisis funciona muy bien sin el otro, 
(@tukeyexpconf) y explicaremos
por qué un poco más adelante. Por el momento, para ilustrar el enfoque exploratorio, 
comenzaremos con datos que podemos
describir de manera completa y efectiva sin necesidad de hacer resúmenes o aplicar
técnicas avanzadas.


## Ejemplo: nacimientos

Consideremos una parte de los datos de nacimientos por día del INEGI de 1999 a 2016.
Consideraremos sólo tres meses: enero a marzo de 2016. Estos datos, por su tamaño,
pueden representarse de manera razonablemente efectiva en una visualización de serie 
de tiempo. Examinamos partes del contenido de la tabla:

```{r}
#| message: false
#| warning: false
#| echo: false
library(tidyverse)
library(lubridate)
library(kableExtra)
library(gt)
nacimientos <- read_rds("datos/nacimientos/natalidad.rds") |>
   ungroup() |> 
   filter(year(fecha) == 2016, month(fecha) <= 3)
```

Examinamos partes del contenido de la tabla:

```{r}
tab_1 <- nacimientos |> 
   select(fecha, n) |> 
   slice_head(n = 5)
tab_2 <- nacimientos |> 
   select(fecha, n) |> 
   slice_tail(n = 5)
kable(list(tab_1, tab_2)) |> kable_paper()
```

En un examen rápido de estos números no vemos nada fuera de orden. Los datos
tienen forma de serie de tiempo regularmente espaciada (un dato para cada día). Podemos graficar
de manera simple como sigue:

```{r, fig.width=9, fig.height = 2.5}
ggplot(nacimientos, aes(x = fecha, y = n)) +
   geom_point() +
   geom_line() + 
   scale_x_date(breaks = "1 week", date_labels = "%d-%b") 
```

Esta es una descripción de los datos, que quizá no es muy compacta pero
muestra varios aspectos importantes. En este caso notamos
algunos patrones que saltan a la vista. Podemos marcar los domingos de cada semana:

```{r, fig.width=9, fig.height = 2.5}
domingos_tbl <- nacimientos |> 
   filter(weekdays(fecha) == "Sunday")
ggplot(nacimientos, aes(x = fecha, y = n)) +
   geom_vline(aes(xintercept = fecha), domingos_tbl, colour = "salmon") +
   geom_point() +
   geom_line() + 
   scale_x_date(breaks = "1 week", date_labels = "%d-%b") 
```

Observamos que los domingos ocurren menos nacimientos y los sábados también ocurren relativamente
menos nacimentos. ¿Por qué crees que sea esto?

Adicionalmente a estos patrones observamos otros aspectos interesantes:

- El primero de enero hay considerablemente menos nacimientos de los que esperaríamos
para un viernes. ¿Por qué?
- El primero de marzo hay un exceso de nacimientos considerable. ¿Qué tiene de especial
este primero de marzo?
- ¿Cómo describirías lo que sucede en la semana que comienza el 21 de marzo? ¿Por qué crees que pase eso?
- ¿Cuáles son los domingos con más nacimientos? ¿Qué tienen de especial y qué explicación puede tener?

La confirmación de estas hipótesis, dependiendo de su forma, 
puede ser relativamente simple (por ejemplo ver una serie más larga de domingos comparados
con otros días de la semana) hasta muy 
compleja (investigar preferencias de madres, de doctores o de hospitales, costumbres y actitudes,
procesos en el registro civil, etc.)

## Procesos generadores de datos {-}

De este primer ejemplo donde usamos una gráfica simple, vemos que 
una visión descontextualizada de estos datos no tiene mucha utilidad


::: callout-tip
# El proceso generador de datos

Nótese que en todas estas preguntas hemos tenido que recurrir a conocimientos generales
y de dominio para interpretar y hacer hipótesis acerca de
lo que vemos en la gráfica.
Las explicaciones son típicamente complejas e intervienen distintos
aspectos del comportamiento de actores, sistemas, y métodos de recolección de datos involucrados.

Al conjunto de esos aspectos que determinan los datos que finalmente observamos le
llamamos el **proceso generador de datos**.
:::


El análisis de datos en general busca entender las partes importantes del proceso
que los generó. En el análisis descriptivo y exploratorio buscamos iluminar ese proceso,
proponer hipótesis y buscar caminos interesantes para investigar, ya sea con
técnicas cuantitativas o con trabajo de campo (como sugiere el título de artículo
de David A. Friedman: [Statistical Models and Shoe Leather](https://psychology.okstate.edu/faculty/jgrice/psyc5314/Freedman_1991A.pdf)).

Con la teoría de probabilidades podemos modelar más explícitamente partes de
estos procesos generadores de datos, especialmente cuando controlamos parte
de ese proceso generador mediante técnicas estadísticas de diseño, por ejemplo,
usando aleatorización.


## Ejemplo (cálculos renales) {-}

En este ejemplo también intentaremos mostrar los datos completos sin intentar
resumir.

Este es un estudio real acerca de tratamientos para cálculos renales 
(@kidney94). Pacientes se asignaron de una forma no controlada
a dos tipos de tratamientos para reducir
cálculos renales. Para cada paciente, conocemos el el tipo de ćalculos que tenía
(grandes o chicos) y si el tratamiento tuvo éxito o no.

La tabla original se ve como sigue (muestreamos algunos renglones):

```{r, message = FALSE}
calculos <- read_csv("./datos/kidney_stone_data.csv")
names(calculos) <- c("tratamiento", "tamaño", "éxito")
calculos <- calculos |> 
   mutate(tamaño = ifelse(tamaño == "large", "grandes", "chicos")) |> 
   mutate(resultado = ifelse(éxito == 1, "mejora", "sin_mejora")) |> 
   select(tratamiento, tamaño, resultado)
nrow(calculos)
calculos |> sample_n(15) |> 
   kable() |> kable_paper(full_width = FALSE)
```

Aunque estos datos contienen información de 700 pacientes (cada renglón es un paciente),
los datos pueden resumirse sin pérdida de información contando como sigue:

```{r}
calculos_agregada <- calculos |> 
   group_by(tratamiento, tamaño, resultado) |> 
   count()
calculos_agregada |> kable() |> kable_paper(full_width = FALSE)
```
Este resumen no es muy informativo, pero al menos vemos qué valores aparecen en 
cada columna de la tabla. Como en este caso nos interesa principalmente la tasa de éxito
de cada tratamiento, podemos mejorar mostrando como sigue:

```{r}
calculos_agregada |> pivot_wider(names_from = resultado, values_from = n) |> 
   mutate(total = mejora + sin_mejora) |> 
   mutate(prop_mejora = round(mejora / total, 2)) |> 
   select(tratamiento, tamaño, total, prop_mejora) |> 
   arrange(tamaño) |> 
   kable() |> kable_paper(full_width = FALSE)
```

Esta tabla descriptiva es una reescritura de los datos, y no hemos resumido nada todavía.
Sin embargo, esta tabla es apropiada para empezar a contestar la pregunta:

- ¿Qué indican estos datos acerca de qué tratamiento es mejor? ¿Acerca del tamaño
de cálculos grandes o chicos?

Supongamos que otro analista decide comparar los pacientes que recibieron cada
tratamiento, ignorando la variable de tamaño:


```{r}
calculos |> group_by(tratamiento) |> 
   summarise(prop_mejora = mean(resultado == "mejora") |> round(2)) |> 
   kable() |> kable_paper(full_width = FALSE)
```

y parece ser que el tratamiento $B$ es mejor que el $A$. Esta es una 
paradoja (un ejemplo de la [paradoja de Simpson](https://es.wikipedia.org/wiki/Paradoja_de_Simpson)) . Si un médico no sabe 
que tipo de cálculos tiene el paciente,
¿entonces debería recetar $B$? ¿Si sabe debería recetar $A$? Esta discusión parece
no tener mucho sentido.

Podemos investigar por qué está pasando esto considerando la siguiente tabla, que
solo examina cómo se asignó el tratamiento dependiendo del tipo de cálculos de cada paciente:

```{r}
calculos |> group_by(tratamiento, tamaño) |> count() |> 
   kable() |> kable_paper(full_width = FALSE)
```

Nuestra hipótesis aquí es que  la decisión de qué tratamiento usar depende del tamaño
de los cálculos. En este caso, por alguna razón se prefiere utilizar el tratamiento $A$ para
cálculos grandes, y $B$ para cálculos chicos. Esto quiere decir que
en la tabla total *el tratamiento $A$ está en desventaja porque se usa en
casos más difíciles*, pero el tratamiento $A$ parece ser en general mejor.

Igual que en el ejemplo anterior, los resúmenes descriptivos están acompañados
de hipótesis acerca del *proceso generador de datos*, y esto ilumina lo que estamos
observando y nos guía hacia descripciones provechosas de los datos. Las explicaciones
no son tan simples y, otra vez, interviene el comportamiento de doctores, 
tratamientos, y distintos tipos de padecimientos. 


## Ejemplo {-}

Ahora supongamos que tenemos datos de un tratamiento para mejorar
enfermedades de corazón. En el estudio también se mide, durante la duración del
estudio, si la presión del paciente
es alta o baja. Supongamos otra vez que tenemos dos tratamientos, A y B, y obtenemos
los siguientes resultados:

```{r}
corazon <- calculos |> rename(presión = tamaño) |> 
  mutate(presión = recode(presión, chicos = "baja", grandes = "alta"))
```



```{r}
#| echo: false
corazon_agregada <- corazon |> 
   group_by(tratamiento, presión, resultado) |> 
   count()
corazon_agregada |> pivot_wider(names_from = resultado, values_from = n) |> 
   mutate(total = mejora + sin_mejora) |> 
   mutate(prop_mejora = round(mejora / total, 2)) |> 
   select(tratamiento, presión, total, prop_mejora) |> 
   arrange(presión) |> 
   kable()|> 
   kable_paper(full_width = FALSE)
```

```{r}
corazon |> group_by(tratamiento) |> 
   summarise(prop_mejora = mean(resultado == "mejora") |> round(2)) |> 
   kable() |> 
   kable_paper(full_width = FALSE)
```

- En este ejemplo, ¿cuál es el análisis más apropiado? ¿Qué cosas necesitarías saber
para tomar una decisión?
- ¿En qué es diferente o similar al caso de los cálculos renales?

## Inferencia

En los ejemplos anteriores, sólo vimos muestras de datos (algunos pacientes, algunas fechas).
Nuestras descripciones son, estrictamente hablando, válidas para esa muestra de los datos.

Si quisiéramos generalizar a la población gneral de pacientes con cálculos (quizá en
nuestra muestra el tratamiento A parece mejor, pero 
¿qué podemos decir para la población de pacientes?), o quisiéramos predecir
cómo van a ser los nacimientos en 2021,  requerimos hacer **inferencia**. Este tipo de análisis, central en la estadística, busca
establecer condiciones para poder generalizar de nuestra muestra a datos no observados (otros
pacientes, nacimientos en el futuro), y cuantificar qué tan bien o mal podemos hacerlo.

Para llegar a este tipo de análisis, generalmente tenemos que comenzar con el análisis
exploratorio, y con la comprensión de los fundamentos del proceso generador asociado
a nuestros datos. En algunos casos, veremos que es posible usar herramientas matemáticas
para modelar aspectos de nuestro proceso generador de datos, que cuando son válidas, nos permiten
generalizar y ampliar apropiadamente el rango de nuestras conclusiones.

La herramienta básica para construir, entender y operar con estos modelos es la **teoría
de probabilidad**, que veremos más adelante.

## Ejemplo: más de nacimientos en México


Este ejemplo sigue sigue ideas de un análisis de [A. Vehtari y A. Gelman](https://statmodeling.stat.columbia.edu/2016/05/18/birthday-analysis-friday-the-13th-update/),
junto con análisis de serie de tiempo de @ClevelandVis

Usaremos los datos de nacimientos registrados por día en México, desde 1999. Haremos una pregunta
como ¿cuáles son los cumpleaños más frecuentes?, o ¿Qué mes del año hay más nacimientos?

Una gráfica popular (ver por ejemplo [esta visualización](http://thedailyviz.com/2016/09/17/how-common-is-your-birthday-dailyviz/)):

```{r, echo = FALSE}
knitr::include_graphics("./imagenes/heatmapbirthdays1.png")
```

¿Cómo criticarías este análisis desde el punto de vista de los tres primeros principios del
diseño analítico? ¿Las comparaciones son útiles? ¿Hay aspectos multivariados? ¿Qué tan bien
explica o sugiere estructura, mecanismos o causalidad?


### Datos de natalidad para México {-}


```{r, echo = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(lubridate)
library(ggthemes)
natalidad <- readRDS("./datos/nacimientos/natalidad.rds") |> 
    mutate(dia_semana = weekdays(fecha)) |> 
    mutate(dia_año = yday(fecha)) |> 
    mutate(año = year(fecha)) |> 
    mutate(mes = month(fecha)) |> ungroup() |> 
    mutate(dia_semana = recode(dia_semana, Monday = "Lunes", Tuesday = "Martes", Wednesday = "Miércoles",
                               Thursday = "Jueves", Friday = "Viernes", Saturday = "Sábado", Sunday = "Domingo")) |> 
    mutate(dia_semana = fct_relevel(dia_semana, c("Lunes", "Martes", "Miércoles", "Jueves", "Viernes", "Sábado", "Domingo")))
```

Consideramos los datos agregados de número de nacimientos (registrados) por día desde 1999 hasta 2016.

Podemos hacer una primera gráfica de la serie de tiempo que no es muy útil:

```{r, fig.width = 10, fig.height=3, echo = FALSE}
ggplot(natalidad, aes(x = fecha, y = n)) + 
    geom_line(alpha = 0.2) + geom_point(alpha = 0.5) +
    ylab("Nacimientos")
```

Hay varias características que notamos. Principalmente, la tendencia ligeramente decreciente de número de nacimientos
a lo largo de los años, un patrón anual, dispersión producida por los días de la semana. 

Solo estas características hacen que la comparación
entre días sea una difícil de interpretar. Supongamos que comparamos el número de nacimientos de dos miércoles dados.
Esa comparación será diferente dependiendo del año donde ocurrieron, el mes donde ocurrieron, si semana santa ocurrió
en algunos de los miércoles, y así sucesivamente.

Como en nuestros ejemplos anteriores, la idea  del siguiente análisis es aislar las componentes que 
observamos en la serie de tiempo: extraemos componentes ajustadas, y luego examinamos los residuales.

En este caso particular, construiremos una **descomposición aditiva** de la serie de tiempo (@ClevelandVis).

### Tendencia {-}

Comenzamos por extraer la tendencia, haciendo promedios loess con vecindades relativamente grandes. Quizá preferiríamos 
suavizar menos para capturar más variación lenta, 
pero si hacemos esto en este punto empezamos a absorber parte de la componente anual:


```{r}
mod_1 <- loess(n ~ as.numeric(fecha), data = natalidad, span = 0.2, degree = 1)
datos_dia <- natalidad |> mutate(ajuste_1 = fitted(mod_1)) |> 
    mutate(res_1 = n - ajuste_1)
```

```{r, message = FALSE, echo = FALSE, fig.width = 10, fig.height = 4}
g_1 <- ggplot(datos_dia, aes(x = fecha)) + geom_point(aes(y = n), alpha = 0.2, size = 1) +
    geom_line(aes(y = ajuste_1), colour = "red", size = 1.2) + xlab("") + labs(caption = "Suavizamiento apropiado")
g_2 <- ggplot(datos_dia, aes(x = fecha, y = n)) + geom_point(alpha = 0.2, size = 1) +
    geom_smooth(method = "loess", span = 0.075, method.args = list(degree = 1), se = FALSE) + xlab("") +
    labs(caption = "Requiere mayor suavizamiento")
gridExtra::grid.arrange(g_1, g_2, ncol = 2) 
```

A principios de 2000 el suavizador está en niveles de alrededor de 7000 nacimientos diarios, hacia 2015 ese número es más
cercano a unos 6000.


### Componente anual {-}

Restamos a la serie la tendencia, y así obtenemos mejores comparaciones controlando por tendencia
(por ejemplo, comparar un día de 2000 y de 2015 tendria más sentido). Ahora ajustamos **los residuales
del suavizado anterior**, pero con menos 
suavizamiento. Así evitamos capturar tendencia:

```{r}
mod_anual <- loess(res_1 ~ as.numeric(fecha), data = datos_dia, degree = 2, span = 0.005)
datos_dia <- datos_dia |> mutate(ajuste_2 = fitted(mod_anual)) |> 
    mutate(res_2 = res_1 - ajuste_2)
```

```{r, echo = FALSE}
ggplot(datos_dia, aes(x = fecha)) +
    geom_point(aes(y = res_1), alpha = 0.2, size = 1) +
    geom_line(aes(y = ajuste_2), colour = "red", size = 1.2) 
```


### Día de la semana {-}

Ahora podemos capturar el efecto de día de la semana. En este caso, podemos hacer suavizamiento
loess para cada serie independiente

```{r}
datos_dia <- datos_dia |> group_by(dia_semana) |> nest() |> 
    mutate(ajuste_mod = 
      map(data, ~ loess(res_2 ~ as.numeric(fecha), data = .x, span = 0.1, degree = 1))) |> 
    mutate(ajuste_3 =  map(ajuste_mod, fitted)) |> 
    select(-ajuste_mod) |> unnest(cols = c(data, ajuste_3)) |> 
    mutate(res_3 = res_2 - ajuste_3) |> ungroup()
```

```{r, echo = FALSE, fig.width = 8}
ggplot(datos_dia, aes(x = fecha)) + geom_point(aes(y = res_2), alpha = 0.5, colour = "gray")   +
    geom_line(aes(y = ajuste_3, colour = dia_semana), size = 1)  + xlab("")
```

### Residuales {-}

Examinamos los residuales finales quitando los efectos ajustados:

```{r}
ggplot(datos_dia, aes(x = fecha, y = res_3)) + geom_line() +
    geom_smooth(method = "loess", span = 0.02, 
                method.args = list(degree=1, family = "symmetric"))
```

**Observación**: nótese que la distribución de estos residuales tiene irregularidades interesante:
es una distribución con colas largas, y no se debe a unos cuantos atípicos. 
Esto generalmente es indicación que hay factores importantes que hay que examinar en los residuales:

```{r, echo = FALSE, fig.width = 4, fig.height = 3}
ggplot(datos_dia, aes(sample = res_3)) + geom_qq(distribution = stats::qunif) +
  ylab("Nacimientos (residual)") + xlab("")
```

### Reestimación {-}

Cuando hacemos este proceso secuencial ajuste -> residual, a veces conviene iterarlo. La razón es que un una
segunda o tercera pasada podemos hacer mejores estimaciones de cada componente, y es posible suavizar menos sin 
capturar componentes de más alta frecuencia.

Así que podemos regresar a la serie
original para hacer mejores estimaciones, más suavizadas:

```{r}
# quitamos componente anual y efecto de día de la semana
datos_dia <- datos_dia |> mutate(n_1 = n - ajuste_2 - ajuste_3)
# reajustamos
mod_1 <- loess(n_1 ~ as.numeric(fecha), data = datos_dia, span = 0.02, degree = 2,
               family = "symmetric")
```

```{r, echo = FALSE}
datos_dia <- datos_dia |> ungroup() |> 
    mutate(ajuste_4 = fitted(mod_1)) |> 
    mutate(res_4 = n - ajuste_4) |> 
    mutate(n_2 = n - ajuste_4 - ajuste_3)
ggplot(datos_dia, aes(x = fecha)) +
    geom_point(aes(y = n_1), alpha = 0.3, size = 1) +
    geom_line(aes(y = ajuste_4), colour = "red", size = 1)
```


```{r}
mod_anual <- loess(n_2 ~ as.numeric(fecha), data = datos_dia, 
               degree = 2,  span = 0.01, family = "symmetric")
datos_dia <- datos_dia |>
    mutate(ajuste_5 = fitted(mod_anual)) |> 
    mutate(res_5 = n_2 - ajuste_5) |>
    mutate(n_3 = n - ajuste_4 - ajuste_5)
```

```{r, echo = FALSE}
ggplot(datos_dia, aes(x = fecha)) +
    geom_point(aes(y = n_2), alpha = 0.2, size = 1) +
    geom_line(aes(y = ajuste_5), colour = "red", size = 1) 
```


Y ahora repetimos con la componente de día de la semana:

```{r, echo = FALSE}
datos_dia <- datos_dia |> group_by(dia_semana) |> nest() |> 
    mutate(ajuste_mod = map(data, ~ loess(n_3 ~ as.numeric(fecha), data = .x, span = 0.1, 
                                          degree=1, family = "symmetric"))) |> 
    mutate(ajuste_6 =  map(ajuste_mod, fitted)) |> 
    select(-ajuste_mod) |> 
    unnest(cols = c(data, ajuste_6)) |> 
    mutate(res_6 = n_3 - ajuste_6)
ggplot(datos_dia, aes(x = fecha, y = n_3, group = dia_semana)) + 
  geom_point(aes(y = n_3), alpha = 0.2, size = 1)  +
    geom_line(aes(y = ajuste_6, colour = dia_semana), size =1) 
```


### Análisis de componentes {-}

Ahora comparamos las componentes estimadas y los residuales en una misma gráfica. La suma de todas
estas componentes da los datos originales: es una descomposición aditiva.

```{r, fig.width =7, fig.height = 7, echo = FALSE}
media <- mean(datos_dia$n) |> round()
datos_l <- datos_dia |> 
    select(fecha, dia_semana, n, ajuste_4, ajuste_5, ajuste_6, res_6) |> 
    mutate(ajuste_4_centrado = ajuste_4 - mean(ajuste_4)) |> 
    gather(componente, valor, ajuste_5:ajuste_4_centrado) |> 
    mutate(componente = recode(componente, ajuste_4_centrado="Tendencia", ajuste_5 = "Anual", ajuste_6 = "Día de la semana",
                               res_6 = "Residual")) |> 
    mutate(componente = fct_relevel(componente, "Tendencia", "Anual", "Día de la semana", "Residual"))
ggplot(datos_l, aes(x = fecha, y = valor, colour = dia_semana)) + 
    facet_wrap(~ componente,  ncol = 1) +
    geom_point(size=0.5) + scale_colour_colorblind()  +
    labs(caption = "Media total: 6435")

```

Y esto nos da muchas comparaciones buenas que explican la variación que vimos en los datos.
Una gran parte de los residuales está entre +-/250 nacimientos por día, pero las colas tienen
una dispersión mucho mayor:

```{r}
quantile(datos_dia$res_6, c(00, .01,0.05, 0.10, 0.90, 0.95, 0.99, 1)) |> round()
```

¿A qué se deben estas colas tan largas?


```{r, echo = FALSE}
pascua <- ymd(as.character(timeDate::Easter(2000:2017)))
pascua_m1 <- ymd(as.character(timeDate::Easter(2000:2017))) - days(1)
pascua_m2 <- ymd(as.character(timeDate::Easter(2000:2017))) - days(2)
pascua_m3 <- ymd(as.character(timeDate::Easter(2000:2017))) - days(3)
pascua_m4 <- ymd(as.character(timeDate::Easter(2000:2017))) - days(4)
pascua_m5 <- ymd(as.character(timeDate::Easter(2000:2017))) - days(5)
pascua_m6 <- ymd(as.character(timeDate::Easter(2000:2017))) - days(6)

datos_dia$pascua <- as.numeric(datos_dia$fecha %in% pascua)
datos_dia$pascua_m1 <- as.numeric(datos_dia$fecha %in% pascua_m1)
datos_dia$pascua_m2 <- as.numeric(datos_dia$fecha %in% pascua_m2)
datos_dia$pascua_m3 <- as.numeric(datos_dia$fecha %in% pascua_m3)
datos_dia$pascua_m4 <- as.numeric(datos_dia$fecha %in% pascua_m4)
datos_dia$pascua_m5 <- as.numeric(datos_dia$fecha %in% pascua_m5)
datos_dia$pascua_m6 <- as.numeric(datos_dia$fecha %in% pascua_m6)
datos_dia <- datos_dia |> mutate(semana_santa = pascua + pascua_m1 +
                                      pascua_m2 + pascua_m3 + pascua_m4 + pascua_m5 + pascua_m6)
```

### Viernes 13? {-}

Podemos empezar con una curosidad: En Viernes o Martes 13, ¿nacen menos niños? 

```{r, echo = FALSE, fig.width = 12, fig.height = 4}
datos_dia <- datos_dia |> 
  ungroup() |> 
  mutate(dia_mes = day(datos_dia$fecha)) |> 
  mutate(viernes_13 = ifelse(dia_mes == 13 & dia_semana == "Viernes", "Viernes 13", "Otro Día")) |> 
  mutate(martes_13 = ifelse(dia_mes == 13 & dia_semana == "Martes", "Martes 13", "Otro Día")) |> 
  mutate(en_semana_santa = ifelse(semana_santa, "Sí", "No"))
datos_13 <- datos_dia |> filter(dia_semana == "Martes" | dia_semana == "Viernes") |> 
  mutate(tipo_dia_13 = ifelse(martes_13 == "Martes 13", "Martes 13",
      ifelse(viernes_13 == "Viernes 13", "Viernes 13", "Otro Martes o Viernes")))
ggplot(datos_13, 
    aes(x = fecha, y = res_6, colour = en_semana_santa)) +
    geom_hline(yintercept = 0, colour = "gray") +
    geom_point(alpha = 0.8) +
    facet_wrap(~tipo_dia_13) + scale_color_colorblind() + ylab("Residual: exceso de nacimientos")
```

Nótese que fue útil agregar el indicador de Semana santa por el Viernes 13 de Semana Santa
que se ve como un atípico en el panel de los viernes 13.

### Residuales: antes y después de 2006 {-}

Veamos primero una agregación sobre los años de los residuales. Lo primero es observar un
cambio que sucedió repentinamente en 2006:

```{r}
sept_1 <- ymd(paste0(2000:2016, "-09-01")) |> yday()
datos_dia <- datos_dia |> mutate(antes_2006 = ifelse(año < 2006, "Antes de 2006", "2006 en adelante"))
ggplot(datos_dia , aes(x = dia_año, y = res_6, group = factor(año))) + 
    geom_point(size = 0.5) +
    geom_vline(xintercept = sept_1, alpha = 0.3, colour = "red") +
    facet_wrap( ~ antes_2006, ncol = 1) + ylab("Residual: exceso de nacimientos") +
    annotate("text", x = 260, y = -1500, label = "Sept 1", colour = "red")
    
```

La razón es un cambio en la ley acerca de cuándo pueden entrar los niños a la primaria. Antes era
por edad y había poco margen. Ese exceso de nacimientos son reportes falsos para que los niños
no tuvieran que esperar un año completo por haber nacido unos cuantos días antes de la fecha límite.

Otras características que debemos investigar:

- Efectos de Año Nuevo, Navidad, Septiembre 16 y otros días feriados como Febrero 14.
- Semana santa: como la fecha cambia, vemos que los residuales negativos tienden a ocurrir dispersos
alrededor del día 100 del año. 

###  Otros días especiales: más de residuales {-}

Ahora promediamos residuales (es posible agregar barras para indicar dispersión a lo largo de los años) para cada 
día del año. Podemos identificar ahora los residuales más grandes: se deben, por ejemplo, a días feriados, con 
consecuencias adicionales que tienen en días ajuntos (excesos de nacimientos):

```{r, echo = FALSE, fig.width = 10, fig.height = 8}
datos_da <- datos_dia |> 
    mutate(bisiesto = (año %in% c(2000, 2004, 2008, 2012, 2016))) |> 
    mutate(dia_año_366 = ifelse(!bisiesto & dia_año >= 60, dia_año + 1, dia_año)) |> 
    group_by(dia_año_366, antes_2006, bisiesto) |> 
    summarise(residual_prom = mean(res_6)) |> 
    mutate(grupo = cut(residual_prom, c(-2000,-200, 200,2000))) 
label_y <- -1000
ggplot(datos_da, aes(x = dia_año_366, y = residual_prom, colour = grupo, group=1)) +
    theme(legend.position = "none") +
    facet_wrap(~ antes_2006,  ncol = 1) +
    annotate("text", x = yday("2014-02-14"), y = label_y, label = "San Valentín", 
             colour="black", alpha = 0.5, angle = 90, vjust = -0.5) +
    geom_vline(xintercept = yday("2014-02-14"), colour = "gray") +
    annotate("text", x = yday("2004-02-29"), y = label_y, label = "Febrero 29", 
             colour="black", alpha = 0.5, angle = 90, vjust = -0.5) +
    geom_vline(xintercept = yday("2004-02-29"), colour = "gray") +
    annotate("text", x = (yday("2013-09-16") + 1 ) %% 365, y = label_y, label = "Independencia", 
             colour="black", alpha = 0.5, angle = 90, vjust = -0.5) +
        geom_vline(xintercept = yday("2004-09-16"), colour = "gray") +
    annotate("text", x = (yday("2013-11-02") + 1) %% 365, y = label_y, label = "Muertos", 
             colour="black", alpha = 0.5, angle = 90, vjust = -0.5) +
      geom_vline(xintercept = yday("2004-11-02"), colour = "gray") +
        annotate("text", x = (yday("2013-12-25") + 1) %% 365, y = label_y, label = "Navidad", 
             colour="black", alpha = 0.5, angle = 90, vjust = -0.5) +
      geom_vline(xintercept = yday("2004-12-25"), colour = "gray") +
    annotate("text", x = (yday("2013-01-01")) %% 365, y = label_y, label = "Año Nuevo", 
             colour="black", alpha = 0.5, angle = 90, vjust = -0.5) +
      geom_vline(xintercept = yday("2004-01-01"), colour = "gray") +
    annotate("text", x = (yday("2013-05-01") + 1) %% 365, y = label_y, label = "Mayo 1", 
             colour="black", alpha = 0.5, angle = 90, vjust = -0.5) +
      geom_vline(xintercept = yday("2004-05-01"), colour = "gray") +
    annotate("text", x = (yday("2013-09-01") + 1) %% 365, y = label_y, label = "Septiembre 1", 
             colour="black", alpha = 0.5, angle = 90, vjust = -0.5) +
      geom_vline(xintercept = yday("2004-09-01"), colour = "gray") +
    geom_line(colour = "gray80") +
    geom_point(size = 1.2) + scale_color_colorblind()+ ylab("Residual: exceso de nacimientos")
```


### Semana santa {-}

Para Semana Santa tenemos que hacer unos cálculos. Si alineamos los datos por días antes de Domingo de Pascua,
obtenemos un patrón de caída fuerte de nacimientos el Viernes de Semana Santa, y la característica forma
de "valle con hombros" en días anteriores y posteriores estos Viernes. ¿Por qué ocurre este patrón?

```{r, echo = FALSE}
pascuas <- tibble(pascua_dia = ymd(as.character(timeDate::Easter(1999:2017)))) |> 
    mutate(año = year(pascua_dia))
datos_dia <- left_join(datos_dia, pascuas, by = "año") |> 
    mutate(dias_para_pascua = fecha - pascua_dia) |> 
    mutate(dias_para_pascua = as.numeric(dias_para_pascua))
datos_pascua <- datos_dia |> filter(abs(dias_para_pascua) < 20)
ggplot(datos_pascua, aes(x = dias_para_pascua, y = res_6)) + 
    geom_line(aes(group=año), colour ="gray") + geom_point(colour = "gray") +
    geom_smooth(data = datos_pascua, aes(x=dias_para_pascua, y = res_6), 
                se = FALSE, span = 0.12, method = "loess", col = "red") +
    geom_hline(yintercept = 0)+ ylab("Residual: exceso de nacimientos")
```


Nótese un defecto de nuestro modelo: el patrón de "hombros" alrededor del Viernes Santo no es suficientemente
fuerte para equilibrar los nacimientos faltantes. ¿Cómo podríamos mejorar nuestra descomposición?


## Descripciones simples y problemas complejos

Muchas veces se descarta al análisis descriptivo o exploratorio (al menos
en sus formas más crudas) como algo que no requiere
mucha habilidad técnica o conocimiento de dominio, o cuando se quiere evitar plantear 
hipótesis claras acerca de los datos que ayuden en su entendimiento.

En realidad el análisis descriptivo y exploratorio es crucial en el análisis
de datos en general, y tiene siempre
que venir acompañado de conocimiento de dominio, habilidad técnica, una mente crítica y
muchas veces ingenio y creatividad.

## ¿Qué preguntas formular?

Existen algunas prácticas generales que utilizamos para hacer validaciones y
resúmenes simples de los datos que discutiremos más adelante. Por el momento, discutimos
las razones por las que estamos haciendo ese análisis en un principio.

En general, comenzamos con algunas preguntas básicas que quisiéramos contestar con los datos.
El análisis exploratorio juega un papel central para comenzar a responder:

- ¿Es razonable la pregunta que queremos contestar?
- ¿Podemos contestar la pregunta con los datos que tenemos?

Aunque estos dos incisos a veces parecen transparentes y simples de contestar,
generalmente no lo son: las preguntas que queremos contestar y los problemas
que queremos resolver usualmente son no triviales.

## Formulación de preguntas y respuestas

El proceso de la ciencia de datos no va desde las preguntas
hasta las respuestas en un camino lineal.

En esta gráfica [Roger Peng](https://simplystatistics.org/2019/04/17/tukey-design-thinking-and-better-questions) hay tres caminos: uno es uno ideal que pocas veces sucede,
otro produce respuestas poco útiles pero es fácil, y otro es tortuoso pero que 
caracteriza el mejor trabajo de análisis de datos:


```{r, echo = FALSE, message = FALSE, fig.cap = "Adaptado de R. Peng: Tukey, design thinking and better questions"}
library(tidyverse)
puntos <- tibble(x = c(0.5, 1.2, 4, 4), y = c(0.5, 4, 0.5, 5),
                 etiqueta = c("Dónde\ncomenzamos\nrealmente", "Análisis de datos \n poco útil, de bajo impacto",  "Dónde creeemos \nque comenzamos", "Nuestra\nmeta "))
set.seed(211)
browniano <- tibble(x = 0.5 +  cumsum(c(0,rnorm(50, 0.03, 0.1))) ,
                    y = 0.5 +  cumsum(c(0, rnorm(50, 0.02, 0.2))))
puntos <- 
  bind_rows(puntos, tail(browniano, 1) |> 
              mutate(etiqueta = "¡¿terminamos!?"))
flechas <- 
  tibble(x = c(0.5, 4), y = c(0.5, 0.5), xend = c(1.2, 4), yend = c(4, 5))
ggplot(puntos, aes(x = x, y = y)) + 
    xlab("Calidad de la pregunta") +
    ylab("Peso de la evidencia") +
    theme(axis.text.x=element_blank(),
        axis.text.y=element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()) +
    geom_segment(data = flechas, aes(xend=xend, yend=yend),
                 arrow = arrow(length = unit(0.3, "inches"))) +
    geom_path(data = browniano) +
    geom_point(data = browniano) +
    geom_point(colour="red", size = 5) +
    geom_text(aes(label = etiqueta), vjust = -0.5, hjust = 1.1, size = 4.2) +
    #labs(caption = "Adaptado de R. Peng: Tukey, design thinking and better questions.") +
    xlim(c(-0.1 , 4)) + ylim(c(0,6))
    
```

El proceso típico involucra pasos como los siguientes, y es un proceso **no lineal**:

* Hacer preguntas de la materia que nos interesa
* Recolectar, consumir y procesar los datos para abordarla
* Explorar estos datos y evaluar su calidad
* Hacer análisis o modelos
* Reportar los resultados de forma adecuada y con esto resolver y replantear las preguntas importantes. 

Por ejemplo, evaluar la calidad de los datos puede llevar a replantear la necesidad de obtener más información o de hacer estudios específicos. Así también, los modelos pueden dar luz sobre las preguntas que los originan.


```{block2, type="comentario"}
¿Por dónde empezar el análisis descriptivo y exploratorio? ¿Cómo sabemos que
vamos por buen camino y qué hacer cuando sentimos que nos estancamos?
```

## ¿Cómo saber que vamos en el camino correcto?
 
Comenzamos por discribir cuáles son los signos de calidad del análisis 
que piensa usarse como insumo para una decisión. 
Los principios del diseño analítico de Edward Tufte (@tufte06) son:

Los análisis exitosos:

1. Muestran y explotan **comparaciones**, diferencias y variación.
2. Tienden a ser **multivariados**: estudian conjuntamente más de 1 o 2 variables.
3. Muestran y explotan **estructura sistemática**, sugieren explicaciones. Cuando es posible,
aportan evidencia de causalidad.

También muy importantes pero en los que pondremos menos énfasis:

4. Datos y procesos están bien **documentados**. El análisis es reproducible y transparente.
5. Intentan **integrar** la evidencia completa: teoría, texto, explicaciones, tablas y
gráficas.

Y finalmente, el principio general:

6. La calidad, relevancia, e integridad del contenido y los datos son los que
al final sostienen al análisis - por sí mismos, **el uso de técnicas sofisticadas, algoritmos novedosos, uso o no de grandes datos, estilo de visualizaciones o presentaciones no son marcas o sellos de un análisis de datos exitoso**.

::: callout-tip
Evaluar un análisis o resultado en estos seis puntos generalmente ayuda en el 
proceso de refinamiento de preguntas y respuestas.
::: 



## Gráfica de Minard 

La ilustración que Tufte usa para mostrar excelencia en diseño analítico es
una [gráfica de Minard](https://en.wikipedia.org/wiki/Charles_Joseph_Minard) que sirve para entender la campaña de Napoleón (1812) 
en Rusia. Es un ejemplo atípico, pero representa bien los principios y también muestra 
la importancia del ingenio en la construcción de un anállsis:


```{r, echo = FALSE, fig.cap = "Marcha de Napoleón de Charles Minard. Tomado de Wikipedia"}
knitr::include_graphics("imagenes/Minard.png")
```

```{block2, type="pregunta"}
¿Cómo satisface los principios del diseño analítico este gráfico?
```

