[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fundamentos de estadística",
    "section": "",
    "text": "En este curso repasaremos:\n\nBásicos de análisis de datos\nInferencia: pruebas de hipótesis\nInferencia: técnicas de remuestreo\nInferencia bayesiana"
  },
  {
    "objectID": "index.html#código-y-ligas",
    "href": "index.html#código-y-ligas",
    "title": "Fundamentos de estadística",
    "section": "Código y ligas",
    "text": "Código y ligas\n\nRepositorio: Github\nEstas notas: https://felipegonzalez.github.io/fundamentos-ine/inferencia.html"
  },
  {
    "objectID": "numericos.html",
    "href": "numericos.html",
    "title": "1  Datos numéricos",
    "section": "",
    "text": "En esta sección mostraremos cómo hacer distintos tipos de resúmenes para mediciones numéricas. Igual que en la sección anterior, consideraremos también el uso de estas descripciones para comparar distintos grupos (o bonches de datos, como les llamaba Tukey), aplicando repetidamente los mismos resúmenes a lo largo de esos distintos grupos."
  },
  {
    "objectID": "numericos.html#cuantiles-o-percentiles-de-una-variable",
    "href": "numericos.html#cuantiles-o-percentiles-de-una-variable",
    "title": "1  Datos numéricos",
    "section": "1.1 Cuantiles o percentiles de una variable",
    "text": "1.1 Cuantiles o percentiles de una variable\nEmpezamos explicando algunas ideas que no serán útiles más adelante.\nPor ejemplo, los siguientes datos fueron registrados en un restaurante durante cuatro días consecutivos:\n\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(kableExtra)\n# usamos los datos tips del paquete reshape2\npropinas <- read_csv(\"./datos/propinas.csv\")\n\nY vemos una muestra\n\nslice_sample(propinas, n = 10) \n\n# A tibble: 10 × 6\n   cuenta_total propina fumador dia   momento num_personas\n          <dbl>   <dbl> <chr>   <chr> <chr>          <dbl>\n 1         16.4    2.3  No      Jue   Comida             2\n 2         13.0    2    No      Jue   Comida             2\n 3         19.4    3    Si      Jue   Comida             2\n 4         13      2    Si      Jue   Comida             2\n 5         32.8    1.17 Si      Sab   Cena               2\n 6         14.5    2    No      Jue   Comida             2\n 7         13.4    1.68 No      Jue   Comida             2\n 8         17.0    1.01 No      Dom   Cena               2\n 9         38.0    3    Si      Sab   Cena               4\n10         12.9    1.1  Si      Sab   Cena               2\n\n\nAquí la unidad de observación es una cuenta particular. Tenemos tres mediciones numéricas de cada cuenta: cúanto fue la cuenta total, la propina, y el número de personas asociadas a la cuenta. Los datos están separados según se fumó o no en la mesa, y temporalmente en dos partes: el día (Jueves, Viernes, Sábado o Domingo), cada uno separado por Cena y Comida.\nEl primer tipo de comparaciones que nos interesa hacer es para una medición numérica es: ¿Varían mucho o poco los datos? ¿Cuáles son valores típicos o centrales? ¿Existen valores muy extremos alejados de valores típicos?\nSupongamos entonces que consideramos simplemente la variable de cuenta_total. Podemos comenzar por ordenar los datos, y ver cuáles datos están en los extremos y cuáles están en los lugares centrales:\n\npropinas <- propinas |> \n  mutate(orden_cuenta = rank(cuenta_total, ties.method = \"first\"), \n         f = (orden_cuenta - 0.5) / n()) \ncuenta <- propinas |>  \n  select(orden_cuenta, f, cuenta_total) |>  \n  arrange(f)\nbind_rows(head(cuenta), tail(cuenta)) |>  kable() |> \n  kable_paper()\n\n\n\n \n  \n    orden_cuenta \n    f \n    cuenta_total \n  \n \n\n  \n    1 \n    0.0020492 \n    3.07 \n  \n  \n    2 \n    0.0061475 \n    5.75 \n  \n  \n    3 \n    0.0102459 \n    7.25 \n  \n  \n    4 \n    0.0143443 \n    7.25 \n  \n  \n    5 \n    0.0184426 \n    7.51 \n  \n  \n    6 \n    0.0225410 \n    7.56 \n  \n  \n    239 \n    0.9774590 \n    44.30 \n  \n  \n    240 \n    0.9815574 \n    45.35 \n  \n  \n    241 \n    0.9856557 \n    48.17 \n  \n  \n    242 \n    0.9897541 \n    48.27 \n  \n  \n    243 \n    0.9938525 \n    48.33 \n  \n  \n    244 \n    0.9979508 \n    50.81 \n  \n\n\n\n\n\ny graficamos los datos en orden, interpolando valores consecutivos.\n\n\n\n\n\nA esta función le llamamos la función de cuantiles para la variable cuenta total. Nos sirve para comparar directamente los distintos valores que observamos los datos según el orden que ocupan.\n\n\n\n\n\n\nCuantiles de datos numéricos\n\n\n\nEl cuantil \\(f\\) de un bonche de datos numéricos es el valor \\(q(f)\\), en la escala de medición de nuestros datos, tal que aproximadamente una fracción \\(f\\) de los datos está por abajo de \\(q(f)\\).\n\nAl cuantil \\(f=0.5\\) le llamamos la mediana.\nA los cuantiles \\(f=0.25\\) y \\(f=0.75\\) les llamamos cuantiles inferior y superior.\n\n\n\nDispersión y valores centrales\n\nEl rango de datos va de unos 3 dólares hasta 50 dólares\nLos valores centrales (del cuantil 0.25 al 0.75, por ejemplo), están entre unos 13 y 25 dólares\nPodemos usar el cuantil 0.5 (mediana) para dar un valor central de esta distribución, que está alrededor de 18 dólares.\n\nY podemos dar resúmenes más refinados si es necesario\n\nEl cuantil 0.95 es de unos 35 dólares - sólo 5% de las cuentas son de más de 35 dólares\nEl cuantil 0.05 es de unos 8 dólares - sólo 5% de las cuentas son de 8 dólares o menos.\n\nFinalmente, la forma de la gráfica se interpreta usando su pendientes, haciendo comparaciones de diferentes partes de la gráfica:\n\nEntre los cuantiles 0.2 y 0.5 es donde existe mayor densidad de datos: la pendiente es baja, lo que significa que al avanzar en los cuantiles, los valores observados no cambian mucho.\nCuando la pendiente es alta, quiere decir que los datos tienen más dispersión local o están más separados.\n\nY podemos considerar qué sucede en las colas de la distribucion:\n\nLa distribución de valores tiene asimetría: el 10% de las cuentas más altas tiene considerablemente más dispersión que el 10% de las cuentas más bajas. A veces decimos que la cola de la derecha es más larga que la cola de la izquierda\n\nEn algunos casos, es más natural hacer un histograma, donde dividimos el rango de la variable en cubetas o intervalos (en este caso de igual longitud), y graficamos cuántos datos caen en cada cubeta:\n\n\n\n\n\nEs una gráfica más popular, pero perdemos cierto nivel de detalle, y distintas particiones resaltan distintos aspectos de los datos.\nFinalmente, una gráfica más compacta que resume la gráfica de cuantiles o el histograma es el diagrama de caja y brazos. Mostramos dos versiones, la clásica de Tukey (T) y otra versión menos común de Spear/Tufte (ST):\n\nlibrary(ggthemes)\ncuartiles <- quantile(cuenta$cuenta_total)\ncuartiles\n\n     0%     25%     50%     75%    100% \n 3.0700 13.3475 17.7950 24.1275 50.8100 \n\ng_1 <- ggplot(cuenta, aes(x = f, y = cuenta_total)) + \n  labs(subtitle = \"Gráfica de cuantiles: Cuenta total\") +\n  geom_hline(yintercept = cuartiles[2], colour = \"gray\") + \n  geom_hline(yintercept = cuartiles[3], colour = \"gray\") +\n  geom_hline(yintercept = cuartiles[4], colour = \"gray\") +\n  geom_point(alpha = 0.5) + geom_line() \ng_2 <- ggplot(cuenta, aes(x = factor(\"ST\", levels =c(\"ST\")), y = cuenta_total)) + \n  geom_tufteboxplot() +\n  labs(subtitle = \" \") +  xlab(\"\") + ylab(\"\")\ng_3 <- ggplot(cuenta, aes(x = factor(\"T\"), y = cuenta_total)) + geom_boxplot() +\n  labs(subtitle = \" \") +  xlab(\"\") + ylab(\"\")\ng_4 <- ggplot(cuenta, aes(x = factor(\"P\"), y = cuenta_total)) + geom_jitter(height = 0, width =0.2, alpha = 0.5) +\n  labs(subtitle = \" \") +  xlab(\"\") + ylab(\"\")\ng_1 + g_2 + g_3 + g_4 +\n  plot_layout(widths = c(8, 2, 2, 2))\n\n\n\n\nNota: Hay varias maneras de definir los cuantiles. Si tenemos \\(n\\) datos, podríamos poner \\(q(4/n)\\) como el cuarto dato (ordenando del más chico al más grande), y así sucesivamente. Esto implica por ejemplo que \\(q(1)\\) está definido como el valor más grande de los datos, y esto no es tan coveniente cuando trabajamos con modelos de probabilidad. Por eso preferimos definir al \\(k\\)-ésimo dato como el cuantil \\(q(\\frac{k - 0.5}{n})\\). Para las gráficas que estamos haciendo por el momento esto no es muy importante."
  },
  {
    "objectID": "numericos.html#media-y-desviación-estándar",
    "href": "numericos.html#media-y-desviación-estándar",
    "title": "1  Datos numéricos",
    "section": "1.2 Media y desviación estándar",
    "text": "1.2 Media y desviación estándar\nOtras medidas más comunes de localización y dispersión para conjuntos de datos son media y desviación estándar muestral.\nLa media de un conjunto de datos \\(x_1,\\ldots, x_n\\) es\n\\[\\bar{x} = \\frac{1}{n}\\sum x_i\\]\ny la desviación estándar es\n\\[\\hat{\\sigma} =\\sqrt{\\frac{1}{n}\\sum (x_i - \\bar{x})^2}\\]\nEn general, no son muy apropiadas para iniciar el análisis exploratorio, pues:\n\nSon medidas más difíciles de interpretar y explicar que los cuantiles. En este sentido, son medidas especializadas. Como ejercicio, intenta explicar intuitivamente qué es la media. Después prueba con la desviación estándar.\nNo son resistentes a valores atípicos o erróneos. Su falta de resistencia los vuelve poco útiles en las primeras etapas de limpieza y descripción.\n\nSin embargo,\n\nLa media y desviación estándar son computacionalmente convenientes, y para el trabajo de modelado, por ejemplo, tienen ventajas claras (cuando se cumplen supuestos). Por lo tanto regresaremos a estas medidas una vez que estudiemos modelos de probabilidad básicos.\nMuchas veces, ya sea por tradición, o por razones específicas, conviene usar estas medidas conocidas (o alguna variación). Por ejemplo cuando estamos estimando cantidades como totales, por ejemplo."
  },
  {
    "objectID": "numericos.html#comparando-grupos-con-variables-numéricas",
    "href": "numericos.html#comparando-grupos-con-variables-numéricas",
    "title": "1  Datos numéricos",
    "section": "1.3 Comparando grupos con variables numéricas",
    "text": "1.3 Comparando grupos con variables numéricas"
  },
  {
    "objectID": "numericos.html#ejemplo-precios-de-casas",
    "href": "numericos.html#ejemplo-precios-de-casas",
    "title": "1  Datos numéricos",
    "section": "Ejemplo: precios de casas",
    "text": "Ejemplo: precios de casas\nConsideramos datos de precios de ventas de la ciudad de Ames, Iowa. Nos interesa entender la variación del precio de las casas.\n\n\n\nCalculamos primeros unos cuantiles de los precios de las casas:\n\nquantile(casas |> pull(precio_miles)) \n\n   0%   25%   50%   75%  100% \n 37.9 132.0 165.0 215.0 755.0 \n\n\nUna primera comparación que podemos hacer es considerar las distintas zonas de la ciudad. Podemos usar diagramas de caja y brazos para comparar precios en distintas zonas de la ciudad:\n\nggplot(casas, aes(x = nombre_zona, y = precio_miles)) + geom_boxplot() + coord_flip()\n\n\n\n\nNótese que de cada zona, los datos tienen una cola derecha más larga que la izquierda, e incluso hay valores extremos en la cola derecha que exceden el rango de variación usual. Una razón por la que puede suceder esto es que haya características particulares que agregan valor considerable a una casa, por ejemplo, el tamaño, una alberca, etc.\nEn primer lugar, podemos considerar el área de las casas. En lugar de graficar el precio, graficamos el precio por metro cuadrado, por ejemplo:\n\n\n\n\nggplot(casas, aes(x = nombre_zona, y = precio_m2)) + geom_boxplot() + coord_flip()\n\n\n\n\nNótese ahora que la variación alrededor de la media es mucho más simétrica, y ya no vemos tantos datos extremos. Aún más, la variación dentro de cada zona parece ser similar, y podríamos describir restos datos de la siguiente forma:\nCuantificamos la variación que observamos de zona a zona y la variación que hay dentro de zonas. La variación que vemos entre las medianas de la zona es:\n\ncasas |> group_by(nombre_zona) |> \n  summarise(mediana_zona = median(precio_m2)) |> \n  pull(mediana_zona) |> quantile() |> round()\n\n  0%  25%  50%  75% 100% \n 963 1219 1298 1420 1725 \n\n\nY las variaciones con respecto a las medianas dentro de cada zona, agrupadas, se resume como:\n\nquantile(casas |> group_by(nombre_zona) |> \n  mutate(residual = precio_m2 - median(precio_m2)) |> \n  pull(residual)) |> round()\n\n  0%  25%  50%  75% 100% \n-765 -166    0  172 1314 \n\n\nNótese que este último paso tiene sentido pues la variación dentro de las zonas, en términos de precio por metro cuadrado, es similar. Esto no lo podríamos hacer de manera efectiva si hubiéramos usado el precio de las casas sin ajustar por su tamaño.\nY vemos que la mayor parte de la variación del precio por metro cuadrado ocurre dentro de cada zona, una vez que controlamos por el tamaño de las casas. La variación dentro de cada zona es aproximadamente simétrica, aunque la cola derecha es ligeramente más larga con algunos valores extremos.\nPodemos seguir con otro indicador importante: la calificación de calidad de los terminados de las casas. Como primer intento podríamos hacer:\n\n\n\n\n\nLo que indica que las calificaciones de calidad están distribuidas de manera muy distinta a lo largo de las zonas, y que probablemente no va ser simple desentrañar qué variación del precio se debe a la zona y cuál se debe a la calidad."
  },
  {
    "objectID": "numericos.html#distribuciones-sesgadas-y-atípicos",
    "href": "numericos.html#distribuciones-sesgadas-y-atípicos",
    "title": "1  Datos numéricos",
    "section": "1.4 Distribuciones sesgadas y atípicos",
    "text": "1.4 Distribuciones sesgadas y atípicos\nEn algunos casos tenemos que trabajar con mediciones que tienen una cola (usualmente la derecha) mucho más larga que la otra. Veamos cuáles son consecuencias típicas.\nConsideremos por ejemplos una muestra de los datos de ENIGH 2018\n\nenigh <- read_csv(\"./datos/enigh-ejemplo.csv\")\n\nY los deciles de ingreso son\n\nenigh <- mutate(enigh, ingreso_mensual_miles = INGTOT / 3000)\n\nenigh |> \n  summarise(\n    f = seq(0, 1, 0.1),\n    cuantiles_ingreso =  quantile(ingreso_mensual_miles, probs = seq(0, 1, 0.1))) |> \n  kable(digits = 2) |> \n  kable_paper(full_width = FALSE)\n\n\n\n \n  \n    f \n    cuantiles_ingreso \n  \n \n\n  \n    0.0 \n    0.81 \n  \n  \n    0.1 \n    2.58 \n  \n  \n    0.2 \n    3.86 \n  \n  \n    0.3 \n    5.53 \n  \n  \n    0.4 \n    6.75 \n  \n  \n    0.5 \n    8.27 \n  \n  \n    0.6 \n    9.99 \n  \n  \n    0.7 \n    12.95 \n  \n  \n    0.8 \n    16.20 \n  \n  \n    0.9 \n    22.18 \n  \n  \n    1.0 \n    317.53 \n  \n\n\n\n\n\ndonde podemos ver cómo cuando nos movemos a deciles más altos, la dispersión aumenta. Existen algunos valores muy grandes. Un histograma no funciona muy bien con estos datos.\n\nggplot(enigh, aes(x = ingreso_mensual_miles)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nSi filtramos los valores muy grandes, de todas formas encontramos una forma similar con una cola larga a la derecha:\n\nggplot(enigh |> filter(ingreso_mensual_miles < 90), \n       aes(x = ingreso_mensual_miles)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nNótese que la media de estos datos no es un resúmen muy útil, porque es difícil de interpretar. Por los valores grandes, la media es considerablemente más alta que la mediana:\n\nenigh |> \n  summarise(\n    media = mean(ingreso_mensual_miles),\n    mediana =  quantile(ingreso_mensual_miles, probs = 0.5)) |> \n  kable(digits = 2)\n\n\n\n \n  \n    media \n    mediana \n  \n \n\n  \n    12.04 \n    8.27 \n  \n\n\n\n\n\nEsta es otra razón para incluir información de cuantiles en la etapa descriptiva. Por ejemplo, podríamos resumir:\n\nenigh |> \n  summarise(\n    f = c(\"min\", 0.05, \"0.50\",  0.95, \"max\"),\n    cuantiles_ingreso =  quantile(ingreso_mensual_miles, probs = c(0, 0.05, 0.5, 0.95, 1))) |> \n  kable(digits = 2)  |> \n  kable_paper(full_width = FALSE)\n\n\n\n \n  \n    f \n    cuantiles_ingreso \n  \n \n\n  \n    min \n    0.81 \n  \n  \n    0.05 \n    1.92 \n  \n  \n    0.50 \n    8.27 \n  \n  \n    0.95 \n    32.24 \n  \n  \n    max \n    317.53 \n  \n\n\n\n\n\nPara obtener una gŕafica más informativa, podemos utilizar una escala logarítmica. El logaritmo de los ingresos es más fácil de describir y veremos también más fácil de trabajar.\n\nggplot(enigh, \n       aes(x = ingreso_mensual_miles)) + \n  geom_histogram(binwidth = 0.12) +\n  scale_x_log10(breaks = c(1, 2, 4, 8, 16, 32, 64, 128, 256)) +\n  xlab(\"Ingreso mensual (miles)\")"
  },
  {
    "objectID": "numericos.html#factor-y-respuesta-numéricos",
    "href": "numericos.html#factor-y-respuesta-numéricos",
    "title": "1  Datos numéricos",
    "section": "1.5 Factor y respuesta numéricos",
    "text": "1.5 Factor y respuesta numéricos\nEn las secciones anteriores vimos cómo describir “bonches” de datos numéricos y categóricos. Adicionalmente, vimos cómo usar esas técnicas para comparar las descripciones a lo largo de varios subconjuntos de los datos.\nEn estos casos, muchas veces llamamos factor a la variables que forma los grupos, y respuesta a la variable que estamos comparando. Por ejemplo, en el caso de tomadores de té comparamos uso de complementos (respuesta) a lo largo de consumidores de distintos tipos de té (factor) En el caso de los precios de las casas comparamos el precio de las casas (respuesta) dependiendo del vecindario (factor) dónde se encuentran.\nCuando tenemos una factor numérico y una respuesta numérica podemos comenzar haciendo diagramas de dispersión. Por ejemplo,"
  },
  {
    "objectID": "numericos.html#ejemplo-cuenta-total-y-propina",
    "href": "numericos.html#ejemplo-cuenta-total-y-propina",
    "title": "1  Datos numéricos",
    "section": "Ejemplo: cuenta total y propina",
    "text": "Ejemplo: cuenta total y propina\n\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(kableExtra)\n# usamos los datos tips del paquete reshape2\npropinas <- read_csv(\"./datos/propinas.csv\")\n\nPodríamos comenzar haciendo:\n\nggplot(propinas, aes(x = cuenta_total, y = propina)) +\n   geom_point() + geom_rug(colour = \"salmon\", alpha = 0.5)\n\n\n\n\nAhora queremos comparar la distribución de propina (respuesta) para distintos niveles del factor (cuenta_total). Por ejemplo, ¿cómo se compara propina cuando la cuenta es de 15 dólares vs 30 dólares?\n\nggplot(propinas, aes(x = cuenta_total, y = propina)) +\n   geom_vline(xintercept = c(15, 30), colour = \"red\") +\n   geom_point() \n\n\n\n\nVemos que los datos de propinas alrededor de 30 dólares están centrados en valores más grandes que en el nivel de 15 dólares, y también que hay más dispersión en el nivel de 30 dólares. Sin embargo, vemos que tenemos un problema: existen realmente muy pocos datos que tengan exactamente 15 o 30 dólares de cuenta. La estrategia es entonces considerar qué sucede cuando la cuenta está alrededor de 15 o alrededor de 30 dólares, donde alrededor depende del problema particular y de cuántos datos tenemos:\n\nggplot(propinas, aes(x = cuenta_total, y = propina)) +\n   geom_ribbon(aes(xmin = 13, xmax = 17), fill = \"salmon\", alpha = 0.5) +\n   geom_ribbon(aes(xmin = 28, xmax = 32), fill = \"salmon\", alpha = 0.5) +\n   geom_point() \n\n\n\n\nConsiderando estos grupos de datos, podemos describir de las siguiente forma, por ejemplo:\n\npropinas |> \n   mutate(grupo = cut(cuenta_total,  breaks = c(0, 13, 17, 28, 32))) |> \n   filter(grupo %in% c(\"(13,17]\", \"(28,32]\")) |> \n   group_by(grupo) |> \n   summarise(\n      n = n(),\n      q10 = quantile(propina, 0.10),\n      mediana = quantile(propina, 0.5),\n      q90 = quantile(propina, 0.90),\n      rango_cuartiles = quantile(propina, 0.75) - quantile(propina, 0.25)) |> \n   kable(digits = 2)  |> \n  kable_paper(full_width = FALSE)\n\n\n\n \n  \n    grupo \n    n \n    q10 \n    mediana \n    q90 \n    rango_cuartiles \n  \n \n\n  \n    (13,17] \n    57 \n    1.85 \n    2.47 \n    3.49 \n    1.0 \n  \n  \n    (28,32] \n    16 \n    2.02 \n    3.69 \n    5.76 \n    2.2 \n  \n\n\n\n\n\nConde confirmamos que el nivel general de propinas es más alto alrededor de cuentas de total 30 que de total 15, y la dispersión también es mayor. Podríamos hacer un diagrama de caja y brazos también."
  },
  {
    "objectID": "numericos.html#suavizadores-locales",
    "href": "numericos.html#suavizadores-locales",
    "title": "1  Datos numéricos",
    "section": "1.6 Suavizadores locales",
    "text": "1.6 Suavizadores locales\nEl enfoque del ejemplo anterior puede ayudar en algunos casos nuestra tarea descriptiva, pero quisiéramos tener un método más general y completo para entender cómo es una respuesta numérica cuando el factor es también numérico.\nEn este caso, podemos hacer por ejemplo medias o medianas locales. La idea general es, en términos de nuestro ejemplo de propinas:\n\nQueremos producir un resumen en un valor de cuenta total \\(x\\).\nConsideramos valores de propina asociados a cuentas totales en un intervalo \\([x-e, x+e]\\).\nCalculamos estadísticas resumen en este rango para la respuesta\nUsualmente también ponderamos más alto valores que están cerca de \\(x\\) y ponderamos menos valores más lejanos a \\(x\\)\n\nEste tipo de suavizadores se llaman a veces suavizadores loess (ver (Cleveland 1993)).\nPor ejemplo,\n\nggplot(propinas, aes(x = cuenta_total, y = propina)) +\n   geom_ribbon(aes(xmin = 13, xmax = 17), fill = \"salmon\", alpha = 0.15) +\n   geom_ribbon(aes(xmin = 28, xmax = 32), fill = \"salmon\", alpha = 0.15) +\n   geom_point() +\n   geom_smooth(method = \"loess\", span = 0.5, \n               method.args = list(family = \"symmetric\", degree = 1), se = FALSE) \n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n# symmetric es un método robusto iterativo, que reduce el peso de atípicos\n\nEl parametro span controla el tamaño de la ventana de datos que se toma en cada punto. Nótese como alrededor de 15 y 30 los valores por donde pasa el suavizador son similares a las medianas que escribimos arriba.\nPodemos ajustar en cada ventana tambien rectas de minimos cuadrados, y obtener un suavizador de tipo lineal. En la siguiente gráfica mostramos cómo funciona este suavizador para distintos tamaños de ventanas (span)\n\n\n\nSuavizador loess\n\n\n\n\n\n\n\n\nTip\n\n\n\nLos suavizadores loess tienen como fin mostrar alrededor de qué valor de distribuye la respuesta (eje vertical) para distintos valores del factor (eje horizontal). Se escoge span suficientemente baja de forma que mostremos patrones claros en los datos y casi no capturemos variación debida a los tamaños de muestra chicos.\n\n\nEn la animación anterior, un valor de span de 0.15 funciona aprpiadamente, y uno de 0.05 es demasiado bajo y uno de 1.0 es demasiado alto. Es importante explorar con el valor de span pues depende de cuántos datos tenemos y cómo es su dispersión."
  },
  {
    "objectID": "numericos.html#regresión-cuantílica",
    "href": "numericos.html#regresión-cuantílica",
    "title": "1  Datos numéricos",
    "section": "1.7 Regresión cuantílica",
    "text": "1.7 Regresión cuantílica\nPodemos también mostrar estimaciones de medianas y cuantiles de la siguiente forma (nota: es necesario escoger lambda con cuidado, cuanto más alto sea lambda más suave es la curva obtenida):\n\nggplot(propinas, aes(x = cuenta_total, y = propina)) +\n   geom_ribbon(aes(xmin = 13, xmax = 17), fill = \"salmon\", alpha = 0.15) +\n   geom_ribbon(aes(xmin = 28, xmax = 32), fill = \"salmon\", alpha = 0.15) +\n   geom_point() +\n   geom_quantile(method = \"rqss\", lambda = 15, quantiles = c(0.10, 0.5, 0.90)) +\n   scale_y_continuous(breaks = seq(0, 10, 1)) +\n   xlab(\"Propina (dólares)\")\n\nSmoothing formula not specified. Using: y ~ qss(x, lambda = 15)\n\n\n\n\n\nFinalmente, entendimiento de los datos no permite también hacer gráficas más útiles. En este ejemplo particular podría por ejemplo calcular el porcentaje de la propina sobre la cuenta total:\n\npropinas <- propinas |> mutate(propinas, pct_propina = propina / cuenta_total) \npropinas_2 <- propinas |> \n  filter(pct_propina < 0.70)  |> \n  mutate(grupo = cut(num_personas, c(1, 3, 10), include.lowest = TRUE)) \nquantile(propinas_2 |> pull(pct_propina)) |> round(2)\n\n  0%  25%  50%  75% 100% \n0.04 0.13 0.15 0.19 0.42 \n\nggplot(propinas, aes(x = cuenta_total, y = pct_propina)) +\n   geom_point() +\n   scale_y_continuous(breaks = seq(0,1, 0.05)) +\n   geom_quantile(method = \"rqss\", \n                 lambda = 20, quantiles = c(0.10, 0.5, 0.90)) \n\n\n\n\nObserva que la descripción es más simple que si usamos propina cruda y cuenta.\n\nPara cuentas chicas, el porcentaje de propina puede ser muy alto (aún cuando la propina en sí no es tan grande):\n\n\nfilter(propinas, pct_propina > 0.30) |> \n  arrange(desc(pct_propina)) |> \n  kable(digits = 2)  |> \n  kable_paper(full_width = FALSE)\n\n\n\n \n  \n    cuenta_total \n    propina \n    fumador \n    dia \n    momento \n    num_personas \n    pct_propina \n  \n \n\n  \n    7.25 \n    5.15 \n    Si \n    Dom \n    Cena \n    2 \n    0.71 \n  \n  \n    9.60 \n    4.00 \n    Si \n    Dom \n    Cena \n    2 \n    0.42 \n  \n  \n    3.07 \n    1.00 \n    Si \n    Sab \n    Cena \n    1 \n    0.33 \n  \n\n\n\n\n\n\nPara cuentas relativamente chicas (10 dólares, el porcentaje de propina está por encima de 15%). Este porcentaje tiende a reducirse a valores 10% y 15% para cuentas más grandes.\nExiste variación considerable alrededor de estos valores centrales. El rango entre los deciles extremos es aproximadamente de 5 puntos porcentuales.\n\nO de manera más resumida:\n\nLa mediana de propinas está ligeramente por arriba de 15% para cuantas relativamente chicas. Esta mediana baja hasta alrededor de 10%-15% para cuentas más grandes (más de 40 dólares)\nLa mitad de las propinas no varía más de unos 5 puntos porcentuales alrededor de estas medianas.\nExisten propinas atípicas: algunas muy bajas de 1 dólar, muy por debajo del 15%, y ocasionalmente algunas muy altas en porcentaje. Estas últimas ocurren ocasinalmente especialmente en cuentas chicas (por ejemplo, una propina de 1 dólar en una cuenta de 3 dólares).\n\nSi dividimos por tamaño de grupo, vemos información adicional: la reducción proporcional de propina parece no ocurrir en grupos más grandes (más personas en la mesa), donde la mediana de propinas se mantiene en 15%.\n\npropinas <- propinas |> mutate(propinas, pct_propina = propina / cuenta_total) \npropinas_2 <- propinas |> \n  filter(pct_propina < 0.70)  |> \n  mutate(grupo = cut(num_personas, c(1, 3, 10), include.lowest = TRUE)) \nquantile(propinas_2 |> pull(pct_propina)) |> round(2)\n\n  0%  25%  50%  75% 100% \n0.04 0.13 0.15 0.19 0.42 \n\nggplot(propinas_2, aes(x = cuenta_total, y = pct_propina)) +\n   geom_point() +\n   scale_y_continuous(breaks = seq(0,1, 0.05)) +\n   geom_quantile(method = \"rqss\", \n                 lambda = 30, quantiles = c(0.10, 0.5, 0.90)) +\n  facet_wrap(~ grupo) \n\n\n\n\n\nEs razonable cortar por número de grupo, pues esta variable afecta tanto a la cuenta total (grupos mayores tienden a gastar más) como al porcentaje de propina (grupos mayores tienen que ponerse de acuerdo con la propina, la propina se divide en más personas, pueden tener más problemas con el servicio, etc).\n\nEste es otro ejemplo de una gráfica de este tipo (usando regresión cuantílica):\n\n\n\nCrecimiento de bebés\n\n\n\n\n\n\nCleveland, William S. 1993. Visualizing Data. Hobart Press."
  },
  {
    "objectID": "categoricos.html",
    "href": "categoricos.html",
    "title": "2  Datos categóricos",
    "section": "",
    "text": "En esta sección mostraremos cómo hacer distintos tipos de resúmenes para mediciones individuales. Consideraremos también el uso de estas descripciones para comparar distintos grupos (o bonches de datos, como les llamaba Tukey), aplicando repetidamente los mismos resúmenes a lo largo de esos distintos grupos."
  },
  {
    "objectID": "categoricos.html#datos-categóricos-y-tablas",
    "href": "categoricos.html#datos-categóricos-y-tablas",
    "title": "2  Datos categóricos",
    "section": "2.1 Datos categóricos y tablas",
    "text": "2.1 Datos categóricos y tablas\nUna medición categórica es una que toma sus valores posibles en un conjunto que no es numérico. Consideremos los siguiente datos de 300 tomadores de té (Lê, Josse, y Husson (2008)):\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(kableExtra)\n\n\n\n\n\n \n  \n    id \n    tipo \n    complementos \n    azucar \n    presentacion \n    precio \n    edad \n  \n \n\n  \n    201 \n    earl_grey \n    limón \n    con_azúcar \n    mixto \n    de_marca \n    25 \n  \n  \n    9 \n    earl_grey \n    leche \n    sin_azúcar \n    mixto \n    fino \n    40 \n  \n  \n    63 \n    earl_grey \n    solo \n    sin_azúcar \n    bolsa \n    variable \n    22 \n  \n  \n    252 \n    earl_grey \n    limón \n    con_azúcar \n    mixto \n    variable \n    24 \n  \n  \n    139 \n    earl_grey \n    solo \n    sin_azúcar \n    bolsa \n    de_marca \n    17 \n  \n  \n    133 \n    earl_grey \n    limón \n    con_azúcar \n    mixto \n    variable \n    29 \n  \n  \n    11 \n    earl_grey \n    solo \n    sin_azúcar \n    bolsa \n    no_sabe \n    32 \n  \n  \n    282 \n    earl_grey \n    solo \n    con_azúcar \n    bolsa \n    variable \n    22 \n  \n  \n    181 \n    negro \n    leche \n    con_azúcar \n    bolsa \n    no_sabe \n    45 \n  \n  \n    13 \n    earl_grey \n    leche \n    sin_azúcar \n    mixto \n    no_sabe \n    56 \n  \n\n\n\n\n\nMediciones como tipo, presentación o azucar son variables categóricas. Desde el punto de vista univariado, generalmente no es necesario resumir, sino simplemente agrupar y contar cuántas veces ocurre cada categoría. Por ejemplo\n\ntabla_1 <- te_tbl |> count(tipo) |> \n   arrange(desc(n))\ntabla_1 |> kable() |> kable_paper(full_width = FALSE)\n\n\n\n \n  \n    tipo \n    n \n  \n \n\n  \n    earl_grey \n    193 \n  \n  \n    negro \n    74 \n  \n  \n    verde \n    33 \n  \n\n\n\n\n\nUsualmente es más útil reportar la porporción o porcentaje de casos por categoría\n\ntabla_2 <- te_tbl |> \n   count(tipo) |> \n   mutate(n_total = sum(n), prop = n / n_total) |> \n   select(tipo, n_total, prop) |> \n   mutate(across(where(is.numeric), round, 2)) |> \n   arrange(desc(prop))\ntabla_2 |> kable() |> kable_paper(full_width = FALSE)\n\n\n\n \n  \n    tipo \n    n_total \n    prop \n  \n \n\n  \n    earl_grey \n    300 \n    0.64 \n  \n  \n    negro \n    300 \n    0.25 \n  \n  \n    verde \n    300 \n    0.11 \n  \n\n\n\n\n\nPodemos hacer varias variables juntas de la siguiente manera:\n\nperfiles_col_tbl <- te_tbl |> select(id, tipo, complementos, presentacion, azucar) |> \n   pivot_longer(cols = tipo:azucar, names_to = \"variable\", values_to = \"valor\") |> \n   count(variable, valor) |> \n   group_by(variable) |> \n   mutate(n_total = sum(n), prop = n / n_total) |>\n   mutate(prop = round(prop, 2)) |> \n   arrange(desc(prop), .by_group = TRUE)\nperfiles_col_tbl |> kable() |> kable_paper(full_width = FALSE)\n\n\n\n \n  \n    variable \n    valor \n    n \n    n_total \n    prop \n  \n \n\n  \n    azucar \n    sin_azúcar \n    155 \n    300 \n    0.52 \n  \n  \n    azucar \n    con_azúcar \n    145 \n    300 \n    0.48 \n  \n  \n    complementos \n    solo \n    195 \n    300 \n    0.65 \n  \n  \n    complementos \n    leche \n    63 \n    300 \n    0.21 \n  \n  \n    complementos \n    limón \n    33 \n    300 \n    0.11 \n  \n  \n    complementos \n    otros \n    9 \n    300 \n    0.03 \n  \n  \n    presentacion \n    bolsa \n    170 \n    300 \n    0.57 \n  \n  \n    presentacion \n    mixto \n    94 \n    300 \n    0.31 \n  \n  \n    presentacion \n    suelto \n    36 \n    300 \n    0.12 \n  \n  \n    tipo \n    earl_grey \n    193 \n    300 \n    0.64 \n  \n  \n    tipo \n    negro \n    74 \n    300 \n    0.25 \n  \n  \n    tipo \n    verde \n    33 \n    300 \n    0.11 \n  \n\n\n\n\n\nPara leer más fácil, imprimimos individualmente estas tablas, o hacemos algo como lo que sigue para mostrarlas todas juntas:\n\nperfiles_col_tbl |> \n   ungroup() |> \n   select(-variable, -n_total) |> \n   kable() |>  \n   pack_rows(index = table(perfiles_col_tbl$variable)) |> \n   kable_paper(full_width = FALSE)\n\n\n\n \n  \n    valor \n    n \n    prop \n  \n \n\n  azucar\n\n    sin_azúcar \n    155 \n    0.52 \n  \n  \n    con_azúcar \n    145 \n    0.48 \n  \n  complementos\n\n    solo \n    195 \n    0.65 \n  \n  \n    leche \n    63 \n    0.21 \n  \n  \n    limón \n    33 \n    0.11 \n  \n  \n    otros \n    9 \n    0.03 \n  \n  presentacion\n\n    bolsa \n    170 \n    0.57 \n  \n  \n    mixto \n    94 \n    0.31 \n  \n  \n    suelto \n    36 \n    0.12 \n  \n  tipo\n\n    earl_grey \n    193 \n    0.64 \n  \n  \n    negro \n    74 \n    0.25 \n  \n  \n    verde \n    33 \n    0.11"
  },
  {
    "objectID": "categoricos.html#comparando-grupos-con-variables-categóricas",
    "href": "categoricos.html#comparando-grupos-con-variables-categóricas",
    "title": "2  Datos categóricos",
    "section": "2.2 Comparando grupos con variables categóricas",
    "text": "2.2 Comparando grupos con variables categóricas\nEste análisis generalmente es más interesante cuando comparamos grupos. Supongamos que nos interesa ver si existe una relación entre usar el tipo de té que toman estas personas y el uso de complementos como leche o limón. Naturalmente, escogeríamos dividir por tipo de té, y ver qué complementos se usan con cada uno:\n\nperfiles_col_tbl <- te_tbl |> count(complementos, tipo) |> \n   group_by(tipo) |> \n   mutate(prop = n / sum(n)) |>\n   group_by(complementos) |> \n   select(-n) |> \n   pivot_wider(names_from = tipo, values_from = prop, values_fill = 0)\nperfiles_col_tbl |>  kable(digits = 2, caption = \"Perfiles por columna\") |> \n  kable_paper(full_width = FALSE)\n\n\n\nPerfiles por columna\n \n  \n    complementos \n    earl_grey \n    negro \n    verde \n  \n \n\n  \n    leche \n    0.20 \n    0.26 \n    0.18 \n  \n  \n    limón \n    0.12 \n    0.09 \n    0.06 \n  \n  \n    otros \n    0.02 \n    0.08 \n    0.00 \n  \n  \n    solo \n    0.66 \n    0.57 \n    0.76 \n  \n\n\n\n\n\nComparando los perfiles de las columnas observamos variaciones interesantes: por ejemplo, los tomadores de Earl Grey tienden a usar más limón como complemento que otros grupos. Son resúmenes univariados que ahora comparamos a lo largo de grupos. Podemos hacer las comparaciones más simples si hacemos todas contra una columna marginal del uso general en la muestra de los distintos complementos\n\ncomp_tbl <- te_tbl |> count(complementos) |> mutate(total = n / sum(n))\nperfiles_col_tbl <- left_join(perfiles_col_tbl, comp_tbl) |> \n      arrange(desc(total)) |> \n      select(-n)\nperfiles_col_tbl |> kable(digits = 2) |> \n  kable_paper(full_width = FALSE)\n\n\n\n \n  \n    complementos \n    earl_grey \n    negro \n    verde \n    total \n  \n \n\n  \n    solo \n    0.66 \n    0.57 \n    0.76 \n    0.65 \n  \n  \n    leche \n    0.20 \n    0.26 \n    0.18 \n    0.21 \n  \n  \n    limón \n    0.12 \n    0.09 \n    0.06 \n    0.11 \n  \n  \n    otros \n    0.02 \n    0.08 \n    0.00 \n    0.03 \n  \n\n\n\n\n\nSi el tipo de té no influyera en los complementos, verías porcentajes muy similares en cada columna, pero vemos que hay coincidencias y diferencias entre los grupos de tomadores de té. Podemos expresar esto de manera simple calculando índices contra la columna de total:\n\nres_tbl <- perfiles_col_tbl |> \n   mutate(across(where(is.numeric), ~ .x / total)) |> \n   select(-total)\nres_tbl |> kable(digits = 2) |> \n  kable_paper(full_width = FALSE)\n\n\n\n \n  \n    complementos \n    earl_grey \n    negro \n    verde \n  \n \n\n  \n    solo \n    1.02 \n    0.87 \n    1.17 \n  \n  \n    leche \n    0.94 \n    1.22 \n    0.87 \n  \n  \n    limón \n    1.13 \n    0.86 \n    0.55 \n  \n  \n    otros \n    0.52 \n    2.70 \n    0.00 \n  \n\n\n\n\n\nValores por encima de 1 indican columnas por arriba de la población general, y análogamente para valores por debajo de uno. Estas cantidades pueden escribirse en términos porcentuales, o se les puede restar 1 para terminar como una variación porcentual del promedio. A estas cantidades se les llama residuales crudos:\n\nres_tbl <- perfiles_col_tbl |> \n   mutate(across(where(is.numeric) & !total, ~ .x / total - 1)) \nres_tbl |> kable(digits = 2) |> \n  kable_paper(full_width = FALSE)\n\n\n\n \n  \n    complementos \n    earl_grey \n    negro \n    verde \n    total \n  \n \n\n  \n    solo \n    0.02 \n    -0.13 \n    0.17 \n    0.65 \n  \n  \n    leche \n    -0.06 \n    0.22 \n    -0.13 \n    0.21 \n  \n  \n    limón \n    0.13 \n    -0.14 \n    -0.45 \n    0.11 \n  \n  \n    otros \n    -0.48 \n    1.70 \n    -1.00 \n    0.03 \n  \n\n\n\n\n\nPodemos finalmente marcar la tabla:\n\nres_tbl |>  mutate(across(where(is.numeric), round, 2)) |> \n   mutate(across(where(is.numeric) & ! total, \n                 ~ cell_spec(.x, color = ifelse(.x > 0.1, \"black\", \n                                         ifelse(.x < -0.1, \"red\", \"gray\"))))) |>\n   arrange(desc(total)) |> \n   kable(escape = FALSE, align = \"r\") |> \n   kable_paper(full_width = FALSE)\n\n\n\n \n  \n    complementos \n    earl_grey \n    negro \n    verde \n    total \n  \n \n\n  \n    solo \n    0.02 \n    -0.13 \n    0.17 \n    0.65 \n  \n  \n    leche \n    -0.06 \n    0.22 \n    -0.13 \n    0.21 \n  \n  \n    limón \n    0.13 \n    -0.14 \n    -0.45 \n    0.11 \n  \n  \n    otros \n    -0.48 \n    1.7 \n    -1 \n    0.03 \n  \n\n\n\n\n\n\n\n\n\n\n\nPerfiles\n\n\n\nA este tipo de análisis de tablas cruzadas a veces se le llama análisis de perfiles columna. Nos permite entender cómo varía la distribución de la variable de los renglones según el grupo indicado por la columna.\n\nDesviaciones grandes en los residuales indican asociaciones fuertes entre la variable de los reglones y de las columnas\nRecordemos que este análisis aplica a la muestra de datos que tenemos. Columnas con pocos individuos tienden a mostrar más variación y debemos ser cuidadosos al generalizar.\n\n\n\nPodemos incluir también totales para ayudarnos a juzgar las variaciones:\n\n\n\n\n \n  \n    complementos \n    earl_grey \n    negro \n    verde \n    total \n  \n \n\n  \n     \n    193 \n    74 \n    33 \n    1.00 \n  \n  \n    solo \n    0.02 \n    -0.13 \n    0.17 \n    0.65 \n  \n  \n    leche \n    -0.06 \n    0.22 \n    -0.13 \n    0.21 \n  \n  \n    limón \n    0.13 \n    -0.14 \n    -0.45 \n    0.11 \n  \n  \n    otros \n    -0.48 \n    1.7 \n    -1 \n    0.03"
  },
  {
    "objectID": "categoricos.html#observación-perfiles-renglón-y-columna",
    "href": "categoricos.html#observación-perfiles-renglón-y-columna",
    "title": "2  Datos categóricos",
    "section": "2.3 Observación: perfiles renglón y columna",
    "text": "2.3 Observación: perfiles renglón y columna\nEl análisis también lo podemos hacer con los perfiles de los renglones. Los residuales crudos que usamos para interpretar son los mismos. La razón es la siguiente:\nPara los perfiles columna, si escribimos \\(n_{+j}\\) como los totales por columna, y \\(n_{i+}\\) los totales por renglón, tenemos que los perfiles columna son: \\[c_{i,j} = \\frac{n_{i,j}}{n_{+j}}\\] Escribimos también \\(c_i = \\frac{n_{i+}}{n}\\) y \\(r_j = \\frac{n_{+j}}{n}\\) como los porcentajes marginales por columna y por renglón respectivamente.\nLos residuales son entonces \\[r_{i,j} = \\frac{\\frac{n_{i,j}}{n_{+,j}}} { \\frac{n_{i,+}}{n}} - 1 = \\frac{p_{i,j} - r_ic_j}{r_ic_j}\\] Nótese que no importa entonces cómo comencemos el cálculo, por renglones o por columnas, el resultado es el mismo.\n\nDiscute qué sentido tiene comparar \\(p_{i,j}\\) contra \\(r_ic_j\\). ¿Qué interpretación tiene esta última cantidad?"
  },
  {
    "objectID": "categoricos.html#visualización-de-tablas-cruzadas",
    "href": "categoricos.html#visualización-de-tablas-cruzadas",
    "title": "2  Datos categóricos",
    "section": "2.4 Visualización de tablas cruzadas",
    "text": "2.4 Visualización de tablas cruzadas\nPara tablas más grandes, muchas veces las técnicas que mostramos arriba no son suficientes para entender y presentar patrones importantes en los datos. En estos casos, buscamos reducir la dimensionalidad de los datos para poder presentarlos en una gráfica de dos dimensiones.\nPodemos utilizar análisis de correspondencias. A grandes rasgos (ver (Izenman 2009) para los detalles) buscamos una representación tal que:\n\nCada categoría de las columnas está representada por una flecha que sale del origen de nuestra gráfica\nCada categoría de los renglones está representada por un punto en nuestra gráfica\nSi proyectamos los puntos (renglones) sobre las direcciones de las columnas, entonces el tamaño de la proyección es lo más cercano posible (en dos dimensiones) al residual correspondiente de las tablas del análisis mostrado arriba.\n\nPara construir esta gráfica, entonces, existe un proceso de optimización que busca representar lo más fielmente los residuales del análisis mostrado arriba en dos dimensiones, y de esta forma buscamos recuperar una buena parte de la información de los residuales de una manera más compacta."
  },
  {
    "objectID": "categoricos.html#ejemplo-tés-y-complementos",
    "href": "categoricos.html#ejemplo-tés-y-complementos",
    "title": "2  Datos categóricos",
    "section": "2.5 Ejemplo: tés y complementos",
    "text": "2.5 Ejemplo: tés y complementos\n\nlibrary(ca)\ncorr_te <- ca(table(te_tbl$complementos, te_tbl$tipo))\nplot(corr_te, map = \"rowprincipal\", arrows = c(FALSE, TRUE))\n\n\n\n\nLa contribución de cada dimensión a la aproximación se indica en los ejes. Como vemos en la gráfica, y la suma de las contribuciones nos da la calidad de la representación, que en este caso es perfecta.\n\n\n\n\n\n\nTip\n\n\n\n\nEl análisis de correspondencias es un tema relativamente avanzado de estadística multivariada, y su definición precisa requiere de matemáticas más avanzadas (por ejemplo la descomposición en valores singulares).\nCualquier hallazgo obtenido en este tipo de análisis debe ser verificado en las tablas correspondientes de perfiles\nHay distintos tipos de gráficas (biplots) asociadas al análisis de correspondencias, que privilegian representar mejor a distintos tipos de características de los datos"
  },
  {
    "objectID": "categoricos.html#ejemplo-robo-en-tiendas",
    "href": "categoricos.html#ejemplo-robo-en-tiendas",
    "title": "2  Datos categóricos",
    "section": "2.6 Ejemplo: robo en tiendas",
    "text": "2.6 Ejemplo: robo en tiendas\nConsideramos los siguientes datos de robos en tiendas en Holanda por personas de distintas edades y genéros (Izenman (2009)). En este caso, las variables ya están cruzadas:\n\nhurto_tbl <- read_csv(\"./datos/hurto.csv\") |> \n   mutate(grupo = ifelse(grupo == \"-12 h\", \"01-12 h\", grupo),\n          grupo = ifelse(grupo == \"-12 m\", \"01-12 m\", grupo)) |> \n   pivot_longer(ropa:otros, names_to = \"departamento\", values_to = \"n\") |> \n   pivot_wider(names_from = grupo, values_from = n)\n\nRows: 18 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): grupo\ndbl (13): ropa, accesorios, tabaco, escritura, libros, discos, bienes, dulce...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhurto_tbl |> kable() |> \n   kable_paper(full_width = FALSE)\n\n\n\n \n  \n    departamento \n    01-12 h \n    12-14 h \n    15-17 h \n    18-20 h \n    21-29 h \n    30-39 h \n    40-49 h \n    50-64 h \n    64+ h \n    01-12 m \n    12-14 m \n    15-17 m \n    18-20 m \n    21-29 m \n    30-39 m \n    40-49 m \n    50-64 m \n    64+ m \n  \n \n\n  \n    ropa \n    81 \n    138 \n    304 \n    384 \n    942 \n    359 \n    178 \n    137 \n    45 \n    71 \n    241 \n    477 \n    436 \n    1180 \n    1009 \n    517 \n    488 \n    173 \n  \n  \n    accesorios \n    66 \n    204 \n    193 \n    149 \n    297 \n    109 \n    53 \n    68 \n    28 \n    19 \n    98 \n    114 \n    108 \n    207 \n    165 \n    102 \n    127 \n    64 \n  \n  \n    tabaco \n    150 \n    340 \n    229 \n    151 \n    313 \n    136 \n    121 \n    171 \n    145 \n    59 \n    111 \n    58 \n    76 \n    132 \n    121 \n    93 \n    214 \n    215 \n  \n  \n    escritura \n    667 \n    1409 \n    527 \n    84 \n    92 \n    36 \n    36 \n    37 \n    17 \n    224 \n    346 \n    91 \n    18 \n    30 \n    27 \n    23 \n    27 \n    13 \n  \n  \n    libros \n    67 \n    259 \n    258 \n    146 \n    251 \n    96 \n    48 \n    56 \n    41 \n    19 \n    60 \n    50 \n    32 \n    61 \n    43 \n    31 \n    57 \n    44 \n  \n  \n    discos \n    24 \n    272 \n    368 \n    141 \n    167 \n    67 \n    29 \n    27 \n    7 \n    7 \n    32 \n    27 \n    12 \n    21 \n    9 \n    7 \n    13 \n    0 \n  \n  \n    bienes \n    47 \n    117 \n    98 \n    61 \n    193 \n    75 \n    50 \n    55 \n    29 \n    22 \n    29 \n    41 \n    32 \n    65 \n    74 \n    51 \n    79 \n    39 \n  \n  \n    dulces \n    430 \n    637 \n    246 \n    40 \n    30 \n    11 \n    5 \n    17 \n    28 \n    137 \n    240 \n    80 \n    12 \n    16 \n    14 \n    10 \n    23 \n    42 \n  \n  \n    juguetes \n    743 \n    684 \n    116 \n    13 \n    16 \n    16 \n    6 \n    3 \n    8 \n    113 \n    98 \n    14 \n    10 \n    12 \n    31 \n    8 \n    17 \n    6 \n  \n  \n    joyería \n    132 \n    408 \n    298 \n    71 \n    130 \n    31 \n    14 \n    11 \n    10 \n    162 \n    548 \n    303 \n    74 \n    100 \n    48 \n    22 \n    26 \n    12 \n  \n  \n    perfumes \n    32 \n    57 \n    61 \n    52 \n    111 \n    54 \n    41 \n    50 \n    28 \n    70 \n    178 \n    141 \n    70 \n    104 \n    81 \n    46 \n    69 \n    41 \n  \n  \n    herramientas \n    197 \n    547 \n    402 \n    138 \n    280 \n    200 \n    152 \n    211 \n    111 \n    15 \n    29 \n    9 \n    14 \n    30 \n    36 \n    24 \n    35 \n    11 \n  \n  \n    otros \n    209 \n    550 \n    454 \n    252 \n    624 \n    195 \n    88 \n    90 \n    34 \n    24 \n    58 \n    72 \n    67 \n    157 \n    107 \n    66 \n    64 \n    55 \n  \n\n\n\n\n\nEsta tabla es más grande y difícil de entender tal cual está. Comenzamos por examinar las marginales:\n\ndepartamentos_tbl <- hurto_tbl |> \n   pivot_longer(cols = -departamento, names_to = \"grupo\", values_to = \"n\") |> \n   group_by(departamento) |> \n   summarise(n = sum(n)) |> \n   mutate(prop = n / sum(n)) |> \n   arrange(desc(prop))\ndepartamentos_tbl |> \n   kable(digits = 2) |> \n   kable_paper(full_width = FALSE)\n\n\n\n \n  \n    departamento \n    n \n    prop \n  \n \n\n  \n    ropa \n    7160 \n    0.22 \n  \n  \n    escritura \n    3704 \n    0.11 \n  \n  \n    otros \n    3166 \n    0.10 \n  \n  \n    tabaco \n    2835 \n    0.09 \n  \n  \n    herramientas \n    2441 \n    0.07 \n  \n  \n    joyería \n    2400 \n    0.07 \n  \n  \n    accesorios \n    2171 \n    0.07 \n  \n  \n    dulces \n    2018 \n    0.06 \n  \n  \n    juguetes \n    1914 \n    0.06 \n  \n  \n    libros \n    1619 \n    0.05 \n  \n  \n    perfumes \n    1286 \n    0.04 \n  \n  \n    discos \n    1230 \n    0.04 \n  \n  \n    bienes \n    1157 \n    0.03 \n  \n\n\n\n\n\n\ngrupos_tbl <- hurto_tbl |> \n   pivot_longer(cols = -departamento, names_to = \"grupo\", values_to = \"n\") |> \n   group_by(grupo) |> \n   summarise(n = sum(n)) |> \n   mutate(prop = n / sum(n)) |> \n   arrange(desc(prop))\ngrupos_tbl |> kable(digits = 2) |> \n   kable_paper(full_width = FALSE)\n\n\n\n \n  \n    grupo \n    n \n    prop \n  \n \n\n  \n    12-14 h \n    5622 \n    0.17 \n  \n  \n    15-17 h \n    3554 \n    0.11 \n  \n  \n    21-29 h \n    3446 \n    0.10 \n  \n  \n    01-12 h \n    2845 \n    0.09 \n  \n  \n    21-29 m \n    2115 \n    0.06 \n  \n  \n    12-14 m \n    2068 \n    0.06 \n  \n  \n    30-39 m \n    1765 \n    0.05 \n  \n  \n    18-20 h \n    1682 \n    0.05 \n  \n  \n    15-17 m \n    1477 \n    0.04 \n  \n  \n    30-39 h \n    1385 \n    0.04 \n  \n  \n    50-64 m \n    1239 \n    0.04 \n  \n  \n    40-49 m \n    1000 \n    0.03 \n  \n  \n    18-20 m \n    961 \n    0.03 \n  \n  \n    01-12 m \n    942 \n    0.03 \n  \n  \n    50-64 h \n    933 \n    0.03 \n  \n  \n    40-49 h \n    821 \n    0.02 \n  \n  \n    64+ m \n    715 \n    0.02 \n  \n  \n    64+ h \n    531 \n    0.02 \n  \n\n\n\n\n\nIntentamos análisis de correspondencias para comparar los perfiles columna:\n\nhurto_df <- as.data.frame(hurto_tbl)\nrownames(hurto_df) <- hurto_tbl$departamento\nhurto_df$departamento <- NULL\ncorr_hurto <- ca(hurto_df)\ngrafica_datos <- plot(corr_hurto, map = \"rowprincipal\", arrows = c(FALSE, TRUE))\n\n\n\n\n\nSegún esta gráfica, ¿qué categorias de productos están sobrerrepresentadas en cada grupo de edad? ¿Cómo tendrían que verse el análisis de perfiles columna?\n\nComo se aprecia, en la siguiente tabla, es difícil entender los patrones generales en los datos:\n\nperfiles_hurto_tbl <- hurto_tbl |> \n   pivot_longer(cols = -departamento, names_to = \"producto\", values_to = \"n\") |> \n   group_by(producto) |> \n   mutate(prop = n / sum(n)) |> \n   select(-n) |> \n   pivot_wider(names_from = producto, values_from = prop) \nperfiles_hurto_tbl |> \n   kable(digits = 2) |> \n   kable_styling(font_size = 12) |> \n   kable_paper()\n\n\n\n \n  \n    departamento \n    01-12 h \n    12-14 h \n    15-17 h \n    18-20 h \n    21-29 h \n    30-39 h \n    40-49 h \n    50-64 h \n    64+ h \n    01-12 m \n    12-14 m \n    15-17 m \n    18-20 m \n    21-29 m \n    30-39 m \n    40-49 m \n    50-64 m \n    64+ m \n  \n \n\n  \n    ropa \n    0.03 \n    0.02 \n    0.09 \n    0.23 \n    0.27 \n    0.26 \n    0.22 \n    0.15 \n    0.08 \n    0.08 \n    0.12 \n    0.32 \n    0.45 \n    0.56 \n    0.57 \n    0.52 \n    0.39 \n    0.24 \n  \n  \n    accesorios \n    0.02 \n    0.04 \n    0.05 \n    0.09 \n    0.09 \n    0.08 \n    0.06 \n    0.07 \n    0.05 \n    0.02 \n    0.05 \n    0.08 \n    0.11 \n    0.10 \n    0.09 \n    0.10 \n    0.10 \n    0.09 \n  \n  \n    tabaco \n    0.05 \n    0.06 \n    0.06 \n    0.09 \n    0.09 \n    0.10 \n    0.15 \n    0.18 \n    0.27 \n    0.06 \n    0.05 \n    0.04 \n    0.08 \n    0.06 \n    0.07 \n    0.09 \n    0.17 \n    0.30 \n  \n  \n    escritura \n    0.23 \n    0.25 \n    0.15 \n    0.05 \n    0.03 \n    0.03 \n    0.04 \n    0.04 \n    0.03 \n    0.24 \n    0.17 \n    0.06 \n    0.02 \n    0.01 \n    0.02 \n    0.02 \n    0.02 \n    0.02 \n  \n  \n    libros \n    0.02 \n    0.05 \n    0.07 \n    0.09 \n    0.07 \n    0.07 \n    0.06 \n    0.06 \n    0.08 \n    0.02 \n    0.03 \n    0.03 \n    0.03 \n    0.03 \n    0.02 \n    0.03 \n    0.05 \n    0.06 \n  \n  \n    discos \n    0.01 \n    0.05 \n    0.10 \n    0.08 \n    0.05 \n    0.05 \n    0.04 \n    0.03 \n    0.01 \n    0.01 \n    0.02 \n    0.02 \n    0.01 \n    0.01 \n    0.01 \n    0.01 \n    0.01 \n    0.00 \n  \n  \n    bienes \n    0.02 \n    0.02 \n    0.03 \n    0.04 \n    0.06 \n    0.05 \n    0.06 \n    0.06 \n    0.05 \n    0.02 \n    0.01 \n    0.03 \n    0.03 \n    0.03 \n    0.04 \n    0.05 \n    0.06 \n    0.05 \n  \n  \n    dulces \n    0.15 \n    0.11 \n    0.07 \n    0.02 \n    0.01 \n    0.01 \n    0.01 \n    0.02 \n    0.05 \n    0.15 \n    0.12 \n    0.05 \n    0.01 \n    0.01 \n    0.01 \n    0.01 \n    0.02 \n    0.06 \n  \n  \n    juguetes \n    0.26 \n    0.12 \n    0.03 \n    0.01 \n    0.00 \n    0.01 \n    0.01 \n    0.00 \n    0.02 \n    0.12 \n    0.05 \n    0.01 \n    0.01 \n    0.01 \n    0.02 \n    0.01 \n    0.01 \n    0.01 \n  \n  \n    joyería \n    0.05 \n    0.07 \n    0.08 \n    0.04 \n    0.04 \n    0.02 \n    0.02 \n    0.01 \n    0.02 \n    0.17 \n    0.26 \n    0.21 \n    0.08 \n    0.05 \n    0.03 \n    0.02 \n    0.02 \n    0.02 \n  \n  \n    perfumes \n    0.01 \n    0.01 \n    0.02 \n    0.03 \n    0.03 \n    0.04 \n    0.05 \n    0.05 \n    0.05 \n    0.07 \n    0.09 \n    0.10 \n    0.07 \n    0.05 \n    0.05 \n    0.05 \n    0.06 \n    0.06 \n  \n  \n    herramientas \n    0.07 \n    0.10 \n    0.11 \n    0.08 \n    0.08 \n    0.14 \n    0.19 \n    0.23 \n    0.21 \n    0.02 \n    0.01 \n    0.01 \n    0.01 \n    0.01 \n    0.02 \n    0.02 \n    0.03 \n    0.02 \n  \n  \n    otros \n    0.07 \n    0.10 \n    0.13 \n    0.15 \n    0.18 \n    0.14 \n    0.11 \n    0.10 \n    0.06 \n    0.03 \n    0.03 \n    0.05 \n    0.07 \n    0.07 \n    0.06 \n    0.07 \n    0.05 \n    0.08 \n  \n\n\n\n\n\n\nres_hurto_tbl <- left_join(perfiles_hurto_tbl, departamentos_tbl |> rename(total = prop)) |> \n    select(-n) |> \n    #select(-bienes, -discos, -perfumes) |> \n    mutate(across(where(is.numeric) & !total, ~ .x / total - 1)) |> \n    mutate(across(where(is.numeric), round, 2)) \n\nJoining, by = \"departamento\"\n\nres_hurto_tbl |> \n    mutate(across(where(is.numeric) & ! total, \n                 ~ cell_spec(.x, color = ifelse(.x > 0.2, \"black\", \n                                         ifelse(.x < -0.2, \"red\", \"gray\"))))) |>\n    select(-total) |> \n    kable(escape = FALSE) |>\n    kable_styling(font_size = 12) |> \n    kable_paper()\n\n\n\n \n  \n    departamento \n    01-12 h \n    12-14 h \n    15-17 h \n    18-20 h \n    21-29 h \n    30-39 h \n    40-49 h \n    50-64 h \n    64+ h \n    01-12 m \n    12-14 m \n    15-17 m \n    18-20 m \n    21-29 m \n    30-39 m \n    40-49 m \n    50-64 m \n    64+ m \n  \n \n\n  \n    ropa \n    -0.87 \n    -0.89 \n    -0.6 \n    0.06 \n    0.26 \n    0.2 \n    0 \n    -0.32 \n    -0.61 \n    -0.65 \n    -0.46 \n    0.49 \n    1.1 \n    1.58 \n    1.64 \n    1.39 \n    0.82 \n    0.12 \n  \n  \n    accesorios \n    -0.65 \n    -0.45 \n    -0.17 \n    0.35 \n    0.31 \n    0.2 \n    -0.02 \n    0.11 \n    -0.2 \n    -0.69 \n    -0.28 \n    0.18 \n    0.71 \n    0.49 \n    0.43 \n    0.56 \n    0.56 \n    0.36 \n  \n  \n    tabaco \n    -0.38 \n    -0.29 \n    -0.25 \n    0.05 \n    0.06 \n    0.15 \n    0.72 \n    1.14 \n    2.19 \n    -0.27 \n    -0.37 \n    -0.54 \n    -0.08 \n    -0.27 \n    -0.2 \n    0.09 \n    1.02 \n    2.51 \n  \n  \n    escritura \n    1.1 \n    1.24 \n    0.33 \n    -0.55 \n    -0.76 \n    -0.77 \n    -0.61 \n    -0.65 \n    -0.71 \n    1.13 \n    0.5 \n    -0.45 \n    -0.83 \n    -0.87 \n    -0.86 \n    -0.79 \n    -0.81 \n    -0.84 \n  \n  \n    libros \n    -0.52 \n    -0.06 \n    0.48 \n    0.77 \n    0.49 \n    0.42 \n    0.2 \n    0.23 \n    0.58 \n    -0.59 \n    -0.41 \n    -0.31 \n    -0.32 \n    -0.41 \n    -0.5 \n    -0.37 \n    -0.06 \n    0.26 \n  \n  \n    discos \n    -0.77 \n    0.3 \n    1.79 \n    1.26 \n    0.3 \n    0.3 \n    -0.05 \n    -0.22 \n    -0.65 \n    -0.8 \n    -0.58 \n    -0.51 \n    -0.66 \n    -0.73 \n    -0.86 \n    -0.81 \n    -0.72 \n    -1 \n  \n  \n    bienes \n    -0.53 \n    -0.4 \n    -0.21 \n    0.04 \n    0.6 \n    0.55 \n    0.74 \n    0.69 \n    0.56 \n    -0.33 \n    -0.6 \n    -0.21 \n    -0.05 \n    -0.12 \n    0.2 \n    0.46 \n    0.82 \n    0.56 \n  \n  \n    dulces \n    1.48 \n    0.86 \n    0.14 \n    -0.61 \n    -0.86 \n    -0.87 \n    -0.9 \n    -0.7 \n    -0.14 \n    1.39 \n    0.9 \n    -0.11 \n    -0.8 \n    -0.88 \n    -0.87 \n    -0.84 \n    -0.7 \n    -0.04 \n  \n  \n    juguetes \n    3.52 \n    1.1 \n    -0.44 \n    -0.87 \n    -0.92 \n    -0.8 \n    -0.87 \n    -0.94 \n    -0.74 \n    1.07 \n    -0.18 \n    -0.84 \n    -0.82 \n    -0.9 \n    -0.7 \n    -0.86 \n    -0.76 \n    -0.85 \n  \n  \n    joyería \n    -0.36 \n    0 \n    0.16 \n    -0.42 \n    -0.48 \n    -0.69 \n    -0.76 \n    -0.84 \n    -0.74 \n    1.37 \n    2.65 \n    1.83 \n    0.06 \n    -0.35 \n    -0.62 \n    -0.7 \n    -0.71 \n    -0.77 \n  \n  \n    perfumes \n    -0.71 \n    -0.74 \n    -0.56 \n    -0.2 \n    -0.17 \n    0 \n    0.29 \n    0.38 \n    0.36 \n    0.91 \n    1.22 \n    1.46 \n    0.87 \n    0.27 \n    0.18 \n    0.18 \n    0.43 \n    0.48 \n  \n  \n    herramientas \n    -0.06 \n    0.32 \n    0.53 \n    0.11 \n    0.1 \n    0.96 \n    1.51 \n    2.07 \n    1.83 \n    -0.78 \n    -0.81 \n    -0.92 \n    -0.8 \n    -0.81 \n    -0.72 \n    -0.67 \n    -0.62 \n    -0.79 \n  \n  \n    otros \n    -0.23 \n    0.02 \n    0.34 \n    0.57 \n    0.89 \n    0.47 \n    0.12 \n    0.01 \n    -0.33 \n    -0.73 \n    -0.71 \n    -0.49 \n    -0.27 \n    -0.22 \n    -0.37 \n    -0.31 \n    -0.46 \n    -0.2 \n  \n\n\n\n\n\n\nCompara tus conclusiones del mapa de correspondencias con esta información de los residuales\n\nNota adicionalmente que el ordenamiento de las categorías en la primera dimensión del mapa de correspondencias ayuda a interpretar.\n\ndim_1 <- grafica_datos$cols[,1]\ncolumnas <- names(dim_1)\ncolumnas_ord <- columnas[order(dim_1)]\nres_hurto_tbl |> select(departamento, all_of(columnas_ord)) |> \n    mutate(across(where(is.numeric), \n                 ~ cell_spec(.x, color = ifelse(.x > 0.2, \"black\", \n                                         ifelse(.x < -0.2, \"red\", \"gray\"))))) |>\n    kable(escape = FALSE) |>\n    kable_styling(font_size = 12) |> \n    kable_paper()\n\n\n\n \n  \n    departamento \n    01-12 h \n    12-14 h \n    01-12 m \n    12-14 m \n    15-17 h \n    64+ h \n    50-64 h \n    18-20 h \n    15-17 m \n    40-49 h \n    30-39 h \n    21-29 h \n    64+ m \n    50-64 m \n    18-20 m \n    40-49 m \n    21-29 m \n    30-39 m \n  \n \n\n  \n    ropa \n    -0.87 \n    -0.89 \n    -0.65 \n    -0.46 \n    -0.6 \n    -0.61 \n    -0.32 \n    0.06 \n    0.49 \n    0 \n    0.2 \n    0.26 \n    0.12 \n    0.82 \n    1.1 \n    1.39 \n    1.58 \n    1.64 \n  \n  \n    accesorios \n    -0.65 \n    -0.45 \n    -0.69 \n    -0.28 \n    -0.17 \n    -0.2 \n    0.11 \n    0.35 \n    0.18 \n    -0.02 \n    0.2 \n    0.31 \n    0.36 \n    0.56 \n    0.71 \n    0.56 \n    0.49 \n    0.43 \n  \n  \n    tabaco \n    -0.38 \n    -0.29 \n    -0.27 \n    -0.37 \n    -0.25 \n    2.19 \n    1.14 \n    0.05 \n    -0.54 \n    0.72 \n    0.15 \n    0.06 \n    2.51 \n    1.02 \n    -0.08 \n    0.09 \n    -0.27 \n    -0.2 \n  \n  \n    escritura \n    1.1 \n    1.24 \n    1.13 \n    0.5 \n    0.33 \n    -0.71 \n    -0.65 \n    -0.55 \n    -0.45 \n    -0.61 \n    -0.77 \n    -0.76 \n    -0.84 \n    -0.81 \n    -0.83 \n    -0.79 \n    -0.87 \n    -0.86 \n  \n  \n    libros \n    -0.52 \n    -0.06 \n    -0.59 \n    -0.41 \n    0.48 \n    0.58 \n    0.23 \n    0.77 \n    -0.31 \n    0.2 \n    0.42 \n    0.49 \n    0.26 \n    -0.06 \n    -0.32 \n    -0.37 \n    -0.41 \n    -0.5 \n  \n  \n    discos \n    -0.77 \n    0.3 \n    -0.8 \n    -0.58 \n    1.79 \n    -0.65 \n    -0.22 \n    1.26 \n    -0.51 \n    -0.05 \n    0.3 \n    0.3 \n    -1 \n    -0.72 \n    -0.66 \n    -0.81 \n    -0.73 \n    -0.86 \n  \n  \n    bienes \n    -0.53 \n    -0.4 \n    -0.33 \n    -0.6 \n    -0.21 \n    0.56 \n    0.69 \n    0.04 \n    -0.21 \n    0.74 \n    0.55 \n    0.6 \n    0.56 \n    0.82 \n    -0.05 \n    0.46 \n    -0.12 \n    0.2 \n  \n  \n    dulces \n    1.48 \n    0.86 \n    1.39 \n    0.9 \n    0.14 \n    -0.14 \n    -0.7 \n    -0.61 \n    -0.11 \n    -0.9 \n    -0.87 \n    -0.86 \n    -0.04 \n    -0.7 \n    -0.8 \n    -0.84 \n    -0.88 \n    -0.87 \n  \n  \n    juguetes \n    3.52 \n    1.1 \n    1.07 \n    -0.18 \n    -0.44 \n    -0.74 \n    -0.94 \n    -0.87 \n    -0.84 \n    -0.87 \n    -0.8 \n    -0.92 \n    -0.85 \n    -0.76 \n    -0.82 \n    -0.86 \n    -0.9 \n    -0.7 \n  \n  \n    joyería \n    -0.36 \n    0 \n    1.37 \n    2.65 \n    0.16 \n    -0.74 \n    -0.84 \n    -0.42 \n    1.83 \n    -0.76 \n    -0.69 \n    -0.48 \n    -0.77 \n    -0.71 \n    0.06 \n    -0.7 \n    -0.35 \n    -0.62 \n  \n  \n    perfumes \n    -0.71 \n    -0.74 \n    0.91 \n    1.22 \n    -0.56 \n    0.36 \n    0.38 \n    -0.2 \n    1.46 \n    0.29 \n    0 \n    -0.17 \n    0.48 \n    0.43 \n    0.87 \n    0.18 \n    0.27 \n    0.18 \n  \n  \n    herramientas \n    -0.06 \n    0.32 \n    -0.78 \n    -0.81 \n    0.53 \n    1.83 \n    2.07 \n    0.11 \n    -0.92 \n    1.51 \n    0.96 \n    0.1 \n    -0.79 \n    -0.62 \n    -0.8 \n    -0.67 \n    -0.81 \n    -0.72 \n  \n  \n    otros \n    -0.23 \n    0.02 \n    -0.73 \n    -0.71 \n    0.34 \n    -0.33 \n    0.01 \n    0.57 \n    -0.49 \n    0.12 \n    0.47 \n    0.89 \n    -0.2 \n    -0.46 \n    -0.27 \n    -0.31 \n    -0.22 \n    -0.37 \n  \n\n\n\n\n\n\nOtras dimensiones\nEn el caso anterior, la calidad de la representación es cercana al 80%. Existen algunas desviaciones que la posiblemente la gŕafica no explica del todo, y algunas proyecciones son aproximadas. Podemos ver cómo se ven otras dimensiones de este análisis para entender desviaciones adicionales:\n\nplot(corr_hurto, dim = c(1, 3), map = \"rowprincipal\", arrows = c(FALSE, TRUE))\n\n\n\n\n\n\n\n\nIzenman, A. J. 2009. Modern Multivariate Statistical Techniques: Regression, Classification, and Manifold Learning. Springer Texts en Statistics. Springer New York. https://books.google.com.mx/books?id=1CuznRORa3EC.\n\n\nLê, Sébastien, Julie Josse, y François Husson. 2008. «FactoMineR: An R Package for Multivariate Analysis». Journal of Statistical Software, Articles 25 (1): 1-18. https://doi.org/10.18637/jss.v025.i01."
  },
  {
    "objectID": "analisis-datos.html",
    "href": "analisis-datos.html",
    "title": "3  Análisis exploratorio",
    "section": "",
    "text": "“Exploratory data analysis can never be the whole story, but nothing else can serve as the foundation stone –as the first step.” — John Tukey\nMuchas veces se le llama análisis exploratorio a una combinación de resúmenes, gráficas y tablas cuyos propósitos pueden englobarse en:\nEsta fase del análisis de datos es fundamental, como la cita de Tukey explica arriba, y se caracteríza por un enfoque de detective: quizá tenemos algunas preguntas, algunas sospechas, y en esta fase acumulamos indicios que nos indiquen caminos prometedores de investigación.\nEn contraste, tenemos el análisis confirmatorio, que busca validar hipótesis o dar respuestas correctamente cuantificadas en cuanto a su incertidumbre o grado de error. En esta parte somos más jueces que detectives, y utilizamos más maquinaria matemática (teoría de probabilidad) para especificar con claridad nuestros supuestos y poder hacer cálculos cuidadosos, generalmente basados en algún tipo de aleatorización.\nNinguno de los dos tipos de análisis funciona muy bien sin el otro, (Tukey (1980)) y explicaremos por qué un poco más adelante. Por el momento, para ilustrar el enfoque exploratorio, comenzaremos con datos que podemos describir de manera completa y efectiva sin necesidad de hacer resúmenes o aplicar técnicas avanzadas."
  },
  {
    "objectID": "analisis-datos.html#ejemplo-nacimientos",
    "href": "analisis-datos.html#ejemplo-nacimientos",
    "title": "3  Análisis exploratorio",
    "section": "3.1 Ejemplo: nacimientos",
    "text": "3.1 Ejemplo: nacimientos\nConsideremos una parte de los datos de nacimientos por día del INEGI de 1999 a 2016. Consideraremos sólo tres meses: enero a marzo de 2016. Estos datos, por su tamaño, pueden representarse de manera razonablemente efectiva en una visualización de serie de tiempo. Examinamos partes del contenido de la tabla:\n\n\n\nExaminamos partes del contenido de la tabla:\n\ntab_1 <- nacimientos |> \n   select(fecha, n) |> \n   slice_head(n = 5)\ntab_2 <- nacimientos |> \n   select(fecha, n) |> \n   slice_tail(n = 5)\nkable(list(tab_1, tab_2)) |> kable_paper()\n\n\n\n\n  \n    \n\n\n \n  \n    fecha \n    n \n  \n \n\n  \n    2016-01-01 \n    3952 \n  \n  \n    2016-01-02 \n    4858 \n  \n  \n    2016-01-03 \n    4665 \n  \n  \n    2016-01-04 \n    5948 \n  \n  \n    2016-01-05 \n    6087 \n  \n\n\n\n \n    \n\n\n \n  \n    fecha \n    n \n  \n \n\n  \n    2016-03-27 \n    4112 \n  \n  \n    2016-03-28 \n    5805 \n  \n  \n    2016-03-29 \n    5957 \n  \n  \n    2016-03-30 \n    5766 \n  \n  \n    2016-03-31 \n    5497 \n  \n\n\n\n \n  \n\n\n\n\n\nEn un examen rápido de estos números no vemos nada fuera de orden. Los datos tienen forma de serie de tiempo regularmente espaciada (un dato para cada día). Podemos graficar de manera simple como sigue:\n\nggplot(nacimientos, aes(x = fecha, y = n)) +\n   geom_point() +\n   geom_line() + \n   scale_x_date(breaks = \"1 week\", date_labels = \"%d-%b\") \n\n\n\n\nEsta es una descripción de los datos, que quizá no es muy compacta pero muestra varios aspectos importantes. En este caso notamos algunos patrones que saltan a la vista. Podemos marcar los domingos de cada semana:\n\ndomingos_tbl <- nacimientos |> \n   filter(weekdays(fecha) == \"Sunday\")\nggplot(nacimientos, aes(x = fecha, y = n)) +\n   geom_vline(aes(xintercept = fecha), domingos_tbl, colour = \"salmon\") +\n   geom_point() +\n   geom_line() + \n   scale_x_date(breaks = \"1 week\", date_labels = \"%d-%b\") \n\n\n\n\nObservamos que los domingos ocurren menos nacimientos y los sábados también ocurren relativamente menos nacimentos. ¿Por qué crees que sea esto?\nAdicionalmente a estos patrones observamos otros aspectos interesantes:\n\nEl primero de enero hay considerablemente menos nacimientos de los que esperaríamos para un viernes. ¿Por qué?\nEl primero de marzo hay un exceso de nacimientos considerable. ¿Qué tiene de especial este primero de marzo?\n¿Cómo describirías lo que sucede en la semana que comienza el 21 de marzo? ¿Por qué crees que pase eso?\n¿Cuáles son los domingos con más nacimientos? ¿Qué tienen de especial y qué explicación puede tener?\n\nLa confirmación de estas hipótesis, dependiendo de su forma, puede ser relativamente simple (por ejemplo ver una serie más larga de domingos comparados con otros días de la semana) hasta muy compleja (investigar preferencias de madres, de doctores o de hospitales, costumbres y actitudes, procesos en el registro civil, etc.)"
  },
  {
    "objectID": "analisis-datos.html#procesos-generadores-de-datos",
    "href": "analisis-datos.html#procesos-generadores-de-datos",
    "title": "3  Análisis exploratorio",
    "section": "Procesos generadores de datos",
    "text": "Procesos generadores de datos\nDe este primer ejemplo donde usamos una gráfica simple, vemos que una visión descontextualizada de estos datos no tiene mucha utilidad\n\n\n\n\n\n\nEl proceso generador de datos\n\n\n\nNótese que en todas estas preguntas hemos tenido que recurrir a conocimientos generales y de dominio para interpretar y hacer hipótesis acerca de lo que vemos en la gráfica. Las explicaciones son típicamente complejas e intervienen distintos aspectos del comportamiento de actores, sistemas, y métodos de recolección de datos involucrados.\nAl conjunto de esos aspectos que determinan los datos que finalmente observamos le llamamos el proceso generador de datos.\n\n\nEl análisis de datos en general busca entender las partes importantes del proceso que los generó. En el análisis descriptivo y exploratorio buscamos iluminar ese proceso, proponer hipótesis y buscar caminos interesantes para investigar, ya sea con técnicas cuantitativas o con trabajo de campo (como sugiere el título de artículo de David A. Friedman: Statistical Models and Shoe Leather).\nCon la teoría de probabilidades podemos modelar más explícitamente partes de estos procesos generadores de datos, especialmente cuando controlamos parte de ese proceso generador mediante técnicas estadísticas de diseño, por ejemplo, usando aleatorización."
  },
  {
    "objectID": "analisis-datos.html#ejemplo-cálculos-renales",
    "href": "analisis-datos.html#ejemplo-cálculos-renales",
    "title": "3  Análisis exploratorio",
    "section": "Ejemplo (cálculos renales)",
    "text": "Ejemplo (cálculos renales)\nEn este ejemplo también intentaremos mostrar los datos completos sin intentar resumir.\nEste es un estudio real acerca de tratamientos para cálculos renales (Julious y Mullee (1994)). Pacientes se asignaron de una forma no controlada a dos tipos de tratamientos para reducir cálculos renales. Para cada paciente, conocemos el el tipo de ćalculos que tenía (grandes o chicos) y si el tratamiento tuvo éxito o no.\nLa tabla original se ve como sigue (muestreamos algunos renglones):\n\ncalculos <- read_csv(\"./datos/kidney_stone_data.csv\")\nnames(calculos) <- c(\"tratamiento\", \"tamaño\", \"éxito\")\ncalculos <- calculos |> \n   mutate(tamaño = ifelse(tamaño == \"large\", \"grandes\", \"chicos\")) |> \n   mutate(resultado = ifelse(éxito == 1, \"mejora\", \"sin_mejora\")) |> \n   select(tratamiento, tamaño, resultado)\nnrow(calculos)\n\n[1] 700\n\ncalculos |> sample_n(15) |> \n   kable() |> kable_paper(full_width = FALSE)\n\n\n\n \n  \n    tratamiento \n    tamaño \n    resultado \n  \n \n\n  \n    A \n    grandes \n    sin_mejora \n  \n  \n    B \n    chicos \n    mejora \n  \n  \n    B \n    chicos \n    mejora \n  \n  \n    B \n    grandes \n    mejora \n  \n  \n    A \n    grandes \n    sin_mejora \n  \n  \n    A \n    grandes \n    mejora \n  \n  \n    A \n    chicos \n    mejora \n  \n  \n    A \n    chicos \n    mejora \n  \n  \n    A \n    grandes \n    mejora \n  \n  \n    A \n    grandes \n    sin_mejora \n  \n  \n    B \n    chicos \n    mejora \n  \n  \n    B \n    chicos \n    mejora \n  \n  \n    B \n    grandes \n    mejora \n  \n  \n    A \n    grandes \n    mejora \n  \n  \n    B \n    chicos \n    mejora \n  \n\n\n\n\n\nAunque estos datos contienen información de 700 pacientes (cada renglón es un paciente), los datos pueden resumirse sin pérdida de información contando como sigue:\n\ncalculos_agregada <- calculos |> \n   group_by(tratamiento, tamaño, resultado) |> \n   count()\ncalculos_agregada |> kable() |> kable_paper(full_width = FALSE)\n\n\n\n \n  \n    tratamiento \n    tamaño \n    resultado \n    n \n  \n \n\n  \n    A \n    chicos \n    mejora \n    81 \n  \n  \n    A \n    chicos \n    sin_mejora \n    6 \n  \n  \n    A \n    grandes \n    mejora \n    192 \n  \n  \n    A \n    grandes \n    sin_mejora \n    71 \n  \n  \n    B \n    chicos \n    mejora \n    234 \n  \n  \n    B \n    chicos \n    sin_mejora \n    36 \n  \n  \n    B \n    grandes \n    mejora \n    55 \n  \n  \n    B \n    grandes \n    sin_mejora \n    25 \n  \n\n\n\n\n\nEste resumen no es muy informativo, pero al menos vemos qué valores aparecen en cada columna de la tabla. Como en este caso nos interesa principalmente la tasa de éxito de cada tratamiento, podemos mejorar mostrando como sigue:\n\ncalculos_agregada |> pivot_wider(names_from = resultado, values_from = n) |> \n   mutate(total = mejora + sin_mejora) |> \n   mutate(prop_mejora = round(mejora / total, 2)) |> \n   select(tratamiento, tamaño, total, prop_mejora) |> \n   arrange(tamaño) |> \n   kable() |> kable_paper(full_width = FALSE)\n\n\n\n \n  \n    tratamiento \n    tamaño \n    total \n    prop_mejora \n  \n \n\n  \n    A \n    chicos \n    87 \n    0.93 \n  \n  \n    B \n    chicos \n    270 \n    0.87 \n  \n  \n    A \n    grandes \n    263 \n    0.73 \n  \n  \n    B \n    grandes \n    80 \n    0.69 \n  \n\n\n\n\n\nEsta tabla descriptiva es una reescritura de los datos, y no hemos resumido nada todavía. Sin embargo, esta tabla es apropiada para empezar a contestar la pregunta:\n\n¿Qué indican estos datos acerca de qué tratamiento es mejor? ¿Acerca del tamaño de cálculos grandes o chicos?\n\nSupongamos que otro analista decide comparar los pacientes que recibieron cada tratamiento, ignorando la variable de tamaño:\n\ncalculos |> group_by(tratamiento) |> \n   summarise(prop_mejora = mean(resultado == \"mejora\") |> round(2)) |> \n   kable() |> kable_paper(full_width = FALSE)\n\n\n\n \n  \n    tratamiento \n    prop_mejora \n  \n \n\n  \n    A \n    0.78 \n  \n  \n    B \n    0.83 \n  \n\n\n\n\n\ny parece ser que el tratamiento \\(B\\) es mejor que el \\(A\\). Esta es una paradoja (un ejemplo de la paradoja de Simpson) . Si un médico no sabe que tipo de cálculos tiene el paciente, ¿entonces debería recetar \\(B\\)? ¿Si sabe debería recetar \\(A\\)? Esta discusión parece no tener mucho sentido.\nPodemos investigar por qué está pasando esto considerando la siguiente tabla, que solo examina cómo se asignó el tratamiento dependiendo del tipo de cálculos de cada paciente:\n\ncalculos |> group_by(tratamiento, tamaño) |> count() |> \n   kable() |> kable_paper(full_width = FALSE)\n\n\n\n \n  \n    tratamiento \n    tamaño \n    n \n  \n \n\n  \n    A \n    chicos \n    87 \n  \n  \n    A \n    grandes \n    263 \n  \n  \n    B \n    chicos \n    270 \n  \n  \n    B \n    grandes \n    80 \n  \n\n\n\n\n\nNuestra hipótesis aquí es que la decisión de qué tratamiento usar depende del tamaño de los cálculos. En este caso, por alguna razón se prefiere utilizar el tratamiento \\(A\\) para cálculos grandes, y \\(B\\) para cálculos chicos. Esto quiere decir que en la tabla total el tratamiento \\(A\\) está en desventaja porque se usa en casos más difíciles, pero el tratamiento \\(A\\) parece ser en general mejor.\nIgual que en el ejemplo anterior, los resúmenes descriptivos están acompañados de hipótesis acerca del proceso generador de datos, y esto ilumina lo que estamos observando y nos guía hacia descripciones provechosas de los datos. Las explicaciones no son tan simples y, otra vez, interviene el comportamiento de doctores, tratamientos, y distintos tipos de padecimientos."
  },
  {
    "objectID": "analisis-datos.html#ejemplo",
    "href": "analisis-datos.html#ejemplo",
    "title": "3  Análisis exploratorio",
    "section": "Ejemplo",
    "text": "Ejemplo\nAhora supongamos que tenemos datos de un tratamiento para mejorar enfermedades de corazón. En el estudio también se mide, durante la duración del estudio, si la presión del paciente es alta o baja. Supongamos otra vez que tenemos dos tratamientos, A y B, y obtenemos los siguientes resultados:\n\ncorazon <- calculos |> rename(presión = tamaño) |> \n  mutate(presión = recode(presión, chicos = \"baja\", grandes = \"alta\"))\n\n\n\n\n\n \n  \n    tratamiento \n    presión \n    total \n    prop_mejora \n  \n \n\n  \n    A \n    alta \n    263 \n    0.73 \n  \n  \n    B \n    alta \n    80 \n    0.69 \n  \n  \n    A \n    baja \n    87 \n    0.93 \n  \n  \n    B \n    baja \n    270 \n    0.87 \n  \n\n\n\n\n\n\ncorazon |> group_by(tratamiento) |> \n   summarise(prop_mejora = mean(resultado == \"mejora\") |> round(2)) |> \n   kable() |> \n   kable_paper(full_width = FALSE)\n\n\n\n \n  \n    tratamiento \n    prop_mejora \n  \n \n\n  \n    A \n    0.78 \n  \n  \n    B \n    0.83 \n  \n\n\n\n\n\n\nEn este ejemplo, ¿cuál es el análisis más apropiado? ¿Qué cosas necesitarías saber para tomar una decisión?\n¿En qué es diferente o similar al caso de los cálculos renales?"
  },
  {
    "objectID": "analisis-datos.html#inferencia",
    "href": "analisis-datos.html#inferencia",
    "title": "3  Análisis exploratorio",
    "section": "3.2 Inferencia",
    "text": "3.2 Inferencia\nEn los ejemplos anteriores, sólo vimos muestras de datos (algunos pacientes, algunas fechas). Nuestras descripciones son, estrictamente hablando, válidas para esa muestra de los datos.\nSi quisiéramos generalizar a la población gneral de pacientes con cálculos (quizá en nuestra muestra el tratamiento A parece mejor, pero ¿qué podemos decir para la población de pacientes?), o quisiéramos predecir cómo van a ser los nacimientos en 2021, requerimos hacer inferencia. Este tipo de análisis, central en la estadística, busca establecer condiciones para poder generalizar de nuestra muestra a datos no observados (otros pacientes, nacimientos en el futuro), y cuantificar qué tan bien o mal podemos hacerlo.\nPara llegar a este tipo de análisis, generalmente tenemos que comenzar con el análisis exploratorio, y con la comprensión de los fundamentos del proceso generador asociado a nuestros datos. En algunos casos, veremos que es posible usar herramientas matemáticas para modelar aspectos de nuestro proceso generador de datos, que cuando son válidas, nos permiten generalizar y ampliar apropiadamente el rango de nuestras conclusiones.\nLa herramienta básica para construir, entender y operar con estos modelos es la teoría de probabilidad, que veremos más adelante."
  },
  {
    "objectID": "analisis-datos.html#ejemplo-más-de-nacimientos-en-méxico",
    "href": "analisis-datos.html#ejemplo-más-de-nacimientos-en-méxico",
    "title": "3  Análisis exploratorio",
    "section": "3.3 Ejemplo: más de nacimientos en México",
    "text": "3.3 Ejemplo: más de nacimientos en México\nEste ejemplo sigue sigue ideas de un análisis de A. Vehtari y A. Gelman, junto con análisis de serie de tiempo de Cleveland (1993)\nUsaremos los datos de nacimientos registrados por día en México, desde 1999. Haremos una pregunta como ¿cuáles son los cumpleaños más frecuentes?, o ¿Qué mes del año hay más nacimientos?\nUna gráfica popular (ver por ejemplo esta visualización):\n\n\n\n\n\n¿Cómo criticarías este análisis desde el punto de vista de los tres primeros principios del diseño analítico? ¿Las comparaciones son útiles? ¿Hay aspectos multivariados? ¿Qué tan bien explica o sugiere estructura, mecanismos o causalidad?\n\nDatos de natalidad para México\n\n\n\nConsideramos los datos agregados de número de nacimientos (registrados) por día desde 1999 hasta 2016.\nPodemos hacer una primera gráfica de la serie de tiempo que no es muy útil:\n\n\n\n\n\nHay varias características que notamos. Principalmente, la tendencia ligeramente decreciente de número de nacimientos a lo largo de los años, un patrón anual, dispersión producida por los días de la semana.\nSolo estas características hacen que la comparación entre días sea una difícil de interpretar. Supongamos que comparamos el número de nacimientos de dos miércoles dados. Esa comparación será diferente dependiendo del año donde ocurrieron, el mes donde ocurrieron, si semana santa ocurrió en algunos de los miércoles, y así sucesivamente.\nComo en nuestros ejemplos anteriores, la idea del siguiente análisis es aislar las componentes que observamos en la serie de tiempo: extraemos componentes ajustadas, y luego examinamos los residuales.\nEn este caso particular, construiremos una descomposición aditiva de la serie de tiempo (Cleveland (1993)).\n\n\nTendencia\nComenzamos por extraer la tendencia, haciendo promedios loess con vecindades relativamente grandes. Quizá preferiríamos suavizar menos para capturar más variación lenta, pero si hacemos esto en este punto empezamos a absorber parte de la componente anual:\n\nmod_1 <- loess(n ~ as.numeric(fecha), data = natalidad, span = 0.2, degree = 1)\ndatos_dia <- natalidad |> mutate(ajuste_1 = fitted(mod_1)) |> \n    mutate(res_1 = n - ajuste_1)\n\n\n\n\n\n\nA principios de 2000 el suavizador está en niveles de alrededor de 7000 nacimientos diarios, hacia 2015 ese número es más cercano a unos 6000.\n\n\nComponente anual\nRestamos a la serie la tendencia, y así obtenemos mejores comparaciones controlando por tendencia (por ejemplo, comparar un día de 2000 y de 2015 tendria más sentido). Ahora ajustamos los residuales del suavizado anterior, pero con menos suavizamiento. Así evitamos capturar tendencia:\n\nmod_anual <- loess(res_1 ~ as.numeric(fecha), data = datos_dia, degree = 2, span = 0.005)\ndatos_dia <- datos_dia |> mutate(ajuste_2 = fitted(mod_anual)) |> \n    mutate(res_2 = res_1 - ajuste_2)\n\n\n\n\n\n\n\n\nDía de la semana\nAhora podemos capturar el efecto de día de la semana. En este caso, podemos hacer suavizamiento loess para cada serie independiente\n\ndatos_dia <- datos_dia |> group_by(dia_semana) |> nest() |> \n    mutate(ajuste_mod = \n      map(data, ~ loess(res_2 ~ as.numeric(fecha), data = .x, span = 0.1, degree = 1))) |> \n    mutate(ajuste_3 =  map(ajuste_mod, fitted)) |> \n    select(-ajuste_mod) |> unnest(cols = c(data, ajuste_3)) |> \n    mutate(res_3 = res_2 - ajuste_3) |> ungroup()\n\n\n\n\n\n\n\n\nResiduales\nExaminamos los residuales finales quitando los efectos ajustados:\n\nggplot(datos_dia, aes(x = fecha, y = res_3)) + geom_line() +\n    geom_smooth(method = \"loess\", span = 0.02, \n                method.args = list(degree=1, family = \"symmetric\"))\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nObservación: nótese que la distribución de estos residuales tiene irregularidades interesante: es una distribución con colas largas, y no se debe a unos cuantos atípicos. Esto generalmente es indicación que hay factores importantes que hay que examinar en los residuales:\n\n\n\n\n\n\n\nReestimación\nCuando hacemos este proceso secuencial ajuste -> residual, a veces conviene iterarlo. La razón es que un una segunda o tercera pasada podemos hacer mejores estimaciones de cada componente, y es posible suavizar menos sin capturar componentes de más alta frecuencia.\nAsí que podemos regresar a la serie original para hacer mejores estimaciones, más suavizadas:\n\n# quitamos componente anual y efecto de día de la semana\ndatos_dia <- datos_dia |> mutate(n_1 = n - ajuste_2 - ajuste_3)\n# reajustamos\nmod_1 <- loess(n_1 ~ as.numeric(fecha), data = datos_dia, span = 0.02, degree = 2,\n               family = \"symmetric\")\n\n\n\n\n\n\n\nmod_anual <- loess(n_2 ~ as.numeric(fecha), data = datos_dia, \n               degree = 2,  span = 0.01, family = \"symmetric\")\ndatos_dia <- datos_dia |>\n    mutate(ajuste_5 = fitted(mod_anual)) |> \n    mutate(res_5 = n_2 - ajuste_5) |>\n    mutate(n_3 = n - ajuste_4 - ajuste_5)\n\n\n\n\n\n\nY ahora repetimos con la componente de día de la semana:\n\n\n\n\n\n\n\nAnálisis de componentes\nAhora comparamos las componentes estimadas y los residuales en una misma gráfica. La suma de todas estas componentes da los datos originales: es una descomposición aditiva.\n\n\n\n\n\nY esto nos da muchas comparaciones buenas que explican la variación que vimos en los datos. Una gran parte de los residuales está entre +-/250 nacimientos por día, pero las colas tienen una dispersión mucho mayor:\n\nquantile(datos_dia$res_6, c(00, .01,0.05, 0.10, 0.90, 0.95, 0.99, 1)) |> round()\n\n   0%    1%    5%   10%   90%   95%   99%  100% \n-2238 -1134  -315  -202   188   268   516  2521 \n\n\n¿A qué se deben estas colas tan largas?\n\n\n\n\n\nViernes 13?\nPodemos empezar con una curosidad: En Viernes o Martes 13, ¿nacen menos niños?\n\n\n\n\n\nNótese que fue útil agregar el indicador de Semana santa por el Viernes 13 de Semana Santa que se ve como un atípico en el panel de los viernes 13.\n\n\nResiduales: antes y después de 2006\nVeamos primero una agregación sobre los años de los residuales. Lo primero es observar un cambio que sucedió repentinamente en 2006:\n\nsept_1 <- ymd(paste0(2000:2016, \"-09-01\")) |> yday()\ndatos_dia <- datos_dia |> mutate(antes_2006 = ifelse(año < 2006, \"Antes de 2006\", \"2006 en adelante\"))\nggplot(datos_dia , aes(x = dia_año, y = res_6, group = factor(año))) + \n    geom_point(size = 0.5) +\n    geom_vline(xintercept = sept_1, alpha = 0.3, colour = \"red\") +\n    facet_wrap( ~ antes_2006, ncol = 1) + ylab(\"Residual: exceso de nacimientos\") +\n    annotate(\"text\", x = 260, y = -1500, label = \"Sept 1\", colour = \"red\")\n\n\n\n\nLa razón es un cambio en la ley acerca de cuándo pueden entrar los niños a la primaria. Antes era por edad y había poco margen. Ese exceso de nacimientos son reportes falsos para que los niños no tuvieran que esperar un año completo por haber nacido unos cuantos días antes de la fecha límite.\nOtras características que debemos investigar:\n\nEfectos de Año Nuevo, Navidad, Septiembre 16 y otros días feriados como Febrero 14.\nSemana santa: como la fecha cambia, vemos que los residuales negativos tienden a ocurrir dispersos alrededor del día 100 del año.\n\n\n\nOtros días especiales: más de residuales\nAhora promediamos residuales (es posible agregar barras para indicar dispersión a lo largo de los años) para cada día del año. Podemos identificar ahora los residuales más grandes: se deben, por ejemplo, a días feriados, con consecuencias adicionales que tienen en días ajuntos (excesos de nacimientos):\n\n\n`summarise()` has grouped output by 'dia_año_366', 'antes_2006'. You can\noverride using the `.groups` argument.\n\n\n\n\n\n\n\nSemana santa\nPara Semana Santa tenemos que hacer unos cálculos. Si alineamos los datos por días antes de Domingo de Pascua, obtenemos un patrón de caída fuerte de nacimientos el Viernes de Semana Santa, y la característica forma de “valle con hombros” en días anteriores y posteriores estos Viernes. ¿Por qué ocurre este patrón?\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nNótese un defecto de nuestro modelo: el patrón de “hombros” alrededor del Viernes Santo no es suficientemente fuerte para equilibrar los nacimientos faltantes. ¿Cómo podríamos mejorar nuestra descomposición?"
  },
  {
    "objectID": "analisis-datos.html#descripciones-simples-y-problemas-complejos",
    "href": "analisis-datos.html#descripciones-simples-y-problemas-complejos",
    "title": "3  Análisis exploratorio",
    "section": "3.4 Descripciones simples y problemas complejos",
    "text": "3.4 Descripciones simples y problemas complejos\nMuchas veces se descarta al análisis descriptivo o exploratorio (al menos en sus formas más crudas) como algo que no requiere mucha habilidad técnica o conocimiento de dominio, o cuando se quiere evitar plantear hipótesis claras acerca de los datos que ayuden en su entendimiento.\nEn realidad el análisis descriptivo y exploratorio es crucial en el análisis de datos en general, y tiene siempre que venir acompañado de conocimiento de dominio, habilidad técnica, una mente crítica y muchas veces ingenio y creatividad."
  },
  {
    "objectID": "analisis-datos.html#qué-preguntas-formular",
    "href": "analisis-datos.html#qué-preguntas-formular",
    "title": "3  Análisis exploratorio",
    "section": "3.5 ¿Qué preguntas formular?",
    "text": "3.5 ¿Qué preguntas formular?\nExisten algunas prácticas generales que utilizamos para hacer validaciones y resúmenes simples de los datos que discutiremos más adelante. Por el momento, discutimos las razones por las que estamos haciendo ese análisis en un principio.\nEn general, comenzamos con algunas preguntas básicas que quisiéramos contestar con los datos. El análisis exploratorio juega un papel central para comenzar a responder:\n\n¿Es razonable la pregunta que queremos contestar?\n¿Podemos contestar la pregunta con los datos que tenemos?\n\nAunque estos dos incisos a veces parecen transparentes y simples de contestar, generalmente no lo son: las preguntas que queremos contestar y los problemas que queremos resolver usualmente son no triviales."
  },
  {
    "objectID": "analisis-datos.html#formulación-de-preguntas-y-respuestas",
    "href": "analisis-datos.html#formulación-de-preguntas-y-respuestas",
    "title": "3  Análisis exploratorio",
    "section": "3.6 Formulación de preguntas y respuestas",
    "text": "3.6 Formulación de preguntas y respuestas\nEl proceso de la ciencia de datos no va desde las preguntas hasta las respuestas en un camino lineal.\nEn esta gráfica Roger Peng hay tres caminos: uno es uno ideal que pocas veces sucede, otro produce respuestas poco útiles pero es fácil, y otro es tortuoso pero que caracteriza el mejor trabajo de análisis de datos:\n\n\n\n\n\nAdaptado de R. Peng: Tukey, design thinking and better questions\n\n\n\n\nEl proceso típico involucra pasos como los siguientes, y es un proceso no lineal:\n\nHacer preguntas de la materia que nos interesa\nRecolectar, consumir y procesar los datos para abordarla\nExplorar estos datos y evaluar su calidad\nHacer análisis o modelos\nReportar los resultados de forma adecuada y con esto resolver y replantear las preguntas importantes.\n\nPor ejemplo, evaluar la calidad de los datos puede llevar a replantear la necesidad de obtener más información o de hacer estudios específicos. Así también, los modelos pueden dar luz sobre las preguntas que los originan.\n\n\n\n¿Por dónde empezar el análisis descriptivo y exploratorio? ¿Cómo sabemos que vamos por buen camino y qué hacer cuando sentimos que nos estancamos?"
  },
  {
    "objectID": "analisis-datos.html#cómo-saber-que-vamos-en-el-camino-correcto",
    "href": "analisis-datos.html#cómo-saber-que-vamos-en-el-camino-correcto",
    "title": "3  Análisis exploratorio",
    "section": "3.7 ¿Cómo saber que vamos en el camino correcto?",
    "text": "3.7 ¿Cómo saber que vamos en el camino correcto?\nComenzamos por discribir cuáles son los signos de calidad del análisis que piensa usarse como insumo para una decisión. Los principios del diseño analítico de Edward Tufte (Tufte (2006)) son:\nLos análisis exitosos:\n\nMuestran y explotan comparaciones, diferencias y variación.\nTienden a ser multivariados: estudian conjuntamente más de 1 o 2 variables.\nMuestran y explotan estructura sistemática, sugieren explicaciones. Cuando es posible, aportan evidencia de causalidad.\n\nTambién muy importantes pero en los que pondremos menos énfasis:\n\nDatos y procesos están bien documentados. El análisis es reproducible y transparente.\nIntentan integrar la evidencia completa: teoría, texto, explicaciones, tablas y gráficas.\n\nY finalmente, el principio general:\n\nLa calidad, relevancia, e integridad del contenido y los datos son los que al final sostienen al análisis - por sí mismos, el uso de técnicas sofisticadas, algoritmos novedosos, uso o no de grandes datos, estilo de visualizaciones o presentaciones no son marcas o sellos de un análisis de datos exitoso.\n\n\n\n\n\n\n\nTip\n\n\n\nEvaluar un análisis o resultado en estos seis puntos generalmente ayuda en el proceso de refinamiento de preguntas y respuestas."
  },
  {
    "objectID": "analisis-datos.html#gráfica-de-minard",
    "href": "analisis-datos.html#gráfica-de-minard",
    "title": "3  Análisis exploratorio",
    "section": "3.8 Gráfica de Minard",
    "text": "3.8 Gráfica de Minard\nLa ilustración que Tufte usa para mostrar excelencia en diseño analítico es una gráfica de Minard que sirve para entender la campaña de Napoleón (1812) en Rusia. Es un ejemplo atípico, pero representa bien los principios y también muestra la importancia del ingenio en la construcción de un anállsis:\n\n\n\n\n\nMarcha de Napoleón de Charles Minard. Tomado de Wikipedia\n\n\n\n\n\n\n\n¿Cómo satisface los principios del diseño analítico este gráfico?\n\n\n\n\n\n\nCleveland, William S. 1993. Visualizing Data. Hobart Press.\n\n\nJulious, Steven A, y Mark A Mullee. 1994. «Confounding and Simpson’s paradox». BMJ 309 (6967): 1480-81. https://doi.org/10.1136/bmj.309.6967.1480.\n\n\nTufte, Edward R. 2006. Beautiful Evidence. Cheshire, CT: Graphics Press.\n\n\nTukey, John W. 1980. «We Need Both Exploratory and Confirmatory». The American Statistician 34 (1): 23-25. http://www.jstor.org/stable/2682991."
  },
  {
    "objectID": "inferencia.html",
    "href": "inferencia.html",
    "title": "4  Inferencia estadística",
    "section": "",
    "text": "Nos concentraremos en dos de las situaciones más comunes:\n\nInferencia a poblaciones: el proceso generador de datos “selecciona” a algunos elementos de una población, y queremos decir algo acerca de la población completa.\n\nPor ejemplo, consideremos esta población de 15 personas:\n\n\n\n\n\n\n  \n  \n    \n      id\n      edad\n      estatura\n      peso\n    \n  \n  \n    1\n21\n1.58\n60\n    2\n49\n1.72\n72\n    3\n40\n1.64\n56\n    4\n28\n1.50\n60\n    5\n45\nNA\nNA\n    6\n44\nNA\nNA\n    7\n33\nNA\nNA\n    8\n43\nNA\nNA\n    9\n18\nNA\nNA\n    10\n36\nNA\nNA\n    11\n29\nNA\nNA\n    12\n25\nNA\nNA\n    13\n41\nNA\nNA\n    14\n48\nNA\nNA\n    15\n22\nNA\nNA\n  \n  \n  \n\n\n\n\nPara una muestra de ellos tenemos información acerca de su estatura y peso. ¿Qué podríamos decir acerca de la estatura y el peso de la población general?\n\nInferencia causal: el proceso generador “asigna” tratamientos a una población o parte de ella, y quisiéramos saber cómo se comportarían las unidades tratadas si no recibieran tratamiento, y también cómo se comportarían unidades no tratadas si recibieran el tratamiento.\n\nEn este caso, la situación se ve como sigue. Imaginemos que tenemos 15 personas con dolor de cabeza, y obtenemos los siguientes datos:\n\n\n\n\n\n\n  \n  \n    \n      id\n      edad\n      dolor_con_aspirina\n      dolor_sin_aspirina\n      tomo_aspirina\n      dolor\n    \n  \n  \n    1\n40\nNA\n3\n0\n3\n    2\n31\nNA\n3\n0\n3\n    3\n64\n6\nNA\n1\n6\n    4\n36\nNA\n3\n0\n3\n    5\n18\n6\nNA\n1\n6\n    6\n49\nNA\n3\n0\n3\n    7\n23\nNA\n3\n0\n3\n    8\n54\nNA\n3\n0\n3\n    9\n33\n6\nNA\n1\n6\n    10\n19\n6\nNA\n1\n6\n    11\n48\nNA\n3\n0\n3\n    12\n27\n6\nNA\n1\n6\n    13\n56\n6\nNA\n1\n6\n    14\n44\n6\nNA\n1\n6\n    15\n45\n6\nNA\n1\n6\n  \n  \n  \n\n\n\n\nNuestra pregunta en este caso es del tipo: ¿ayuda la aspirina a reducir el dolor de cabeza en esta población? ¿qué tanto ayuda? Igualmente, tenemos información incompleta, en el sentido de que sólo observamos un resultado potencial de cada persona, dependiendo de si tomó aspirina o no. Si supiéramos los dos resultados potenciales de cada persona entonces podríamos contestar la pregunta sin dificultad.\n\n\n\n\n\n\nDatos incompletos e incertidumbre\n\n\n\nCasi por regla general, el hecho de que tengamos datos incompletos implica que una respuesta apropiada a la pregunta incorporará cierto grado de incertidumbre, y por lo tanto conviene utilizar modelos de probabilidad para describir esa incertidumbre.\nEn algunos casos, sin embargo, si controlamos algunas partes del proceso generador de datos, entonces es posible simplificar mucho el análisis, incluso al punto de que los modelos tienen menos relevancia."
  },
  {
    "objectID": "pruebas-hipotesis.html",
    "href": "pruebas-hipotesis.html",
    "title": "5  Pruebas de hipótesis",
    "section": "",
    "text": "Las primeras técnicas que veremos intentan contestar la siguiente pregunta:\nPor ejemplo:\nO también:"
  },
  {
    "objectID": "pruebas-hipotesis.html#comparación-con-poblaciones-de-referencia",
    "href": "pruebas-hipotesis.html#comparación-con-poblaciones-de-referencia",
    "title": "5  Pruebas de hipótesis",
    "section": "Comparación con poblaciones de referencia",
    "text": "Comparación con poblaciones de referencia\nEn las prueba de hipótesis, tratamos de construir distribuciones de referencia para comparar resultados que obtengamos con un “estándar” de variación, y juzgar si nuestros resultados son consistentes con la referencia o no (Box et al. (1978)).\nEn algunos casos, ese estándar de variación puede construirse con datos históricos.\n\nEjemplo\nSupongamos que estamos considerando cambios rápidos en una serie de tiempo de alta frecuencia. Hemos observado la serie en su estado “normal” durante un tiempo considerable, y cuando observamos nuevos datos quisiéramos juzgar si hay indicaciones o evidencia en contra de que el sistema sigue funcionando de manera similar.\nDigamos que monitoreamos ventanas de tiempo de tamaño 20 y necesitamos tomar una decisión. Abajo mostramos cinco ejemplos donde el sistema opera normalmente, que muestra la variabilidad en el tiempo en ventanas cortas del sistema.\nAhora suponemos que obtenemos una nueva ventana de datos. ¿Hay evidencia en contra de que el sistema sigue funcionando de manera similar?\nNuestra primera inclinación debe ser comparar: en este caso, compararamos ventanas históricas con nuestra nueva serie:\n\n\n\n\n# usamos datos simulados para este ejemplo\nset.seed(8812)\nhistoricos <- simular_serie(5000)\n\n\n\n\n\n\n¿Vemos algo diferente en los datos nuevos (el panel de color diferente)?\nIndendientemente de la respuesta, vemos que hacer este análisis de manera tan simple no es siempre útil: seguramente podemos encontrar maneras en que la nueva muestra (4) es diferente a muestras históricas. Por ejemplo, ninguna de muestras tiene un “forma de montaña” tan clara.\nNos preguntamos si no estamos sobreinterpretando variaciones que son parte normal del proceso.\nPodemos hacer un mejor análisis si extraemos varias muestras del comportamiento usual del sistema, graficamos junto a la nueva muestra, y revolvemos las gráficas para que no sepamos cuál es cuál. Entonces la pregunta es:\n\n¿Podemos detectar donde están los datos nuevos?\n\nEsta se llama una prueba de lineup, o una prueba de ronda de sospechosos (Hadley Wickham et al. (2010)). En la siguiente gráfica, en uno de los páneles están los datos recientemente observados. ¿Hay algo en los datos que distinga al patrón nuevo?\n\n\n\n\n\nEjercicio: ¿cuáles son los datos nuevos (solo hay un panel con los nuevos datos)? ¿Qué implica que la gráfica que escogamos como “más diferente” no sean los datos nuevos? ¿Qué implica que le “atinemos” a la gráfica de los datos nuevos?\nAhora observamos al sistema en otro momento y repetimos la comparación. En el siguiente caso obtenemos:\n\n\n\n\n\nAunque es imposible estar seguros de que ha ocurrido un cambio, la diferencia de una de las series es muy considerable. Si identificamos los datos correctos, la probabilidad de que hayamos señalado la nueva serie “sobreinterpretando” fluctuaciones en un proceso que sigue comportándose normalente es 0.05 - relativamente baja. Detectar los datos diferentes es evidencia en contra de que el sistema sigue funcionando de la misma manera que antes.\n\n\n\n\n\n\nPruebas de hipótesis\n\n\n\n\nLlamamos hipótesis nula a la hipótesis de que los nuevos datos son producidos bajo las mismas condiciones que los datos de control o de referencia.\nSi no escogemos la gráfica de los nuevos datos, nuestra conclusión es que la prueba no aporta evidencia en contra de la hipótesis nula.\nSi escogemos la gráfica correcta, nuestra conclusión es que la prueba aporta evidencia en contra de la hipótesis nula.\n\n¿Qué tan fuerte es la evidencia, en caso de que descubrimos los datos no nulos?\n\nCuando el número de paneles es más grande y detectamos los datos, la evidencia es más alta en contra de la nula. Decimos que el nivel de significancia de la prueba es la probabilidad de seleccionar a los datos correctos cuando la hipótesis nula es cierta (el sistema no ha cambiado).\nAdicionalmente, si acertamos, y la diferencia es más notoria y fue muy fácil detectar la gráfica diferente (pues sus diferencias son más extremas), esto también sugiere más evidencia en contra de la hipótesis nula (valor p).\n\n\n\nEn el caso de 20 paneles, la significancia es de 1/20 = 0.05. Cuando detectamos los datos nuevos, niveles de significancia más bajos implican más evidencia en contra de la nula.\nEsta prueba rara vez (o nunca) nos da seguridad completa acerca de ninguna conclusión, aún cuando hiciéramos muchos páneles."
  },
  {
    "objectID": "pruebas-hipotesis.html#cuantificando-la-distribución-de-referencia",
    "href": "pruebas-hipotesis.html#cuantificando-la-distribución-de-referencia",
    "title": "5  Pruebas de hipótesis",
    "section": "5.1 Cuantificando la distribución de referencia",
    "text": "5.1 Cuantificando la distribución de referencia\nEn el ejemplo anterior estamos intentando dectectar cualquier desviación del comportamiento normal del sistema de una manera rigurosa. Podemos hacerlo más cuantitativo creando estadísticas resumen de las series. Por ejemplo, podríamos utilizar la variabilidad que tienen las series alrededor de su nivel general.\n\nsd_simple <- function(x){\n  # suavizamiento exponencial\n  mod <- HoltWinters(x, beta=FALSE, gamma=FALSE)\n  suavizamiento <- fitted(mod)[,1] |> as.numeric()\n  sd(x[-1] - suavizamiento)\n}\nreferencia_tbl <- muestrear_ventanas(historicos, n_ventana = 1500) |> \n  pluck(\"lineup\") |> \n  group_by(rep) |> \n  summarise(est_prueba = sd_simple(obs))\nreferencia_tbl |> head()\n\n# A tibble: 6 × 2\n    rep est_prueba\n  <int>      <dbl>\n1     1      0.820\n2     2      1.23 \n3     3      0.921\n4     4      1.21 \n5     5      0.909\n6     6      1.16 \n\n\n\nggplot(referencia_tbl, aes(x = est_prueba)) + \n  geom_histogram() +\n  geom_vline(xintercept = sd_simple(observados$obs), colour = \"red\") +\n  annotate(\"text\", x = 2.5, y = 30, \n     label = \"diferencia observada\", colour = \"red\", angle = 90)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nY confirmamos que en efecto el valor observado (línea roja) es uno muy extremo, y poco consistente con el comportamiento usual del sistema.\nEl valor p (de una cola) Se define como la probabilidad de observar este resultado, o uno más grande, suponiendo que el sistema está funcionando usualmente, y en este caso lo calculamos como:\n\ndiferencia_obs <- sd_simple(observados$obs)\nreferencia_2 <- bind_rows(referencia_tbl,\n  tibble(rep = 0, est_prueba = diferencia_obs))\nreferencia_2 |> \n  mutate(mayor_obs = est_prueba > diferencia_obs) |> \n  summarise(valor_p = mean(mayor_obs)) |> \n  kable() |> kable_paper(full_width = FALSE)\n\n\n\n \n  \n    valor_p \n  \n \n\n  \n    0.0006662 \n  \n\n\n\n\n\nQue cuantifica que es muy poco probable observar el sistema en el estado actual si fuera cierto que no ha sufrido cambios.\n\n\n\n\n\n\nEstadísticas de prueba\n\n\n\nUna estadística de prueba es un resumen de datos, a partir del cual construimos una distribución de referencia. Distintas estadísticas miden distintos aspectos de las diferencias que puede haber entre los datos de prueba y los de referencia."
  },
  {
    "objectID": "pruebas-hipotesis.html#comparando-distribuciones",
    "href": "pruebas-hipotesis.html#comparando-distribuciones",
    "title": "5  Pruebas de hipótesis",
    "section": "Comparando distribuciones",
    "text": "Comparando distribuciones\nAhora intentamos un ejemplo más típico.\nSupongamos tenemos varias observaciones de tres poblaciones a, b y c, donde cada grupo podría tener su proceso generador de datos particular.\nNuestra pregunta es: ¿los procesos generadores de datos de estos tres grupos son iguales o hay evidencia de que son diferentes? Quizá la pregunta detrás de esta comparación es: el grupo de capturistas b recibió un entrenamiento especial. ¿Tienen menos errores de captura? La medición que comparamos es la proporción de datos erróneos que capturan en una tarea dada.\n\n\n\n\n\nEn la muestra observamos diferencias entre los grupos. Pero notamos adicionalmente que hay mucha variación dentro de cada grupo. Nos podríamos preguntar entonces si las diferencias que observamos se deben variación muestral, y no necesariamente a que los procesos generadores de datos de cada muestra son diferentes.\nPodemos construir ahora una hipótesis nula, que establece que las observaciones provienen de una población similar:\n\nLos procesos generadores de (a, b, c) son prácticamente indistiguibles. En este caso, la variación que observamos se debería a que tenemos información incompleta acerca de esas poblaciones.\n\nComo en el ejemplo anterior necesitamos construir o obtener una distribución de referencia para comparar qué tan extremos o diferentes son los datos que observamos. Esa distribución de referencia debería estar basada en el supuesto de que los grupos producen datos de distribuciones similares. En este caso, no tenemos datos de referencia históricos para comparar los grupos."
  },
  {
    "objectID": "pruebas-hipotesis.html#permutaciones-y-el-lineup",
    "href": "pruebas-hipotesis.html#permutaciones-y-el-lineup",
    "title": "5  Pruebas de hipótesis",
    "section": "Permutaciones y el lineup",
    "text": "Permutaciones y el lineup\nPara abordar este problema podemos pensar en usar permutaciones de los grupos de la siguiente forma (Box et al. (1978), Hesterberg (2015)):\n\nSi los grupos producen datos bajo procesos idénticos, entonces los grupos a, b, c solo son etiquetas que no contienen información.\nPodríamos permutar al azar las etiquetas y observar nuevamente la gráfica de caja y brazos por grupos.\nSi la hipótesis nula es cierta (procesos generadores idénticos), esta es una muestra tan verosímil como la que obtuvimos.\nAsí que podemos construir datos de referencia permutando las etiquetas de los grupos al azar, y observando la variación que ocurre.\nSi la hipótesis nula es cercana a ser cierta, no deberíamos de poder distinguir fácilmente los datos observados de los producidos con las permutaciones al azar.\n\nVamos a intentar esto, por ejemplo usando una gráfica de cuantiles simplificada. Hacemos un lineup, o una rueda de sospechosos (usamos el paquete H. Wickham, Chowdhury, y Cook (2012), ver Hadley Wickham et al. (2010)), donde 19 de los acusados son generados mediante permutaciones al azar de la variable del grupo, y el culpable (los verdaderos datos) están en una posición escogida al azar. ¿Podemos identificar los datos verdaderos? Para evitar sesgarnos, también ocultamos la etiqueta verdadera\nUsamos una gráfica que muestra los cuantiles 0.10, 0.50, 0.90:\n\nset.seed(88)\nreps <- lineup(null_permute(\"grupo\"), muestra_tab, n = 20)\n\ndecrypt(\"pPrt Zh4h Bk VyJB4Byk ub\")\n\nreps_mezcla <- reps |>  mutate(grupo_1 = factor(digest::digest2int(grupo) %% 177))\ngrafica_cuantiles(reps_mezcla, grupo_1, x) + \n    facet_wrap(~.sample, ncol = 5) + ylab(\"x\") + \n    labs(caption = \"Mediana y percentiles 10% y 90%\")+ geom_point(aes(colour = grupo_1))\n\n`summarise()` has grouped output by 'grupo_1'. You can override using the\n`.groups` argument.\n\n\n\n\n\nY la pregunta que hacemos es podemos distinguir nuestra muestra entre todas las replicaciones producidas con permutaciones?\nEjercicio: ¿dónde están los datos observados? Según tu elección, ¿qué tan diferentes son los datos observados de los datos nulos?\nEn este ejemplo, es difícil indicar cuáles son los datos. Los grupos tienen distribuciones similares y es factible que las diferencias que observamos se deban a variación muestral.\n\nSi la persona escoge los verdaderos datos, encontramos evidencia en contra de la hipótesis nula (los tres grupos son equivalentes). En algunos contextos, se dice que los datos son significativamente diferentes al nivel 0.05. Esto es evidencia en contra de que los datos se producen de manera homogénea, independientemente del grupo.\nSi la persona escoge uno de los datos permutados, no encontramos evidencia en contra de que los tres grupos producen datos con distribuciones similares."
  },
  {
    "objectID": "pruebas-hipotesis.html#comparaciones-con-lineup-2",
    "href": "pruebas-hipotesis.html#comparaciones-con-lineup-2",
    "title": "5  Pruebas de hipótesis",
    "section": "Comparaciones con lineup 2",
    "text": "Comparaciones con lineup 2\nRepetimos el ejemplo para otra muestra (en este ejemplo el proceso generador de datos es diferente para el grupo b):\n\n\n\n\n\nHacemos primero la prueba del lineup:\n\nset.seed(121)\nreps <- lineup(null_permute(\"grupo\"), muestra_tab, n = 20)\n\ndecrypt(\"pPrt Zh4h Bk VyJB4Byk uG\")\n\ngrafica_cuantiles(reps |>  mutate(grupo_escondido = factor(digest::digest2int(grupo) %% 177)), \n                             grupo_escondido, x) + facet_wrap(~.sample) + ylab(\"x\") +\n    coord_flip() + geom_point(aes(colour = grupo_escondido))\n\n`summarise()` has grouped output by 'grupo_escondido'. You can override using\nthe `.groups` argument.\n\n\n\n\n\nPodemos distinguir más o menos claramente que está localizada en valores más altos y tiene mayor dispersión. En este caso, como en general podemos identificar los datos, obtenemos evidencia en contra de que los tres grupos tienen distribuciones iguales."
  },
  {
    "objectID": "pruebas-hipotesis.html#aleatorización-e-interpretación",
    "href": "pruebas-hipotesis.html#aleatorización-e-interpretación",
    "title": "5  Pruebas de hipótesis",
    "section": "5.2 Aleatorización e interpretación",
    "text": "5.2 Aleatorización e interpretación\nEn los ejemplos anteriores, nuestras conclusiones se refieren a los procesos generadores de los datos, y no necesariamente a una relación del grupo con la variable respuesta en la población.\nPara entender esto, supongamos que a, b y c son distintos grupos de capturistas. Pero decidimos escoger en el grupo b sólo aquellos capturistas que no asistieron a los entrenamientos. Nuestra conclusión entonces es difícil de interpretar: no sabemos si la evidencia que encontramos de que en el grupo b comenten más errores se debe al hecho de pertenecer al grupo b o al hecho de que seleccionamos de una manera distinta a los capturistas del grupo b.\nHay dos maneras de establecer la relación del grupo con la respuesta según nuestras pruebas:\n\nHaciendo ciertos supuestos acerca de los procesos generadores de datos, con conocimiento de dominio.\nControlar el proceso generador de datos, escogiendo las muestras de cada grupo de manera aleatoria.\n\nSi consideramos que el proceso generador incluye el proceso de selección de las unidades, y la relación que existe entre pertenencia a un grupo y otra respuesta, cuando aleatorizamos la selección de la muestra nuestra hipótesis nula cambia:\n\nSin aleatorización: los procesos generadores de los tres grupos son iguales: esto incluye el proceso de selección de las unidades y el “efecto” del grupo a,b,c sobre la respuesta.\nCon aleatorización: las distribuciones de la variable respuesta dentro de cada grupo son iguales.\n\nY la segunda hipótesis nula es que la que generalmente nos interesa probar\n\n\n\n\n\n\nInferencia a poblaciones\n\n\n\nCuando aleatorizamos la selección de la muestra, nuestras pruebas de hipótesis son inferencias a la población de la que se extrajo la muestra."
  },
  {
    "objectID": "pruebas-hipotesis.html#prueba-de-permutaciones-para-proporciones",
    "href": "pruebas-hipotesis.html#prueba-de-permutaciones-para-proporciones",
    "title": "5  Pruebas de hipótesis",
    "section": "Prueba de permutaciones para proporciones",
    "text": "Prueba de permutaciones para proporciones\nVeremos otro ejemplo donde podemos hacer más concreta la idea de distribución nula o de referencia usando pruebas de permutaciones. Supongamos que con nuestra muestra aleatoria de tomadores de té, queremos probar la siguiente hipótesis nula:\n\nLos tomadores de té en bolsas exclusivamente usan azúcar más a tasas simillares que los tomadores de té suelto (que pueden o no también tomar té en bolsita).\n\nLos datos que obtuvimos en nuestra encuesta, en conteos, son:\n\n\n\n\nte_azucar <- tea |> select(how, sugar) |> \n  mutate(how = ifelse(how == \"tea bag\", \"bolsa_exclusivo\", \"suelto o bolsa\"))\nte_azucar |> group_by(how, sugar) |> tally() |> \n  spread(how, n) |> \n  kable() |> kable_paper(full_width = FALSE)\n\n\n\n \n  \n    sugar \n    bolsa_exclusivo \n    suelto o bolsa \n  \n \n\n  \n    No.sugar \n    81 \n    74 \n  \n  \n    sugar \n    89 \n    56 \n  \n\n\n\n\n\nY en proporciones tenemos que:\n\n\n\n\n \n  \n    how \n    prop_azucar \n    n \n  \n \n\n  \n    bolsa_exclusivo \n    0.52 \n    170 \n  \n  \n    suelto o bolsa \n    0.43 \n    130 \n  \n\n\n\n\n\nPero distintas muestras podrían haber dado distintos resultados. Nos preguntamos que tan fuerte es la evidencia en contra de que en realidad los dos grupos de personas usan azúcar en proporciones similares, y la diferencia que vemos se puede atribuir a variación muestral.\nEn este ejemplo, podemos usar una estádistica de prueba numérica, por ejemplo, la diferencia entre las dos proporciones:\n\\[p_1 - p_2\\].\n(tomadores de en bolsa solamente vs. suelto y bolsa). El proceso sería entonces:\n\nLa hipótesis nula es que los dos grupos tienen distribuciones iguales, que este caso quiere decir que en la población, tomadores de té solo en bolsa usan azúcar a las mismas tasas que tomadores de suelto o bolsas.\nBajo nuestra hipótesis nula (proporciones iguales), producimos una cantidad grande (por ejemplo 10 mil o más) de muestras permutando las etiquetas de los grupos.\nEvaluamos nuestra estadística de prueba en cada una de las muestras permutadas.\nEl conjunto de valores obtenidos nos da nuestra distribución de referencia (ya no estamos limitados a 20 replicaciones como en las pruebas gráficas).\nY la pregunta clave es: ¿el valor de la estadística en nuestra muestra es extrema en comparación a la distribución de referencia?\n\n\n# ESta función calcula la diferencia entre grupos de interés\ncalc_diferencia <- function(datos){\n  datos |>\n    mutate(usa_azucar = as.numeric(sugar == \"sugar\")) |> \n    group_by(how) |> \n    summarise(prop_azucar = mean(usa_azucar)) |> \n    spread(how, prop_azucar) |> \n    mutate(diferencia_prop = bolsa_exclusivo - `suelto o bolsa`) |> pull(diferencia_prop)\n}\n# esta función hace permutaciones y calcula la diferencia para cada una\npermutaciones_est <- function(datos, variable, calc_diferencia, n = 1000){\n  # calcular estadística para cada grupo\n  permutar <- function(variable){\n    sample(variable, length(variable))\n  }\n  tbl_perms <- tibble(.sample = seq(1, n-1, 1)) |>\n    mutate(diferencia = map_dbl(.sample, \n              ~ datos |> mutate({{variable}}:= permutar({{variable}})) |> calc_diferencia()))\n  bind_rows(tbl_perms, tibble(.sample = n, diferencia = calc_diferencia(datos)))\n}\n\nLa diferencia observada es:\n\ndif_obs <- calc_diferencia(te_azucar)\ndif_obs |> round(3)\n\n[1] 0.093\n\n\nAhora construimos nuestra distribución nula o de referencia:\n\nvalores_ref <- permutaciones_est(te_azucar, how, calc_diferencia, n = 10000)\n\nY graficamos nuestros resultados (con un histograma y una gráfica de cuantiles, por ejemplo). la estadística evaluada un cada una de nuestras muestras permutadas:\n\ng_1 <- ggplot(valores_ref, aes(sample = diferencia)) + geom_qq(distribution = stats::qunif)  +\n    xlab(\"f\") + ylab(\"diferencia\") + labs(subtitle = \"Distribución nula o de referencia\")\ng_2 <- ggplot(valores_ref, aes(x = diferencia)) + geom_histogram(binwidth = 0.04) + \n    coord_flip() + xlab(\"\") + labs(subtitle = \" \")\ngridExtra::grid.arrange(g_1, g_2, ncol = 2) \n\n\n\n\nEste es el rango de fluctuación usual para nuestra estadística bajo la hipótesis de que los dos grupos de tomadores de té tienen un mismo proceso generador de datos.\nEl valor que obtuvimos en nuestros datos es 0.0927602, que no es un valor extremo en la distribución de referencia que vimos arriba: esta muestra no aporta mucha evidencia en contra de que los grupos tienen distribuciones similares.\nPodemos graficar otra vez marcando el valor de referencia:\n\n# Función de distribución acumulada (inverso de función de cuantiles)\ndist_perm <- ecdf(valores_ref$diferencia)\n# Calculamos el percentil del valor observado\npercentil_obs <- dist_perm(dif_obs)\n\n\ng_1 <- ggplot(valores_ref, aes(sample = diferencia)) + geom_qq(distribution = stats::qunif)  +\n    xlab(\"f\") + ylab(\"diferencia\") + labs(subtitle = \"Distribución nula o de referencia\") +\n    geom_hline(yintercept = dif_obs, colour = \"red\") +\n    annotate(\"text\", x = 0.3, y = dif_obs - 0.05, label = \"diferencia observada\", colour = \"red\")\ng_2 <- ggplot(valores_ref, aes(x = diferencia)) + geom_histogram(binwidth = 0.04) + \n    coord_flip() + xlab(\"\") + labs(subtitle = \" \") +\n    geom_vline(xintercept = dif_obs, colour = \"red\") +\n    annotate(\"text\", x = dif_obs, y = 2000, label = percentil_obs,vjust = -0.2, colour = \"red\")\ngridExtra::grid.arrange(g_1, g_2, ncol = 2) \n\n\n\n\nY vemos que es un valor algo (pero no muy) extremo en la distribución de referencia que vimos arriba: esta muestra no aporta una gran cantidad de evidencia en contra de que los grupos tienen distribuciones similares, que en este caso significa que los dos grupos usan azúcar a tasas similares.\n\nValor p\nNótese que calculamos una cantidad adicional, que es el percentil donde nuestra observación cae en la distribución generada por las permutación. Esta cantidad puede usarse para calcular un valor p. Podemos calcular, por ejemplo:\n\nValor p de dos colas: Si la hipótesis nula es cierta, ¿cuál es la probabilidad de observar una diferencia tan extrema o más extrema de lo que observamos?\n\nConsiderando en este caso interpretamos extrema como que cae lejos de donde a mayoría de la distribución se concentra, podemos calcular el valor p como sigue. A partir de el valor observado, consideramos cuál dato es menor: la probabilidad bajo lo hipótesis nula de observar una diferencia mayor de a que observamos, o la probabilidad de observar una diferencia menor a la que observamos. Tomamos el mínimo y multiplicamos por dos (Hesterberg (2015)):\n\n2 * min(dist_perm(dif_obs), (1 - dist_perm(dif_obs)))\n\n[1] 0.1026\n\n\nEste valor p se considera como evidencia “moderada” en contra de la hipótesis nula. Valores p más chicos (observaciones más extremas en comparación con la referencia) aportan más evidencia en contra de la hipótesis de que los grupos de tomadores de té , y valores más grandes aportan menos evidencia."
  },
  {
    "objectID": "pruebas-hipotesis.html#tomadores-de-té-2",
    "href": "pruebas-hipotesis.html#tomadores-de-té-2",
    "title": "5  Pruebas de hipótesis",
    "section": "Tomadores de té 2",
    "text": "Tomadores de té 2\nAhora hacemos una prueba de permutaciones otro par de proporciones con el mismo método. Ahora comparamos tomadores de té Earl Gray y de té negro según sus tasas de uso de azúcar.\nLos datos que obtuvimos en nuestra encuesta, en conteos, son:\n\n\n\n\n \n  \n    sugar \n    black \n    Earl Grey \n  \n \n\n  \n    No.sugar \n    51 \n    84 \n  \n  \n    sugar \n    23 \n    109 \n  \n\n\n\n\n\nY en porcentajes tenemos que:\n\nprop_azucar <- te_azucar |> group_by(Tea, sugar) |> tally() |> \n  group_by(Tea) |> mutate(prop = 100 * n / sum(n), n = sum(n)) |> \n  filter(sugar == \"sugar\") |> select(Tea, prop_azucar = prop, n) |> \n  mutate('% usa azúcar' = round(prop_azucar)) |> select(-prop_azucar)\nprop_azucar |> formatear_tabla()\n\n\n\n \n  \n    Tea \n    n \n    % usa azúcar \n  \n \n\n  \n    black \n    74 \n    31 \n  \n  \n    Earl Grey \n    193 \n    56 \n  \n\n\n\n\n\nPero distintas muestras podrían haber dado distintos resultados. Nos preguntamos que tan fuerte es la evidencia en contra de que en realidad los dos grupos de personas usan azúcar en proporciones similares, y la diferencia que vemos se puede atribuir a variación muestral.\nEscribimos la función que calcula diferencias para cada muestra:\n\ncalc_diferencia_2 <- function(datos){\n  datos |>\n    mutate(usa_azucar = as.numeric(sugar == \"sugar\")) |> \n    group_by(Tea) |> \n    summarise(prop_azucar = mean(usa_azucar)) |> \n    spread(Tea, prop_azucar) |> \n    mutate(diferencia_prop = `Earl Grey` - black) |> pull(diferencia_prop)\n}\n\nLa diferencia observada es:\n\n\n[1] 0.254\n\n\nAhora construimos nuestra distribución nula o de referencia:\n\nset.seed(2)\nvalores_ref <- permutaciones_est(te_azucar, Tea, calc_diferencia_2, n = 10000)\n\nY podemos graficar la distribución de referencia otra vez marcando el valor observado\n\n\n\n\n\n\n\n\nEn este caso, la evidencia es muy fuerte en contra de la hipótesis nula, pues el resultado que obtuvimos es muy extremo en relación a la distribución de referencia. El valor p es cercano a 0."
  },
  {
    "objectID": "pruebas-hipotesis.html#ejemplo-tiempos-de-fusión",
    "href": "pruebas-hipotesis.html#ejemplo-tiempos-de-fusión",
    "title": "5  Pruebas de hipótesis",
    "section": "Ejemplo: tiempos de fusión",
    "text": "Ejemplo: tiempos de fusión\nConsideremos el ejemplo de fusión de estereogramas que vimos anteriormente. Una pregunta que podríamos hacer es: considerando que hay mucha variación en el tiempo de fusión que depende de las personas, necesitamos calificar la evidencia de nuestra conclusión (el tiempo de fusión se reduce con información verbal).\nPodemos usar una prueba de permutaciones, esta vez justificándola por el hecho de que los tratamientos se asignan al azar: si los tratamientos son indistinguibles, entonces las etiquetas de los grupos son solo etiquetas, y permutarlas daría muestras igualmente verosímiles.\nEn este caso, compararemos gráficas de cuantiles de los datos con los producidos por permutaciones:\n\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  n = col_double(),\n  time = col_double(),\n  nv.vv = col_character()\n)\n\n\ndecrypt(\"pPrt Zh4h Bk VyJB4Byk uu\")\n\n\n\n\n\nEjercicio: ¿Podemos identificar los datos? En general, muy frecuentemente las personas identifican los datos correctamente, lo que muestra evidencia considerable de que la instrucción verbal altera los tiempos de respuesta de los partipantes, y en este caso ayuda a reducir el tiempo de fusión de los estereogramas.\n\n\n\n\n\n\nAleatoriación en experimentos\n\n\n\nEn este caso, no usamos una muestra aleatoria de la población, de forma que nuestro resultado no aplica necesariamente para la población general. Sin embargo, es una inferencia válida acerca del tratamiento, pues asignamos al azar el tratamiento\n\n\nEn este caso, tenemos evidencia de que la diferencia se debe al tratamiento y no sólo variación muestral dentro de esta muestra. Si no aleatorizamos el tratamiento (por ejemplo, cada persona escoge si quiere ayuda verbal o no), no podríamos concluir para el efecto del tratamiento."
  },
  {
    "objectID": "pruebas-hipotesis.html#ejemplo-tiempos-de-fusión-2",
    "href": "pruebas-hipotesis.html#ejemplo-tiempos-de-fusión-2",
    "title": "5  Pruebas de hipótesis",
    "section": "Ejemplo: tiempos de fusión 2",
    "text": "Ejemplo: tiempos de fusión 2\nPodemos usar las pruebas de permutaciones para distintos de tipos de estadísticas: medianas, medias, comparar dispersión usando rangos intercuartiles o varianzas, etc.\nRegresamos a los tiempos de fusión. Podemos hacer una prueba de permutaciones para la diferencia de las medias o medianas, por ejemplo. En este ejemplo usaremos una medida de centralidad un poco diferente, como ilustración: el promedio de los cuartiles superior e inferior de las dos distribuciones. Usaremos el cociente de estas dos cantidades para medir su diferencia\n\nstat_fusion <- function(x){\n    (quantile(x, 0.75) + quantile(x, 0.25))/2\n}\ncalc_fusion <- function(stat_fusion){\n  fun <- function(datos){\n    datos |> \n      group_by(nv.vv) |> \n      summarise(est = stat_fusion(time)) |> \n      spread(nv.vv, est) |> mutate(dif = VV / NV ) |> pull(dif)\n  }\n  fun\n}\n\n\ncalc_cociente <- calc_fusion(stat_fusion)\ndif_obs <- calc_cociente(fusion)\n# permutar\nvalores_ref <- permutaciones_est(fusion, nv.vv, calc_cociente, n = 10000)\ndist_perm_nv <- ecdf(valores_ref$diferencia) \ncuantil_obs <- dist_perm_nv(dif_obs)\n\n\n\n\n\n\nY el valor p de dos colas es\n\ndist_perm_nv <- ecdf(valores_ref$diferencia)\n2 * min(dist_perm_nv(dif_obs), 1- dist_perm_nv(dif_obs))\n\n[1] 0.0354\n\n\nLo que muestra evidencia considerable, aunque no muy fuerte, de que la instrucción verbal ayuda a reducir el tiempo de fusión de los estereogramas: la caja del diagrama de caja y brazos para el grupo VV está encogida por un factor menor a 1."
  },
  {
    "objectID": "pruebas-hipotesis.html#tipos-de-estudio",
    "href": "pruebas-hipotesis.html#tipos-de-estudio",
    "title": "5  Pruebas de hipótesis",
    "section": "5.3 Tipos de estudio",
    "text": "5.3 Tipos de estudio\nEl alcance de la inferencia en cada caso es diferente dependiendo del proceso generador de los datos y qué tanto lo controlamos o lo conocemos, como explica esta imagen de Ramsey y Schafer (2002)\n\n\n\nTipos de estudio e inferencia\n\n\nEl cuadro en la esquina superior izquierda es donde el análisis es más simple y los resultados son más fáciles de interpretar. Es posible hacer análisis fuera de este cuadro, pero el proceso es más complicado, requieren más supuestos, conocimiento del dominio y habilidades de análisis. En general resultan conclusiones menos sólidas. Muchas veces no nos queda otra opción más que trabajar fuera del cuadro ideal.\nComo ejercicio ubica en los cuadros los siguientes tipos de estudios:\n\nPruebas clínicas para vacuna para aplicar a la población general\nEntre los que tienen prueba positiva, ver la relación entre ser fumador y posibilidad de hospitalización por COVID para dar recomendaciones a la población general.\nAnalizar cómo afecta tener seguro médico a los ingresos, usando datos del ENIGH, para entender cómo ayudaría tener seguro médico universal.\nSeleccionar dos muestras al azar de capturistas, y asignarnos al azar a dos tipos de entrenamiento para medir su desempeño en una tarea específica."
  },
  {
    "objectID": "pruebas-hipotesis.html#supuestos-para-pruebas-de-permutaciones",
    "href": "pruebas-hipotesis.html#supuestos-para-pruebas-de-permutaciones",
    "title": "5  Pruebas de hipótesis",
    "section": "Supuestos para pruebas de permutaciones",
    "text": "Supuestos para pruebas de permutaciones\nLa pruebas de permutaciones son más útiles cuando nuestra hipótesis nula se refiere que la distribución de los grupos son muy similares, o la independencia entre observaciones y grupo. Esto también aplica cuando queremos probar por ejemplo, que una variable numérica Y es independiente de X.\n\nHay algunas hipótesis que no se pueden probar con este método, como por ejemplo, las que se refieren a una sola muestra: ¿los datos son consistentes con que su media es igual a 5?\nAdicionalmente, en algunas ocasiones queremos probar aspectos más específicos de las diferencias: como ¿son iguales las medias o medianas de dos grupos de datos? ¿Tienen dispersión similar?\n\nLas pruebas de permutaciones no están tan perfectamente adaptadas a este problema, pues prueban todos los aspectos de las distribuciones que se comparan, aún cuando escogamos una estadística particular que pretende medir, por ejemplo, diferencia de medias. Eso quiere decir que podemos rechazar igualdad de medias, por ejemplo, cuando en realidad otra característica de las distribuciones es la que difiere mucho en las poblaciones\nEn algunas referencias (ver Chihara y Hesterberg (2018), Efron y Tibshirani (1993)) se argumenta que de todas formas las pruebas de permutaciones son relativamente robustas a esta desadaptación. Un caso excepcional, por ejemplo, es cuando las poblaciones que comparamos resultan tener dispersión extremadamente distinta, y adicionalmente los tamaños de muestra de los grupos son muy desiguales (otra vez, ver ejemplos en Chihara y Hesterberg (2018))."
  },
  {
    "objectID": "pruebas-hipotesis.html#separación-de-grupos",
    "href": "pruebas-hipotesis.html#separación-de-grupos",
    "title": "5  Pruebas de hipótesis",
    "section": "Separación de grupos",
    "text": "Separación de grupos\nEste ejemplo tomado de Chowdhury et al. (2015) (tanto la idea como el código). La pregunta que se aborda en ese estudio es:\n\nExisten métodos de clasificación (supervisados o no supervisados) para formar grupos en términos de variables que describen a los individuos\nEstos métodos (análisis discriminante, o k-means, por ejemplo), pretenden formar grupos compactos, bien separados entre ellos. Cuando aplicamos el método, obtenemos clasificadores basados en las variables de entrada.\nLa pregunta es: ¿los grupos resultantes son producto de patrones que se generalizan a la población, o capitalizaron en variación aleatoria para formarse?\nEspecialmente cuando tenemos muchas mediciones de los individuos, y una muestra relativamente chica, Es relativamente fácil encontrar combinaciones de variables que separan los grupos, aunque estas combinaciones y diferencias están basadas en ruido y no generalizan a la población.\n\nComo muestran en Chowdhury et al. (2015), el lineup es útil para juzgar si tenemos evidencia en contra de que los grupos en realidad son iguales, y usamos variación muestral para separarlos.\n\nAvispas (opcional)\nEn el siguiente ejemplo, tenemos 4 grupos de avispas (50 individuos en total), y para cada individuo se miden expresiones de 42 genes distintos. La pregunta es: ¿Podemos separar a los grupos de avispas dependiendo de sus mediciones?\nEn este se usó análisis discriminante para buscar proyecciones de los datos en dimensión baja de forma que los grupos sean lo más compactos y separados posibles.\nPara probar qué tan bien funciona este método, podemos hacer una prueba de permutación, aplicamos LDA y observamos los resultados.\n\n\n\n\n\nY vemos que incluso permutando los grupos, es generalmente posible separarlos en grupos bien definidos: la búsqueda es suficientemente agresiva para encontrar combinaciones lineales que los separan. Que no podamos distinguir los datos verdaderos de las replicaciones nulas indica que este método difícilmente puede servir para separar los grupos claramente.\nOtro enfoque sería separar los datos en una muestra de entrenamiento y una de prueba (que discutiremos en la última sesión). Aplicamos el procedimiento a la muestra de entrenamiento y luego vemos qué pasa con los datos de prueba:\n\nset.seed(8)\nwasps_1 <- wasps |> mutate(u = runif(nrow(wasps), 0, 1))\nwasps_entrena <- wasps_1 |> filter(u <= 0.8)\nwasps_prueba <- wasps_1 |> filter(u > 0.8)                            \n                            \nwasp.lda <- MASS::lda(Group ~ ., data=wasps_entrena[,-1])\nwasp_ld_entrena <- predict(wasp.lda,  dimen=2)$x |> \n    as_tibble(.name_repair = \"universal\") |>\n     mutate(tipo = \"entrenamiento\") |> \n    mutate(grupo = wasps_entrena$Group)\nwasp_ld_prueba <- predict(wasp.lda, newdata = wasps_prueba, dimen=2)$x  |> \n    as_tibble(.name_repair = \"universal\") |>\n    mutate(tipo = \"prueba\")|> \n    mutate(grupo = wasps_prueba$Group)\nwasp_lda <- bind_rows(wasp_ld_entrena, wasp_ld_prueba)\nggplot(wasp_lda, aes(x = LD1, y = LD2, colour = grupo)) + geom_point(size = 3) +\n    facet_wrap(~tipo) + scale_color_colorblind()\n\n\n\n\nAunque esta separación de datos es menos efectiva en este ejemplo por la muestra chica, podemos ver que la separación lograda en los datos de entrenamiento probablemente se debe a variación muestral."
  },
  {
    "objectID": "pruebas-hipotesis.html#la-crisis-de-replicabilidad",
    "href": "pruebas-hipotesis.html#la-crisis-de-replicabilidad",
    "title": "5  Pruebas de hipótesis",
    "section": "La “crisis de replicabilidad”",
    "text": "La “crisis de replicabilidad”\nRecientemente (Ioannidis (2005)) se ha reconocido en campos como la sicología la crisis de replicabilidad. Varios estudios que recibieron mucha publicidad inicialmente no han podido ser replicados posteriormente por otros investigadores. Por ejemplo:\n\nHacer poses poderosas produce cambios fisiológicos que mejoran nuestro desempeño en ciertas tareas\nMostrar palabras relacionadas con “viejo” hacen que las personas caminen más lento (efectos de priming)\n\nEn todos estos casos, el argumento de la evidencia de estos efectos fue respaldada por una prueba de hipótesis nula con un valor p menor a 0.05. La razón es que ese es el estándar de publicación seguido por varias áreas y revistas. La tasa de no replicabilidad parece ser mucho más alta (al menos la mitad o más según algunas fuentes, como la señalada arriba) que lo sugeriría la tasa de falsos positivos (menos de 5%)\nEste problema de replicabilidad parece ser más frecuente cuando:\n\nSe trata de estudios de potencia baja: mediciones ruidosas y tamaños de muestra chicos.\nEl plan de análisis no está claramente definido desde un principio (lo cual es difícil cuando se están investigando “fenómenos no estudiados antes”)\n\n¿A qué se atribuye esta crisis de replicabilidad?"
  },
  {
    "objectID": "pruebas-hipotesis.html#el-jardín-de-los-senderos-que-se-bifurcan",
    "href": "pruebas-hipotesis.html#el-jardín-de-los-senderos-que-se-bifurcan",
    "title": "5  Pruebas de hipótesis",
    "section": "El jardín de los senderos que se bifurcan",
    "text": "El jardín de los senderos que se bifurcan\nAunque haya algunos ejemplos de manipulaciones conscientes –e incluso, en menos casos, malintencionadas– para obtener resultados publicables o significativos (p-hacking), como vimos en ejemplos anteriores, hay varias decisiones, todas razonables, que podemos tomar cuando estamos buscando las comparaciones correctas. Algunas pueden ser:\n\nTransformar los datos (tomar o no logaritmos, u otra transformación)\nEditar datos atípicos (razonable si los equipos pueden fallar, o hay errores de captura, por ejemplo)\nDistintas maneras de interpretar los criterios de inclusión de un estudio (por ejemplo, algunos participantes mostraron tener gripa, o revelaron que durmieron muy poco la noche anterior, etc. ¿los dejamos o los quitamos?)\n\nDado un conjunto de datos, las justificaciones de las decisiones que se toman en cada paso son razonables, pero con datos distintos las decisiones podrían ser diferentes. Este es el jardín de los senderos que se bifurcan Gelman, que invalida en parte el uso valores p como criterio de evidencia contra la hipótesis nula.\nEsto es exacerbado por:\n\nTamaños de muestra chicos y efectos “inestables” que se quieren medir (por ejemplo en sicología)\nEl hecho de que el criterio de publicación es obtener un valor p < 0.05, y la presión fuerte sobre los investigadores para producir resultados publicables (p < 0.05)\nEl que estudios o resultados similares que no obtuvieron valores \\(p\\) por debajo del umbral no son publicados o reportados.\n\nVer por ejemplo el comunicado de la ASA.\nOjo: esas presiones de publicación no sólo ocurre para investigadores en sicología. Cuando trabajamos en problemas de análisis de datos en problemas que son de importancia, es común que existan intereses de algunas partes o personas involucradas por algunos resultados u otros (por ejemplo, nuestros clientes de consultoría o clientes internos). Eso puede dañar nuestro trabajo como analistas, y el avance de nuestro equipo. Aunque esas presiones son inevitables, se vuelven manejables cuando hay una relación de confianza entre las partes involucradas."
  },
  {
    "objectID": "pruebas-hipotesis.html#ejemplo-decisiones-de-análisis-y-valores-p",
    "href": "pruebas-hipotesis.html#ejemplo-decisiones-de-análisis-y-valores-p",
    "title": "5  Pruebas de hipótesis",
    "section": "Ejemplo: decisiones de análisis y valores p",
    "text": "Ejemplo: decisiones de análisis y valores p\nEn el ejemplo de datos de fusión, decidimos probar, por ejemplo, el promedio de los cuartiles inferior y superior, lo cual no es una decisión típica pero usamos como ilustración. Ahora intentamos usar distintas mediciones de la diferencia entre los grupos, usando distintas medidas resumen y transformaciones (por ejemplo, con o sin logaritmo). Aquí hay unas 12 combinaciones distintas para hacer el análisis (multiplicadas por criterios de “aceptación de datos en la muestra”, que simulamos tomando una submuestra al azar):\n\ncalc_fusion <- function(stat_fusion, trans, comparacion){\n  fun <- function(datos){\n    datos |> \n      group_by(nv.vv) |> \n      summarise(est = stat_fusion({{ trans }}(time))) |> \n      spread(nv.vv, est) |> mutate(dif = {{ comparacion }}) |> pull(dif)\n  }\n  fun\n}\nvalor_p <- function(datos, variable, calc_diferencia, n = 1000){\n  # calcular estadística para cada grupo\n  permutar <- function(variable){\n    sample(variable, length(variable))\n  }\n  tbl_perms <- tibble(.sample = seq(1, n-1, 1)) |>\n    mutate(diferencia = map_dbl(.sample, \n              ~ datos |> mutate({{variable}} := permutar({{variable}})) |> calc_diferencia()))\n  perms <- bind_rows(tbl_perms, tibble(.sample = n, diferencia = calc_diferencia(datos)))\n  perms_ecdf <- ecdf(perms$diferencia)\n  dif <- calc_diferencia(datos)\n  2 * min(perms_ecdf(dif), 1- perms_ecdf(dif))\n}\n\n\nset.seed(7272)\nmedia_cuartiles <- function(x){\n    (quantile(x, 0.75) + quantile(x, 0.25))/2\n}\n# nota: usar n=10000 o más, esto solo es para demostración:\ncalc_dif <- calc_fusion(mean, identity, VV - NV)\nvalor_p(fusion |> sample_frac(0.95), nv.vv, calc_dif, n = 1000)\n\n[1] 0.072\n\ncalc_dif <- calc_fusion(mean, log, VV - NV)\nvalor_p(fusion |> sample_frac(0.95), nv.vv, calc_dif, n = 1000)\n\n[1] 0.024\n\ncalc_dif <- calc_fusion(median, identity, VV / NV)\nvalor_p(fusion |> sample_frac(0.95), nv.vv, calc_dif, n = 1000)\n\n[1] 0.016\n\ncalc_dif <- calc_fusion(media_cuartiles, identity, VV / NV)\nvalor_p(fusion |> sample_frac(0.95), nv.vv, calc_dif, n = 1000)\n\n[1] 0.026\n\n\nSi existen grados de libertad - muchas veces necesarios para hacer un análisis exitoso-, entonces los valores p pueden tener poco significado."
  },
  {
    "objectID": "pruebas-hipotesis.html#alternativas-o-soluciones",
    "href": "pruebas-hipotesis.html#alternativas-o-soluciones",
    "title": "5  Pruebas de hipótesis",
    "section": "Alternativas o soluciones",
    "text": "Alternativas o soluciones\nEl primer punto importante es reconocer que la mayor parte de nuestro trabajo es exploratorio (recordemos el proceso complicado del análisis de datos de refinamiento de preguntas). En este tipo de trabajo, reportar valores p puede tener poco sentido, y mucho menos tiene sentido aceptar algo “verdadero” cuando pasa un umbral de significancia dado.\nNuestro interés principal al hacer análisis es expresar correctamente y de manera útil la incertidumbre asociada a las conclusiones o patrones que mostramos (asociada a variación muestral, por ejemplo) para que el proceso de toma de decisiones sea informado. Un resumen de un número (valor p, o el que sea) no puede ser tomado como criterio para tomar una decisión que generalmente es compleja. En la siguiente sección veremos cómo podemos mostrar parte de esa incertidumbre de manera más útil.\nPor otra parte, los estudios confirmatorios (donde se reportan valores p) también tienen un lugar. En áreas como la sicología, existen ahora movimientos fuertes en favor de la repetición de estudios prometedores pero donde hay sospecha de grados de libertad del investigador. Este movimiento sugiere dar valor a los estudios exploratorios que no reportan valor p, y posteriormente, si el estudio es de interés, puede intentarse una replicación confirmatoria, con potencia más alta y con planes de análisis predefinidos.\n\n\n\n\nBox, George EP, William H Hunter, Stuart Hunter, et al. 1978. Statistics for experimenters. Vol. 664. John Wiley; sons New York.\n\n\nChihara, Laura M., y Tim C. Hesterberg. 2018. Mathematical Statistics with Resampling and R. 2.ª ed. Hoboken, NJ: John Wiley & Sons. https://sites.google.com/site/chiharahesterberg/home.\n\n\nChowdhury, Niladri Roy, Dianne Cook, Heike Hofmann, Mahbubul Majumder, Eun-Kyung Lee, y Amy L Toth. 2015. «Using visual statistical inference to better understand random class separations in high dimension, low sample size data». Computational Statistics 30 (2): 293-316.\n\n\nEfron, B., y R. Tibshirani. 1993. «An Introduction to the Bootstrap». Miscellaneous. Macmillan Publishers Limited. All rights reserved.\n\n\nHesterberg, Tim C. 2015. «What teachers should know about the bootstrap: Resampling in the undergraduate statistics curriculum». The American Statistician 69 (4): 371-86.\n\n\nIoannidis, John PA. 2005. «Why most published research findings are false». PLoS medicine 2 (8): e124.\n\n\nRamsey, Fred, y Daniel Schafer. 2002. The Statistical Sleuth: A Course in Methods of Data Analysis. 2.ª ed. Duxbury Press. http://www.amazon.com/Statistical-Sleuth-Course-Methods-Analysis/dp/0534386709.\n\n\nWickham, Hadley, Dianne Cook, Heike Hofmann, y Andreas Buja. 2010. «Graphical inference for infovis». IEEE Transactions on Visualization and Computer Graphics 16 (6): 973-79.\n\n\nWickham, H, NR Chowdhury, y D Cook. 2012. «nullabor: Tools for graphical inference». R package version 0.2 1: 213."
  },
  {
    "objectID": "remuestreo-bootstrap.html#ejemplo-estimación-e-intervalos-de-confianza",
    "href": "remuestreo-bootstrap.html#ejemplo-estimación-e-intervalos-de-confianza",
    "title": "6  Remuestreo",
    "section": "Ejemplo: estimación e intervalos de confianza",
    "text": "Ejemplo: estimación e intervalos de confianza\nRegresamos a nuestro ejemplo anterior donde muestreamos 3 grupos, y nos preguntábamos acerca de la diferencia de sus medianas. En lugar de hacer pruebas de permutaciones (con gráficas o numéricas), podríamos considerar qué tan precisa es cada una de nuestras estimacione para las medianas de los grupos, por ejemplo.\nNuestros resultados podríamos presentarlos como sigue:\n\n\n\n\n\nDonde en rojo está nuestro estimador puntual de la mediana de cada grupo (la mediana muestral), y las rectas mustran un intervalo de 95% para nuestra estimación de la mediana: esto quiere decir que los valores poblacionales tienen probabilidad aproximada de 95% de estar dentro del intervalo.\nEste análisis comunica correctamente que tenemos incertidumbre alta acerca de nuestras estimaciones (especialmente grupos b y c), y que no tenemos mucha evidencia de que el grupo b tenga una mediana poblacional considerablemente más alta que a o c."
  },
  {
    "objectID": "remuestreo-bootstrap.html#interpretación-de-intervalos-de-confianza",
    "href": "remuestreo-bootstrap.html#interpretación-de-intervalos-de-confianza",
    "title": "6  Remuestreo",
    "section": "Interpretación de intervalos de confianza",
    "text": "Interpretación de intervalos de confianza\nGeneralmente, “intervalo de confianza” (de 90% de confianza, por ejemplo) significa, desde el punto de vista frecuentista:\n\nCada muestra produce un intervalo distinto. Para el 90% de las muestras posibles, el intervalo cubre al valor poblacional.\nAsí que con alta probabilidad, el valor poblacional está dentro del intervalo.\nIntervalos más anchos nos dan más incertidumbre acerca de dónde está el verdadero valor poblacional (y al revés para intervalos más angostos)\n\nExisten también “intervalos creíbles” (de 90% de probabilidad, por ejemplo), que se interpetan de forma bayesiana:\n\nCon alta probabilidad, creemos que el valor poblacional está dentro del intervalo creíble.\n\nLa técnica que veremos a continuación (bootstrap) se puede interpretar de las dos maneras.\n\nLa interpretación bayesiana puede ser más natural\nLa interpretación frecuentista nos da maneras empíricas de probar si los intervalos de confianza están bien calibrados o no: es un mínimo que “intervalos del 90%” debería satisfacer.\n\nAsí que tomamos el punto de vista bayesiano en la intepretación, pero buscamos que nuestros intervalos cumplan o aproximen bien garantías frecuentistas (discutimos esto más adelante)."
  },
  {
    "objectID": "remuestreo-bootstrap.html#cómo-producir-intervalos-para-estimación",
    "href": "remuestreo-bootstrap.html#cómo-producir-intervalos-para-estimación",
    "title": "6  Remuestreo",
    "section": "Cómo producir intervalos para estimación",
    "text": "Cómo producir intervalos para estimación\nExisten muchas técnicas para construir estos intervalos que muestran la incertidumbre en nuestras estimaciones: métodos basados en distribuciones estándar, métodos paramétricos y no paramétricos, distintos métodos bayesianos (entonces se llaman intervalos creíbles o de probabilidad), etc.\nEn este curso, como ejemplo, y también por ser una técnica versátil, presentaremos el bootstrap no paramétrico (ver Efron y Tibshirani (1993)), donde utilizaremos simulación (y poder de cómputo) para producir este tipo de intervalos, bajo ciertas condiciones de extracción de la muestra que discutiremos más adelante."
  },
  {
    "objectID": "remuestreo-bootstrap.html#distribución-de-muestreo",
    "href": "remuestreo-bootstrap.html#distribución-de-muestreo",
    "title": "6  Remuestreo",
    "section": "Distribución de Muestreo",
    "text": "Distribución de Muestreo\nSupongamos que consideramos la población de casas de nuestro ejemplo anterior\n\ncasas_pob <- casas |> select(id, precio_miles, nombre_zona)\ncasas_pob |> sample_n(20) |> formatear_tabla()\n\n\n\n \n  \n    id \n    precio_miles \n    nombre_zona \n  \n \n\n  \n    721 \n    275.0 \n    StoneBr \n  \n  \n    237 \n    185.5 \n    CollgCr \n  \n  \n    270 \n    148.0 \n    Edwards \n  \n  \n    269 \n    120.5 \n    IDOTRR \n  \n  \n    362 \n    145.0 \n    BrkSide \n  \n  \n    1152 \n    149.9 \n    Edwards \n  \n  \n    973 \n    99.5 \n    SawyerW \n  \n  \n    1037 \n    315.5 \n    Timber \n  \n  \n    1092 \n    160.0 \n    Somerst \n  \n  \n    1286 \n    132.5 \n    BrkSide \n  \n  \n    1208 \n    200.0 \n    CollgCr \n  \n  \n    540 \n    272.0 \n    CollgCr \n  \n  \n    367 \n    159.0 \n    NAmes \n  \n  \n    283 \n    207.5 \n    NridgHt \n  \n  \n    52 \n    114.5 \n    BrkSide \n  \n  \n    761 \n    127.5 \n    NAmes \n  \n  \n    1297 \n    155.0 \n    NAmes \n  \n  \n    822 \n    93.0 \n    OldTown \n  \n  \n    956 \n    145.0 \n    Crawfor \n  \n  \n    790 \n    187.5 \n    ClearCr \n  \n\n\n\n\n\nY nos interesa saber, para la población, cuál es la mediana de los precios de casas. Suponemos que no tenemos acceso a los datos poblacionales, y decidimos diseñar una encuesta para tomar una muestra de 50 casas que fueron vendidas en cierto periodo. Suponemos una muestra aleatoria simple con reemplazo (la población es grande y no hay mucha diferencia entre hacerlo con o sin reemplazo) de tamaño fijo, por ejemplo \\(n = 50\\)\nBuscamos estimar la mediana poblacional con la mediana de nuestra muestra:\n\nfun_muestra <- function(x){\n    median(x)\n}\n\nComo es de esperarse, distintas muestras dan distintas estimaciones de la mediana\n\ncasas_pob |> sample_n(50, replace = T) |> summarise(mediana = fun_muestra(precio_miles))\n\n# A tibble: 1 × 1\n  mediana\n    <dbl>\n1     156\n\ncasas_pob |> sample_n(50, replace = T) |> summarise(mediana = fun_muestra(precio_miles))\n\n# A tibble: 1 × 1\n  mediana\n    <dbl>\n1     145\n\n\nEn estimación, uno de los conceptos básicos el de la distribución de muestreo. La distribución de muestreo son los valores que puede tomar nuestro estimador bajo todas las posibles muestras que pudiéramos obtener.\n¿Por qué es importante este concepto? La distribución de muestreo del estimador nos indica qué tan lejos o cerca vamos a caer del verdadero valor poblacional que queremos estimar. No sabemos qué muestra vamos a obtener, pero con la distribución de muestreo podemos saber qué tan mal o bien nos puede ir y con qué probabilidades."
  },
  {
    "objectID": "remuestreo-bootstrap.html#aproximando-la-distribución-de-muestreo",
    "href": "remuestreo-bootstrap.html#aproximando-la-distribución-de-muestreo",
    "title": "6  Remuestreo",
    "section": "Aproximando la distribución de muestreo",
    "text": "Aproximando la distribución de muestreo\nEn nuestro ejemplo tenemos la población (esto normalmente no es cierto) y podemos extraer un número muy grande de muestras de tamaño 50. Calculamos el estimador para cada una de esas muestras. El código es simple:\n\n# Repetir 5000 veces\nmediana_muestras <- map_dbl(1:5000, ~ casas_pob |> \n    sample_n(50, replace = T) |>  # muestra de 50\n    summarise(mediana_precio = fun_muestra(precio_miles)) |> pull(mediana_precio)) # calcular mediana de la muestra\n\nAhora examinamos la distribución de los valores que obtuvimos:\n\nsims_dm <- tibble(muestra = 1:length(mediana_muestras), mediana_precio = mediana_muestras)\nvalor_poblacional <- median(casas$precio_miles) \nggplot(sims_dm, aes(sample = mediana_muestras)) + geom_qq(distribution = stats::qunif) +\n    ylab(\"Mediana muestral\") + xlab(\"f\") + labs(subtitle = \"Distribución de muestreo para mediana (n = 50)\") +\n    geom_hline(yintercept = valor_poblacional, colour = \"red\") +\n    annotate(\"text\", x = 0.2, y = valor_poblacional+5, label = \"Mediana poblacional\", colour = \"red\")\n\n\n\n\n\nCon esta gráfica podemos juzgar qué tan lejos puede caer nuestra estimación muestral del valor poblacional. Cuanto más concentrada esté alrededor del valor poblacional, la probabilidad es más alta de que obtengamos una estimación precisa cuando tomemos una muestra particular. Podemos hacer un histograma también:\n\n\n\n\n\n\nLos cuantiles que cubren a un 95% de las muestras son:\n\nquantile(mediana_muestras - valor_poblacional , c(0.025, 0.975)) |> round(1)\n\n 2.5% 97.5% \n-21.5  21.9 \n\n\n\nEsto quiere decir que si estimamos con una muestra el valor poblacional, esperamos con 95% de probabilidad que el error sea menos de unas 20 unidades. *Esta es la precisión de nuestro estimador.\n\nSi usamos una muestra más grande (n = 200, por ejemplo) podemos obtener un resultado más preciso:\n\n\n\n\n\nY como es de esperarse, vemos que muestras más grandes resultan en menos variablidad, y menor error de estimación.\n\nMejores distribuciones de muestreo: más concentradas alrededor del verdadero valor poblacional"
  },
  {
    "objectID": "remuestreo-bootstrap.html#distribución-de-muestreo-y-distribución-poblacional",
    "href": "remuestreo-bootstrap.html#distribución-de-muestreo-y-distribución-poblacional",
    "title": "6  Remuestreo",
    "section": "Distribución de muestreo y distribución poblacional",
    "text": "Distribución de muestreo y distribución poblacional\nUna confusión inicial que es común es entre la distribución de muestreo y la distribución poblacional. La poblacional muestra cómo se distribuyen los valores de la variable de interés:\n\nggplot(casas_pob, aes(x = precio_miles)) + geom_histogram() +\n    geom_vline(xintercept = valor_poblacional)\n\n\n\n\nQue es muy diferente que las distribuciones de muestreo de nuestros dos estimadores:\n\ng_dist_muestreo"
  },
  {
    "objectID": "remuestreo-bootstrap.html#el-mundo-bootstrap",
    "href": "remuestreo-bootstrap.html#el-mundo-bootstrap",
    "title": "6  Remuestreo",
    "section": "El mundo bootstrap",
    "text": "El mundo bootstrap\nEl problema que tenemos ahora es que normalmente sólo tenemos una muestra, así que no es posible calcular las distribuciones de muestreo como hicimos arriba. Sin embargo, podemos hacer lo siguiente:\n\nSi tuviéramos la distribución poblacional, simulamos muestras para aproximar la distribución de muestreo de nuestro estimador, y así entender su variabilidad.\nPero no tenemos la distribución poblacional\nSin embargo, podemos estimar la distribución poblacional con nuestros valores muestrales\n\nMundo bootstrap\n\nSi usamos la estimación del inciso anterior, entonces usando 1 podríamos tomar muestras de nuestros datos muestrales, como si fueran de la población, y usando el mismo tamaño de muestra. El muestreo lo hacemos con reemplazo, como la muestra original.\nA la distribución resultante le llamamos distribución bootstrap de la muestra\nUsamos la distribución bootstrap de la muestra para estimar la variabilidad en nuestra estimación con la muestra original.\n\nVeamos que sucede para un ejemplo concreto. Primero extraemos nuestra muestra:\n\nset.seed(2112)\nmuestra <- sample_n(casas_pob, 150, replace = T)\n\nEsta muestra nos da nuestro estimador de la distribución poblacional:\n\nbind_rows(muestra |> mutate(tipo = \"muestra\"),\n    casas_pob |> mutate(tipo = \"población\")) |> \nggplot(aes(sample = precio_miles, colour = tipo, group = tipo)) + \n    geom_qq(distribution = stats::qunif, alpha = 0.7, size = 2) + \n    scale_color_colorblind()\n\n\n\n\nY vemos que la aproximación es razonable, especialmente en las partes centrales de la distribución. Usamos nuestra muestra para estimar la población.\nPara evaluar ahora la variabilidad de nuestro estimador, podemos extraer un número grande de muestras con reemplazo de tamaño 150 de la muestra - estamos en el mundo Bootstrap!\n\nmediana_muestras <- map_dbl(1:5000, ~ muestra |>  \n    sample_n(150, replace = T) |>\n    summarise(mediana_precio = fun_muestra(precio_miles)) |> pull(mediana_precio)) \n\nY nuestra estimación de la distribución de muestreo es entonces:\n\nbootstrap <- tibble(mediana = mediana_muestras)\nggplot(bootstrap, aes(sample = mediana)) + geom_qq(distribution = stats::qunif)\n\n\n\n\nY podemos calcular ahora un intervalo de confianza del 90% simplemente calculando los cuantiles de esta distribución (no son los cuantiles de la muestra original!):\n\nlimites_ic <- quantile(mediana_muestras, c(0.05,  0.95)) |> round()\nlimites_ic\n\n 5% 95% \n154 173 \n\n\nPresentaríamos nuestro resultado como sigue: nuestra estimación puntual de la mediana es 165, con un intervalo de confianza del 90% de (154, 173)"
  },
  {
    "objectID": "remuestreo-bootstrap.html#experimento-de-simulación",
    "href": "remuestreo-bootstrap.html#experimento-de-simulación",
    "title": "6  Remuestreo",
    "section": "Experimento de simulación",
    "text": "Experimento de simulación\nEn nuestro ejemplo, podemos ver varias muestras (por ejemplo 20) de tamaño 100, y vemos cómo se ve la aproximación a la distribución de la población:\n\n\n\n\n\nPodemos calcular las distribuciones de remuestreo para cada muestra bootstrap, y compararlas con la distribución de muestreo real.\n\n\n\n\n\n\n\n\nCada una de estas distribuciones de remuestreo nos da un intervalo para estimar la mediana poblacional:\n\nmediana_pob <- median(casas$precio_miles)\ndist_boot |> group_by(rep) |> \n  summarise(q05 = quantile(precio_miles, 0.05), q95 = quantile(precio_miles, 0.95)) |> \n  ggplot(aes(x = rep, ymin = q05, ymax = q95)) +\n  geom_hline(yintercept = mediana_pob, colour = \"red\") +\n  geom_linerange() \n\n\n\n\nNotese que aquí estamos intentando construir intervalos de 90% de cobertura, y logramos nuestro propósito aproximadamente."
  },
  {
    "objectID": "remuestreo-bootstrap.html#cobertura-nominal-y-cobertura-real",
    "href": "remuestreo-bootstrap.html#cobertura-nominal-y-cobertura-real",
    "title": "6  Remuestreo",
    "section": "Cobertura nominal y cobertura real",
    "text": "Cobertura nominal y cobertura real\n¿Cómo sabemos que la cobertura nominal del 90% es cercana a la realidad? Sería muy malo que los intervalos fueran demasiado anchos (exageramos la variabilidad) o demasiado angostos (damos la idea de que nuestra estimación es más precisa de lo que realmente es). Que esto se cumpla depende de:\n\nCuál es la estadística de interés\nCómo es la población\nEl tamaño de muestra y otros aspectos del muestreo\n\nVarias observaciones útiles se pueden consultar en Hesterberg (2015) y en Efron y Tibshirani (1993) (por ejemplo, el bootstrap no funciona bien para estadísticas como el mínimo o el máximo). En estas referencias también pueden consultarse recomendaciones de cómo mejorar intervalos basados en boostrap - los que vimos se llaman intervalos de percentiles, pero hay más opciones simples que se desempeñan mejor en ciertos casos.\nY siempre podemos hacer ejercicios de simulación bajo ciertos supuestos acerca de la población para una estadística dada, y estimar empíricamente si la cobertura es adecuada. \n\nEjemplo\nConstruimos para nuestra población varias muestras bootstrap con sus respectivos intervalos de cuantiles. ¿Qué porcentaje de veces cubrimos al verdadero valor?\n\n\n\n\n#rep_remuestreo <- map(1:200, ~ muestras_boot(.x, B = 2000, fun_muestra = fun_muestra)) |> bind_rows\nrep_remuestreo <- read_csv(\"./datos/bootstrap_reps.csv\")\n\nCon nuestras muestras, checamos ahora nuestros intervalos y su cobertura\n\nintervalos <- rep_remuestreo |> \n    group_by(n, rep) |> \n    summarise(inf =  quantile(mediana, 0.05), sup = quantile(mediana, 0.95)) |> \n    mutate(valor_poblacional = median(casas_pob$precio_miles))\n\n`summarise()` has grouped output by 'n'. You can override using the `.groups`\nargument.\n\nggplot(intervalos, aes(x = rep, ymin = inf, ymax = sup)) + \n    geom_hline(yintercept = median(casas_pob$precio_miles), colour = \"salmon\") +\n    geom_linerange(alpha = 0.7) +\n    facet_wrap(~n) \n\n\n\n\nLa cobertura para nuestros intervalos es:\n\nintervalos |> mutate(cubre = valor_poblacional > inf & valor_poblacional < sup) |> \n     group_by(n) |> summarise(cobertura = mean(cubre), \n                               ee_cobertura = (sd(cubre) /sqrt(n())) |> round(3)) \n\n# A tibble: 2 × 3\n      n cobertura ee_cobertura\n  <dbl>     <dbl>        <dbl>\n1    50     0.895        0.022\n2   150     0.915        0.02 \n\n\nPara este número de repeticiones, estos números son consistentes con la cobertura nominal de 90%."
  },
  {
    "objectID": "remuestreo-bootstrap.html#ejemplo-estereogramas",
    "href": "remuestreo-bootstrap.html#ejemplo-estereogramas",
    "title": "6  Remuestreo",
    "section": "Ejemplo: estereogramas",
    "text": "Ejemplo: estereogramas\nEn este caso, queremos hacer inferencia sobre la diferencia de tiempo de reconocimiento de los grupos. Como discutimos antes, preferimos hacer comparaciones multiplicativas. En este caso particular, compararemos el cociente de las medias:\nPodemos adaptar el bootstrap en este caso para dos grupos: hacemos remuestreo de cada grupo, comparamos diferencias, y repetimos\n\nfusion <- read_table(\"./datos/fusion_time.txt\")\nmuestra_boot <- function(datos, grupo, medicion, fun_muestra, comparacion){\n    est_boot <- datos |> group_by({{ grupo }}) |> \n      sample_n(n(), replace = T) |> \n      summarise(est = fun_muestra( {{ medicion }})) |> \n      spread(nv.vv, est) |> \n      mutate(comp = {{ comparacion }}) |> \n      pull(comp)\n    est_boot\n}\nmuestra_boot(fusion, nv.vv, time, median, VV / NV) |> round(2)\n\n[1] 0.59\n\n\nLa distribución de remuestreo es:\n\nreps_boot <- map_dbl(1:2000, ~ muestra_boot(fusion, nv.vv, time, mean, VV / NV))\nggplot(tibble(cociente_boot = reps_boot), aes(sample = reps_boot)) +\n  geom_qq(distribution = stats::qunif) + xlab(\"f\") + ylab(\"Cociente\")\n\n\n\n\ny un intervalo de 90% sería:\n\nquantile(reps_boot, c(0.05, 0.95)) |> round(2)\n\n  5%  95% \n0.46 0.91 \n\n\nY esta sería una forma de presentar nuestros resultados: hay probabilidad considerable de que el efecto de este tratamiento sea marginal (una reducción de 10%), aunque lo más probables es que tenga un efecto consdierable (reducción alrededor de 60% del tiempo de fusión)."
  },
  {
    "objectID": "remuestreo-bootstrap.html#bootstrap-con-muestras-complejas",
    "href": "remuestreo-bootstrap.html#bootstrap-con-muestras-complejas",
    "title": "6  Remuestreo",
    "section": "6.1 Bootstrap con muestras complejas",
    "text": "6.1 Bootstrap con muestras complejas\nCuando el esquema de muestreo es estratificado o complejo (polietápico), es posible adaptar el bootstrap para producir intervalos apropiados. La idea es:\n\nEl esquema de remuestreo debe replicar el esquema con el que se seleccionó la muestra original.\nMuestreo estratificado: hacemos muestras bootstrap de cada estrato por separado.\nMuestreo complejo: dentro de estratos, seleccionamos muestras bootstrap de unidades primarias de muestreo. En este caso, a veces es necesario también reajustar los ponderadores para replicar su construcción original (ver bootstrap de Rao-Wu).\n\n\nEjemplo: conteo rápido 2018\n\nestratificacion_tbl <- read_delim(\"datos/conteo_rapido/presidencia.csv\", \n    skip = 5, delim = \"|\") |> \n  select(CLAVE_CASILLA, ID_ESTADO, ID_DISTRITO) |>\n  filter(ID_DISTRITO != 0) |> \n  mutate(estrato = interaction(ID_ESTADO, ID_DISTRITO)) |> \n  count(estrato)\n\n\nhead(estratificacion_tbl) |> kable() |> kable_paper()\n\n\n\n \n  \n    estrato \n    n \n  \n \n\n  \n    1.1 \n    488 \n  \n  \n    2.1 \n    563 \n  \n  \n    3.1 \n    551 \n  \n  \n    4.1 \n    591 \n  \n  \n    5.1 \n    550 \n  \n  \n    6.1 \n    504 \n  \n\n\n\n\n\n\nremesa_tbl <- read_delim(\"datos/conteo_rapido/REMESAS0100020000.txt\", \n  skip = 1, delim = \"|\") |> \n  mutate(estrato = interaction(iD_ESTADO, ID_DISTRITO_FEDERAL)) |> \n  select(estrato, TOTAL, JAMK, RAC, AMLO) |> \n  left_join(estratificacion_tbl)\n\nIgnoraremos casillas faltantes: lo cual sabemos que en general no es una buena idea. Primero usamos una aproximación de Taylor para estimar error estándar y los intervalos:\n\nlibrary(survey)\ndiseño_cr <- svydesign(id = ~1, strata = ~estrato, fpc = ~ n, data = remesa_tbl)\nsvyratio(~ RAC + JAMK + AMLO, ~ TOTAL, diseño_cr) |> \n  confint() |>\n  round(4) |> \n  as_tibble(rownames = \"candidato\")\n\n# A tibble: 3 × 3\n  candidato  `2.5 %` `97.5 %`\n  <chr>        <dbl>    <dbl>\n1 RAC/TOTAL    0.220    0.225\n2 JAMK/TOTAL   0.158    0.162\n3 AMLO/TOTAL   0.532    0.538\n\n\nAhora utilizaremos bootstrap. Se puede hacer directamente con el paquete survey, por ejemplo:\n\ndiseño_cr_boot <- as.svrepdesign(diseño_cr, type = \"subbootstrap\", replicates = 1000)\nsvyratio(~ RAC + JAMK + AMLO, ~ TOTAL, diseño_cr_boot) |> \n  confint() |>\n  round(4) |> \n  as_tibble(rownames = \"candidato\")\n\n# A tibble: 3 × 3\n  candidato  `2.5 %` `97.5 %`\n  <chr>        <dbl>    <dbl>\n1 RAC/TOTAL    0.220    0.226\n2 JAMK/TOTAL   0.158    0.162\n3 AMLO/TOTAL   0.532    0.538\n\n\nEn este caso simple de muestreo estratificado, también podemos escribir nuestro propio código. Tenemos dos funciones:\n\n# extrae muestra bootstrap estratificada\nextraer_bootstrap <- function(muestra, var_estrato){\n  remuestra <- muestra |> \n    group_by({{ var_estrato }}) |> \n    slice_sample( prop = 1, replace = TRUE) |> \n    ungroup()\n  remuestra\n}\n# cálculo del estimador de razón para una muestra\nestimar_razon <- function(muestra, num, denom, var_estrato){\n  estimador <- muestra |> \n    group_by({{ var_estrato }}) |> \n    summarise(candidato = mean({{ num }}), total = mean({{ denom }}), n = first(n)) |> \n    mutate(candidato = n * candidato, total = n * total) |> \n    ungroup() |> \n    summarise(prop = sum(candidato) / sum(total))\n  estimador\n}\n\nY ahora aplicamos repetidamente el estimador a las muestras bootstrap estratificadas:\n\nremuestreo_tbl <- map_df(1:500, function(rep){\n  remuestra <- extraer_bootstrap(remesa_tbl, estrato)\n  estimador <- estimar_razon(remuestra, RAC, TOTAL, estrato)\n  tibble(rep = rep, est_rep = estimador$prop)\n})\nggplot(remuestreo_tbl, aes(sample = est_rep)) + geom_qq()\n\n\n\nquantile(remuestreo_tbl |> pull(est_rep), c(0.025, 0.975)) |> \n  round(4)\n\n  2.5%  97.5% \n0.2204 0.2254"
  },
  {
    "objectID": "remuestreo-bootstrap.html#ventajas-y-desventajas-del-bootstrap",
    "href": "remuestreo-bootstrap.html#ventajas-y-desventajas-del-bootstrap",
    "title": "6  Remuestreo",
    "section": "Ventajas y desventajas del bootstrap",
    "text": "Ventajas y desventajas del bootstrap\n\nEl bootstrap es una técnica versátil generalmente fácil de implementar (ventaja) - especialmente cuando a algún nivel podemos suponer que las muestras son idependientes e idénticamente distribuidas (desventaja).\n\nPor ejemplo: en muestreo estratificado, podemos hacer bootstrap sobre cada estrato por separado. En muestreo complejo, podemos hacer bootstrap de unidades primarias de muestreo, etc.\n\nRequiere más cómputo que fórmulas estándar (desventaja), pero tenemos flexibilidad (ventaja) para aplicar en estadísticas diferentes de manera muchas veces trivial (ventaja).\nEs una técnica estándar en el análisis de datos que se usa en un rango grande de aplicaciones (ventaja).\nEn el caso de muestras chicas y ciertas distribuciones poblacionales, los intervalos bootstrap de percentiles que vimos aquí pueden ser un poco angostos y no cumplir la cobertura nominal por ejemplo, si la muestra es de tamaño < 40. la cobertura puede ser de 90% en lugar de 95% en algunos casos (población normal, o de 80% en lugar de 95% en una poblacion exponencial), ver Hesterberg (2015)). Hay mejores opciones en estos casos (por ejempo, intervalos bootstrap-t, que se calculan fácilmente también).\nFinalmente, en casos donde tenemos la población total, o el supuesto de muestras aleatorias es dudoso, lo podemos utilizar más informalmente como un análisis de sensibilidad de nuestros resultados. Es una perturbación a los datos (que podemos combinar con otros tipos de perturbaciones) para juzgar qué tan fuertemente depende nuestro análisis de los datos que tenemos a mano."
  },
  {
    "objectID": "remuestreo-bootstrap.html#sesgo",
    "href": "remuestreo-bootstrap.html#sesgo",
    "title": "6  Remuestreo",
    "section": "Sesgo",
    "text": "Sesgo\nAlgunos estimadores comunes (por ejemplo, cociente de dos cantidades aleatorias) pueden sufrir de **sesgo* grande, especialmente en el caso de muestras chicas. Esto afecta la cobertura, pues es posible que nuestros intervalos no tengan “cobertura simétrica”, por ejemplo. Para muchos estimadores, y muestras no muy chicas, esté sesgo tiende a ser poco importante y no es necesario hacer correcciones.\nPodemos evaluar el sesgo comparando la media de nuestras replicaciones bootstrap con el valor muestral que obtuvimos (para estadísticas funcionales, ver Hesterberg (2015)). Si el tamaño del sesgo es chico comparado con la dispersión de la distribución bootstrap (por ejemplo, menos de 20% de la desviación estándar, Efron y Tibshirani (1993)), no es muy importante hacer correcciones.\nEn caso de que esta cantidad sea relativamente grande en relación a la dispersión de la distribución bootstrap, hay variantes los intervalos bootstrap de percentiles que mejoran esta situación (Efron y Tibshirani (1993))."
  },
  {
    "objectID": "remuestreo-bootstrap.html#bootstrap-y-estimadores-complejos-suavizadores",
    "href": "remuestreo-bootstrap.html#bootstrap-y-estimadores-complejos-suavizadores",
    "title": "6  Remuestreo",
    "section": "Bootstrap y estimadores complejos: suavizadores",
    "text": "Bootstrap y estimadores complejos: suavizadores\nEl bootstrap es una técnica versátil. Por ejemplo, podemos usarlo para juzgar la variabilidad de un suavizador:\n\ngraf_casas <- function(data){\n    ggplot(data |> filter(calidad_gral < 7), \n        aes(x = area_habitable_sup_m2)) + \n        geom_point(aes(y = precio_m2_miles), alpha = 0.75) +\n        geom_smooth(aes(y = precio_m2_miles), method = \"loess\", span = 0.7, \n                se = FALSE, method.args = list(degree = 1, family = \"symmetric\"))     \n}\nset.seed(250)\ncasas_muestra <- sample_frac(casas, 0.2)\ngraf_casas(casas_muestra)\n\n\n\n\nPodemos hacer bootstrap para juzgar la estabilidad del suavizador:\n\nsuaviza_boot <- function(x, data){\n    # remuestreo\n    muestra_boot <- sample_n(data, nrow(data), replace = T)\n    ajuste <- loess(precio_m2_miles ~ area_habitable_sup_m2, data = muestra_boot, \n                    degree = 1, span = 0.7, family = \"symmetric\")\n    datos_grafica <- tibble(area_habitable_sup_m2 = seq(25, 250, 5))\n    ajustados <- predict(ajuste, newdata = datos_grafica)\n    datos_grafica |> mutate(ajustados = ajustados) |> \n        mutate(rep = x)\n}\nreps <- map(1:10, ~ suaviza_boot(.x, casas_muestra |> filter(calidad_gral < 7))) |> \n    bind_rows()\n\n\n# ojo: la rutina loess no tienen soporte para extrapolación\ngraf_casas(casas_muestra) + \n    geom_line(data = reps, aes(y = ajustados, group = rep), alpha = 1, colour = \"red\") \n\n\n\n\nDonde vemos que algunas cambios de pendiente del suavizador original no son muy interpretables (por ejemplo, para áreas chicas) y alta variabilidad en general en los extremos. Podemos hacer más iteraciones para calcular bandas de confianza:\n\nreps <- map(1:200, ~ suaviza_boot(.x, casas_muestra |> filter(calidad_gral < 7))) |> \n    bind_rows()\n# ojo: la rutina loess no tienen soporte para extrapolación\ngraf_casas(casas_muestra) + \n    geom_line(data = reps, aes(y = ajustados, group = rep), alpha = 0.2, colour = \"red\")"
  },
  {
    "objectID": "remuestreo-bootstrap.html#bootstrap-y-estimadores-complejos-tablas-de-perfiles",
    "href": "remuestreo-bootstrap.html#bootstrap-y-estimadores-complejos-tablas-de-perfiles",
    "title": "6  Remuestreo",
    "section": "Bootstrap y estimadores complejos: tablas de perfiles",
    "text": "Bootstrap y estimadores complejos: tablas de perfiles\nPodemos regresar al ejemplo de la primera sesión donde calculamos perfiles de los tomadores de distintos tés: en bolsa, suelto, o combinados:\n\n\n\n\n\n\n\n\n\n\n \n  \n    price \n    tea bag \n    tea bag+unpackaged \n    unpackaged \n    promedio \n  \n \n\n  \n    p_upscale \n    -0.71 \n    -0.28 \n    0.98 \n    28 \n  \n  \n    p_variable \n    -0.12 \n    0.44 \n    -0.31 \n    36 \n  \n  \n    p_cheap \n    0.3 \n    -0.53 \n    0.23 \n    2 \n  \n  \n    p_branded \n    0.62 \n    -0.16 \n    -0.45 \n    25 \n  \n  \n    p_private label \n    0.72 \n    -0.22 \n    -0.49 \n    5 \n  \n  \n    p_unknown \n    1.58 \n    -0.58 \n    -1 \n    3 \n  \n\n\n\n\n\n\n\n\n\n\nHacemos bootstrap sobre toda la muestra, y repetimos exactamente el mismo proceso de construción de perfiles:\n\nboot_perfiles <- map(1:1000, function(x){\n    te_boot <- te |> sample_n(nrow(te), replace = TRUE)\n    calcular_perfiles(te_boot) |> mutate(rep = x)\n}) |> bind_rows()\n\nAhora resumimos y graficamos, esta vez de manera distinta:\n\nresumen_perfiles <- boot_perfiles |> group_by(how, price) |> \n    summarise(perfil_media = mean(perfil), ymax = quantile(perfil, 0.9), ymin = quantile(perfil, 0.10)) \n\n`summarise()` has grouped output by 'how'. You can override using the `.groups`\nargument.\n\nresumen_bolsa <- resumen_perfiles |> ungroup() |> \n    filter(how == \"tea bag\") |> select(price, perfil_bolsa = perfil_media)\nresumen_perfiles <- resumen_perfiles |> left_join(resumen_bolsa) |> \n    ungroup() |> \n    mutate(price = fct_reorder(price, perfil_bolsa))\n\nJoining, by = \"price\"\n\nggplot(resumen_perfiles, aes(x = price, y = perfil_media, ymax = ymax, ymin = ymin)) + \n    geom_point(colour = \"red\") + geom_linerange() +\n    facet_wrap(~how) + coord_flip() +\n    geom_hline(yintercept = 0, colour = \"gray\") + ylab(\"Perfil\") + xlab(\"Precio\")\n\n\n\n\nNótese una deficiencia clara del bootstrap: para los que compran té suelto, en la muestra no existen personas que desconocen de dónde provienen su té (No sabe/No contestó). Esto produce un intervalo colapsado en 0 que no es razonable.\nPodemos remediar esto de varias maneras: quitando del análisis los que no sabe o no contestaron, agrupando en otra categoría, usando un modelo, o regularizar usando proporciones calculadas con conteos modificados: por ejemplo, agregando un caso de cada combinación (agregaría 18 personas “falsas” a una muestra de 290 personas).\n\n\n\n\nEfron, B., y R. Tibshirani. 1993. «An Introduction to the Bootstrap». Miscellaneous. Macmillan Publishers Limited. All rights reserved.\n\n\nHesterberg, Tim C. 2015. «What teachers should know about the bootstrap: Resampling in the undergraduate statistics curriculum». The American Statistician 69 (4): 371-86."
  },
  {
    "objectID": "intro-bayesiana-1.html",
    "href": "intro-bayesiana-1.html",
    "title": "7  Introducción a inferencia bayesiana",
    "section": "",
    "text": "En las secciones anteriores estudiamos métodos de remuestreo para estimar parámetros, y cuantificar la incertidumbre qué tenemos acerca de valores poblacionales.\nOtra manera de hacer inferencia es usando modelos probabilísticos para los datos: asumir que los datos tienen cierto comportamiento (por ejemplo, provienen de distribuciones normales), y usar métodos como máxima verosimilitud para hacer estimaciones junto con su incertidumbre.\nUna tercera opción es utilizar inferencia bayesiana, que tiene objetivos similares.\nEl concepto probabilístico básico que utilizamos para construir estos modelos y la inferencia es el de probabilidad condicional: la probabilidad de que ocurran ciertos eventos dada la información disponible del fenómeno que nos interesa."
  },
  {
    "objectID": "intro-bayesiana-1.html#un-primer-ejemplo-completo-de-inferencia-bayesiana",
    "href": "intro-bayesiana-1.html#un-primer-ejemplo-completo-de-inferencia-bayesiana",
    "title": "7  Introducción a inferencia bayesiana",
    "section": "Un primer ejemplo completo de inferencia bayesiana",
    "text": "Un primer ejemplo completo de inferencia bayesiana\nConsideremos el siguiente problema simple: Nos dan una moneda, y solo sabemos que la moneda puede tener probabilidad \\(3/5\\) de tirar sol (está cargada a sol) o puede ser una moneda cargada a águila, con probabilidad \\(2/5\\) de tirar sol.\nVamos a lanzar la moneda dos veces y observamos su resultado (águila o sol). Queremos decir algo acerca de qué tan probable es que hayamos tirado la moneda cargada a sol o la moneda cargada a águila.\nEn este caso, tenemos dos variables: \\(X\\), que cuenta el número de soles obtenidos en el experimento aleatorio, y \\(\\theta\\), que da la probabilidad de que un volado resulte en sol (por ejemplo, si la moneda es justa entonces \\(\\theta = 0.5\\)).\n¿Qué cantidades podríamos usar para evaluar qué moneda es la que estamos usando? Si hacemos el experimento, y tiramos la moneda 2 veces, podríamos considerar la probabilidad\n\\[P(\\theta = 0.4 | X = x)\\]\ndonde \\(x\\) es el número de soles que obtuvimos en el experimento. Esta es la probabilidad condicional de que estemos tirando la moneda con probabilidad de sol 2/5 dado que observamos \\(x\\) soles. Por ejemplo, si tiramos 2 soles, deberíamos calcular\n\\[P(\\theta=0.4|X=2).\\]\n¿Cómo calculamos esta probabilidad? ¿Qué sentido tiene?\nUsando reglas de probabildad (regla de Bayes en particular), podríamos calcular\n\\[P(\\theta=0.4|X=2) = \\frac{P(X=2 | \\theta = 0.4) P(\\theta =0.4)}{P(X=2)}\\]\nNota que en el numerador uno de los factores, \\(P(X=2 | \\theta = 0.4),\\) es la verosimilitud. Así que primero necesitamos la verosimilitud:\n\\[P(X=2|\\theta = 0.4) = (0.4)^2 = 0.16.\\]\nLa novedad es que ahora tenemos que considerar la probabilidad \\(P(\\theta = 0.4)\\). Esta cantidad no la habíamos encontrado antes. Tenemos que pensar entonces que este parámetro es una cantidad aleatoria, y puede tomar dos valores \\(\\theta=0.4\\) ó \\(\\theta = 0.6\\).\nConsiderar esta cantidad como aleatoria requiere pensar, en este caso, en cómo se escogió la moneda, o qué sabemos acerca de las monedas que se usan para este experimento (conocimiento de dominio). Supongamos que en este caso, nos dicen que la moneda se escoge al azar de una bolsa donde hay una proporción similar de los dos tipos de moneda (0.4 ó 0.6). Es decir el espacio parametral es \\(\\Theta = \\{0.4, 0.6\\},\\) y las probabilidades asociadas a cada posibilidad son las mismas. Tenemos\n\\[P(\\theta = 0.4) = P(\\theta = 0.6) =0.5,\\]\nque representa la probabilidad de escoger de manera aleatoria la moneda con una carga en particular.\nAhora queremos calcular \\(P(X=2)\\), pero con el trabajo que hicimos esto es fácil. Pues requiere usar reglas de probabilidad usuales para hacerlo. Podemos utilizar probabilidad total \\[\\begin{align}\nP(X) &= \\sum_{\\theta \\in \\Theta} P(X, \\theta)\\\\\n&= \\sum_{\\theta \\in \\Theta} P(X\\, |\\, \\theta) P(\\theta),\n\\end{align}\\] lo cual en nuestro ejemplo se traduce en escribir\n\\[ P(X=2) = P(X=2|\\theta = 0.4)P(\\theta = 0.4) + P(X=2|\\theta=0.6)P(\\theta =0.6),\\]\npor lo que obtenemos\n\\[P(X=2) = 0.16(0.5) + 0.36(0.5) = 0.26.\\]\nFinalmente la probabilidad de haber escogido la moneda con carga \\(2/5\\) dado que observamos dos soles en el lanzamiento es\n\\[P(\\theta=0.4|X=2) = \\frac{0.16(0.5)}{0.26} \\approx  0.31.\\]\nEs decir, la probabilidad posterior de que estemos tirando la moneda \\(2/5\\) baja de 0.5 (nuestra información inicial) a 0.31.\nEste es un ejemplo completo, aunque muy simple, de inferencia bayesiana. La estrategia de inferencia bayesiana implica tomar decisiones basadas en las probabilidades posteriores.\n\n\n\n¿Cuál sería la estimación de máxima verosimilitud para este problema? ¿Cómo cuantificaríamos la incertidumbre en la estimación de máxima verosimilitud?\n\n\n\nFinalmente, podríamos hacer predicciones usando la posterior predictiva. Si \\({X}_{nv}\\) es una nueva tirada adicional de la moneda que estamos usando, nos interesaría saber:\n\\[P({X}_{nv}=\\mathsf{sol}\\, | \\, X=2)\\]\nNotemos que un volado adicional es un resultado binario. Por lo que podemos calcular observando que \\(P({X}_{nv}|X=2, \\theta)\\) es una variable Bernoulli con probabilidad \\(\\theta\\), que puede valer 0.4 ó 0.6. Como tenemos las probabilidades posteriores \\(P(\\theta|X=2)\\) podemos usar probabilidad total, condicionado en \\(X=2\\): \\[\\begin{align*}\nP({X}_{nv}=\\mathsf{sol}\\, | \\, X=2) & = \\sum_{\\theta \\in \\Theta} P({X}_{nv}=\\mathsf{sol}, \\theta \\, | \\, X=2) & \\text{(probabilidad total)}\\\\\n&= \\sum_{\\theta \\in \\Theta} P({X}_{nv}=\\mathsf{sol}\\, | \\theta , X=2) P(\\theta \\, | \\, X=2) & \\text{(probabilidad condicional)}\\\\\n&= \\sum_{\\theta \\in \\Theta} P({X}_{nv}=\\mathsf{sol}\\, | \\theta ) P(\\theta \\, | \\, X=2), & \\text{(independencia condicional)}\n\\end{align*}\\]\nlo que nos da el siguiente cálculo\n\\[P(X_{nv}=\\mathsf{sol}\\, |\\, \\theta=0.4) \\,  P(\\theta=0.4|X=2) \\,  +\\, P(X_{nv}=\\mathsf{sol}|\\theta = 0.6) \\, P(\\theta =0.6|X=2)\\]\nEs decir, promediamos ponderando con las probabilidades posteriores. Por lo tanto obtenemos\n\\[P(X_{nv} = \\mathsf{sol}|X=2) =  0.4 ( 0.31) + 0.6 (0.69) = 0.538.\\]\n\nObservación 0\nNótese que en contraste con máxima verosimilitud, en este ejemplo cuantificamos con probabilidad condicional la incertidumbre de los parámetros que no conocemos. En máxima verosimilitud esta probabilidad no tiene mucho sentido, pues nunca consideramos el parámetro desconocido como una cantidad aleatoria.\n\n\nObservación 1\nNótese el factor \\(P(X=2)\\) en la probabilidad posterior puede entenderse como un factor de normalización. Notemos que los denominadores en la distribución posterior son\n\\[P(X=2 | \\theta = 0.4) P(\\theta =0.4) = 0.16(0.5) = 0.08,\\]\ny\n\\[P(X=2 | \\theta = 0.6) P(\\theta =0.6) = 0.36(0.5) = 0.18.\\]\nLas probabilidades posteriores son proporcionales a estas dos cantidades, y como deben sumar uno, entonces normalizando estos dos números (dividiendo entre su suma) obtenemos las probabilidades.\n\n\nObservación 2\nLa nomenclatura que usamos es la siguiente:\n\nComo \\(X\\) son los datos observados, llamamos a \\(P(X|\\theta)\\) la verosimilitud, o modelo de los datos.\nA \\(P(\\theta)\\) le llamamos la distribución inicial o previa.\nLa distribución que usamos para hacer inferencia \\(P(\\theta|X)\\) es la distribución final o posterior.\n\nPara utilizar inferencia bayesiana, hay que hacer supuestos para definir las primeras dos partes del modelo. La parte de iniciales o previas está ausente de enfoques como máxima verosimlitud usual.\n\n\nObservación 3\n¿Cómo decidimos las probabilidades iniciales, por ejemplo \\(P(\\theta=0.4)\\) ?\nQuizá es un supuesto y no tenemos razón para pensar que se hace de otra manera. O quizá conocemos el mecanismo concreto con el que se selecciona la moneda. Discutiremos esto más adelante.\n\n\nObservación 4\n¿Cómo decidimos el modelo de los datos? Aquí típicamente también tenemos que hacer algunos supuestos, aunque algunos de estos pueden estar basados en el diseño del estudio, por ejemplo. Igual que cuando usamos máxima verosimilitud, es necesario checar que nuestro modelo ajusta razonablemente a los datos.\n\n\nEjercicio\nCambia distintos parámetros del número de soles observados, las probabilidades de sol de las monedas, y las probabilidades iniciales de selección de las monedas.\n\nn_volados <- 2\n# posible valores del parámetro desconocido\ntheta = c(0.4, 0.6)\n# probabilidades iniciales\nprobs_inicial <- tibble(moneda = c(1, 2),\n                        theta = theta,\n                        prob_inicial = c(0.5, 0.5))\nprobs_inicial\n\n# A tibble: 2 × 3\n  moneda theta prob_inicial\n   <dbl> <dbl>        <dbl>\n1      1   0.4          0.5\n2      2   0.6          0.5\n\n# verosimilitud\ncrear_verosim <- function(no_soles){\n    verosim <- function(theta){\n      # prob de observar no_soles en 2 volados con probabilidad de sol theta\n      dbinom(no_soles, 2, theta)\n    }\n    verosim\n}\n# evaluar verosimilitud\nverosim <- crear_verosim(2)\n# ahora usamos regla de bayes para hacer tabla de probabilidades\ntabla_inferencia <- probs_inicial |>\n  mutate(verosimilitud = map_dbl(theta, verosim)) |>\n  mutate(inicial_x_verosim = prob_inicial * verosimilitud) |>\n  # normalizar\n  mutate(prob_posterior = inicial_x_verosim / sum(inicial_x_verosim))\ntabla_inferencia |>\n  mutate(moneda_obs = moneda) |>\n  select(moneda_obs, theta, prob_inicial, verosimilitud, prob_posterior)\n\n# A tibble: 2 × 5\n  moneda_obs theta prob_inicial verosimilitud prob_posterior\n       <dbl> <dbl>        <dbl>         <dbl>          <dbl>\n1          1   0.4          0.5          0.16          0.308\n2          2   0.6          0.5          0.36          0.692\n\n\n\n\n\n\n¿Qué pasa cuando el número de soles es 0? ¿Cómo cambian las probabilidades posteriores de cada moneda?\n\n\nIncrementa el número de volados, por ejemplo a 10. ¿Qué pasa si observaste 8 soles, por ejemplo? ¿Y si observaste 0?\n\n\n¿Qué pasa si cambias las probabilidades iniciales (por ejemplo incrementas la probabilidad inicial de la moneda 1 a 0.9)?\n\n\n\n\nJustifica las siguientes aseveraciones (para este ejemplo):\n\n\n\n\nLas probabilidades posteriores o finales son una especie de punto intermedio entre verosimilitud y probablidades iniciales.\n\n\nSi tenemos pocas observaciones, las probabilidades posteriores son similares a las iniciales.\n\n\nCuando tenemos muchos datos, las probabilidades posteriores están más concentradas, y no es tan importante la inicial.\n\n\nSi la inicial está muy concentrada en algún valor, la posterior requiere de muchas observaciones para que se pueda concentrar en otros valores diferentes a los de la inicial.\n\n\n\n\nAhora resumimos los elementos básicos de la inferencia bayesiana, que son relativamente simples:\nInferencia bayesiana. Con la notación de arriba: - Como \\(X\\) son los datos observados, llamamos a \\(P(X|\\theta)\\) la verosimilitud, proceso generador de datos o modelo de los datos. - El factor \\(P(\\theta)\\) le llamamos la distribución inicial o previa. - La distribución que usamos para hacer inferencia \\(P(\\theta|X)\\) es la distribución final o posterior Hacemos inferencia usando la ecuación\n\\[P(\\theta | X) = \\frac{P(X | \\theta) P(\\theta)}{P(X)}\\]\nque también escribimos:\n\\[P(\\theta | X) \\propto P(X | \\theta) P(\\theta)\\]\n\n\n\ndonde () significa “proporcional a”. No ponemos (P(X)) pues como vimos arriba, es una constante de normalización.\n\n\n\nEn estadística Bayesiana, las probablidades posteriores \\(P(\\theta|X)\\) dan toda la información que necesitamos para hacer inferencia. ¿Cuándo damos probablidad alta a un parámetro particular \\(\\theta\\)? Cuando su verosimilitud es alta y/o cuando su probabilidad inicial es alta. De este modo, la posterior combina la información inicial que tenemos acerca de los parámetros con la información en la muestra acerca de los parámetros (verosimilitud). Podemos ilustrar como sigue:"
  },
  {
    "objectID": "intro-bayesiana-1.html#ejemplo-estimando-una-proporción",
    "href": "intro-bayesiana-1.html#ejemplo-estimando-una-proporción",
    "title": "7  Introducción a inferencia bayesiana",
    "section": "Ejemplo: estimando una proporción",
    "text": "Ejemplo: estimando una proporción\nRegresamos ahora a nuestro problema de estimar una proporción \\(\\theta\\) de una población dada usando una muestra \\(X_1,X_2,\\ldots, X_n\\) de variables Bernoulli () extraídas de modo independiente. Ya sabemos calcular la verosimilitud (el modelo de los datos):\n\\[P(X_1=x_1,X_2 =x_2,\\ldots, X_n=x_n|\\theta) = \\theta^k(1-\\theta)^{n-k},\\]\ndonde \\(k = x_1 + x_2 +\\cdots + x_k\\) es el número de éxitos que observamos.\nAhora necesitamos una distribución inicial o previa \\(P(\\theta)\\). Aunque esta distribución puede tener cualquier forma, supongamos que nuestro conocimiento actual podemos resumirlo con una distribución \\(\\mathsf{Beta}(3, 3)\\):\n\\[P(\\theta) \\propto \\theta^2(1-\\theta)^2.\\]\nLa constante de normalización es 1/30, pero no la requerimos. Podemos simular para examinar su forma:\n\nsim_inicial <- tibble(theta = rbeta(10000, 3, 3))\nggplot(sim_inicial) + geom_histogram(aes(x = theta, y = ..density..), bins = 15)\n\n\n\n\n\n\n\n\nDe modo que nuestra información inicial es que la proporción puede tomar cualquier valor entre 0 y 1, pero es probable que tome un valor no tan lejano de 0.5. Por ejemplo, con probabilidad 0.95 creemos que \\(\\theta\\) está en el intervalo\n\nquantile(sim_inicial$theta, c(0.025, 0.975)) |> round(2)\n\n 2.5% 97.5% \n 0.15  0.86 \n\n\nEs difícil justificar en abstracto por qué escogeriamos una inicial con esta forma. Aunque esto los detallaremos más adelante, puedes pensar, por el momento, que alguien observó algunos casos de esta población, y quizá vio tres éxitos y tres fracasos. Esto sugeriría que es poco probable que la probablidad \\(\\theta\\) sea muy cercana a 0 o muy cercana a 1.\nAhora podemos construir nuestra posterior. Tenemos que\n\\[P(\\theta| X_1=x_1, \\ldots, X_n=x_n) \\propto P(X_1 = x_1,\\ldots X_n=x_n | \\theta)P(\\theta) = \\theta^{k+2}(1-\\theta)^{n-k + 2}\\]\ndonde la constante de normalización no depende de \\(\\theta\\). Como \\(\\theta\\) es un parámetro continuo, la expresión de la derecha nos debe dar una densidad posterior.\nSupongamos entonces que hicimos la prueba con \\(n = 30\\) (número de prueba) y observamos 19 éxitos. Tendríamos entonces\n\\[P(\\theta | S_n = 19) \\propto \\theta^{19 + 2} (1-\\theta)^{30 - 19 +2} = \\theta^{21}(1-\\theta)^{13}\\]\nLa cantidad de la derecha, una vez que normalizemos por el número \\(P(X=19)\\), nos dará una densidad posterior (tal cual, esta expresion no integra a 1). Podemos obtenerla usando cálculo, pero recordamos que una distribución \\(\\mathsf{\\mathsf{Beta}}(a,b)\\) tiene como fórmula\n\\[\\frac{1}{B(a,b)} \\theta^{a-1}(1-\\theta)^{b-1}\\]\nConcluimos entonces que la posterior tiene una distribución \\(\\mathsf{Beta}(22, 14)\\). Podemos simular de la posterior usando código estándar para ver cómo luce:\n\nsim_inicial <- sim_inicial |> mutate(dist = \"inicial\")\nsim_posterior <- tibble(theta = rbeta(10000, 22, 14)) |> mutate(dist = \"posterior\")\nsims <- bind_rows(sim_inicial, sim_posterior)\nggplot(sims, aes(x = theta, fill = dist)) +\n  geom_histogram(aes(x = theta), bins = 30, alpha = 0.5, position = \"identity\")\n\n\n\n\n\n\n\n\nLa posterior nos dice cuáles son las posibilidades de dónde puede estar el parámetro \\(\\theta\\). Nótese que ahora excluye prácticamente valores más chicos que 0.25 o mayores que 0.9. Esta distribución posterior es el objeto con el que hacemos inferencia: nos dice dónde es creíble que esté el parámetro \\(\\theta\\).\nPodemos resumir de varias maneras. Por ejemplo, si queremos un estimador puntual usamos la media posterior:\n\nsims |> group_by(dist) |>\n  summarise(theta_hat = mean(theta) |> round(3))\n\n# A tibble: 2 × 2\n  dist      theta_hat\n  <chr>         <dbl>\n1 inicial       0.504\n2 posterior     0.611\n\n\nNota que el estimador de máxima verosimilitud es \\(\\hat{p} = 19/30 = 0.63\\), que es ligeramente diferente de la media posterior. ¿Por qué?\nY podemos construir intervalos de percentiles, que en esta situación suelen llamarse intervalos de credibilidad, por ejemplo:\n\nf <- c(0.025, 0.975)\nsims |> group_by(dist) |>\n  summarise(cuantiles = quantile(theta, f) |> round(2), f = f) |>\n  pivot_wider(names_from = f, values_from = cuantiles)\n\n# A tibble: 2 × 3\n# Groups:   dist [2]\n  dist      `0.025` `0.975`\n  <chr>       <dbl>   <dbl>\n1 inicial      0.15    0.86\n2 posterior    0.44    0.76\n\n\nEl segundo renglón nos da un intervalo posterior para \\(\\theta\\) de credibilidad 95%. En inferencia bayesiana esto sustituye a los intervalos de confianza.\n\nEl intervalo de la inicial expresa nuestras creencias a priori acerca de \\(\\theta\\). Este intervalo es muy amplio (va de 0.15 a 0.85)\nEl intervalo de la posterior actualiza nuestras creencias acerca de \\(\\theta\\) una vez que observamos los datos, y es considerablemente más angosto y por lo tanto informativo.\n\nObservaciones:\n\nNótese que escogimos una forma analítica fácil para la inicial, pues resultó así que la posterior es una distribución beta. No siempre es así, y veremos qué hacer cuando nuestra inicial no es de un tipo “conveniente”.\nComo tenemos la forma analítica de la posterior, es posible hacer los cálculos de la media posterior, por ejemplo, integrando la densidad posterior a mano. Esto generalmente no es factible, y en este ejemplo preferimos hacer una aproximación numérica. En este caso particular es posible usando cálculo, y sabemos que la media de una \\(\\mathsf{\\mathsf{Beta}}(a,b)\\) es \\(a/(a+b)\\), de modo que nuestra media posterior es\n\n\\[\\hat{\\mu} = (19 + 2)/(30 + 4) = 21/34 = 0.617 \\]\nque podemos interpretar como sigue: para calcular la media posterior, a nuestras \\(n\\) pruebas iniciales agregamos 4 pruebas adicionales fijas, con 2 éxitos y 2 fracasos, y calculamos la proporción usual de éxitos.\n\n\n\nRepite el análisis considerando en general (n) pruebas, con (k) éxitos. Utiliza la misma distribución inicial.\n\n\n\n\nLo mismo aplica para el intervalo de 95% (¿cómo se calcularía integrando?). También puedes usar la aproximación de R, por ejemplo:\n\n\nqbeta(0.025, shape1 = 22, shape2 = 14) |> round(2)\n\n[1] 0.45\n\nqbeta(0.975, shape1 = 22, shape2 = 14) |> round(2)\n\n[1] 0.76"
  },
  {
    "objectID": "intro-bayesiana-1.html#ejemplo-observaciones-uniformes",
    "href": "intro-bayesiana-1.html#ejemplo-observaciones-uniformes",
    "title": "7  Introducción a inferencia bayesiana",
    "section": "Ejemplo: observaciones uniformes",
    "text": "Ejemplo: observaciones uniformes\nAhora regresamos al problema de estimación del máximo de una distribución uniforme. En este caso, consideraremos un problema más concreto. Supongamos que hay una lotería (tipo tradicional) en México y no sabemos cuántos números hay. Obtendremos una muestra iid de \\(n\\) números, ya haremos una aproximación continua, suponiendo que\n\\[X_i \\sim U[0,\\theta]\\]\nLa verosimilitud es entonces\n\\[P(X_1,\\ldots, X_n|\\theta) = \\theta^{-n},\\]\ncuando \\(\\theta\\) es mayor que todas las \\(X_i\\), y cero en otro caso. Necesitaremos una inicial \\(P(\\theta)\\).\nPor la forma que tiene la verosimilitud, podemos intentar una distribución Pareto, que tiene la forma\n\\[P(\\theta) = \\frac{\\alpha \\theta_0^\\alpha}{\\theta^{\\alpha + 1}}\\]\ncon soporte en \\([\\theta_0,\\infty]\\). Tenemos que escoger entonces el mínimo \\(\\theta_0\\) y el parámetro \\(\\alpha\\). En primer lugar, como sabemos que es una lotería nacional, creemos que no puede haber menos de unos 300 mil números, así que \\(\\theta_0 = 300\\). La función acumulada de la pareto es \\(1- (300/\\theta)^\\alpha\\), así que el cuantil 99% es\n\nalpha <- 1.1\n(300/(0.01)^(1/alpha))\n\n[1] 19738\n\n\nes decir, alrededor de 20 millones de números. Creemos que es un poco probable que el número de boletos sea mayor a esta cota. Nótese ahora que la posterior cumple (multiplicando verosimilitud por inicial):\n\\[P(\\theta|X_1,\\ldots, X_n |\\theta) \\propto \\theta^{-(n + 2.1)}\\]\npara \\(\\theta\\) mayor que el máximo de las \\(X_n\\)’s y 300, y cero en otro caso. Esta distribución es pareto con \\(\\theta_0' = \\max\\{300, X_1,\\ldots, X_n\\}\\) y \\(\\alpha = n + 1.1\\)\nUna vez planteado nuestro modelo, veamos los datos. Obtuvimos la siguiente muestra de números:\n\nloteria_tbl <- read_csv(\"datos/nums_loteria_avion.csv\", col_names = c(\"id\", \"numero\")) |>\n  mutate(numero = as.integer(numero))\nset.seed(334)\nmuestra_loteria <- sample_n(loteria_tbl, 25) |>\n  mutate(numero = numero/1000)\nmuestra_loteria |> as.data.frame() |> head()\n\n  id   numero\n1 87  348.341\n2  5 5851.982\n3 40 1891.786\n4 51 1815.455\n5 14 5732.907\n6 48 3158.414\n\n\nPodemos simular de una Pareto como sigue:\n\nrpareto <- function(n, theta_0, alpha){\n  # usar el método de inverso de distribución acumulada\n  u <- runif(n, 0, 1)\n  theta_0 / (1 - u)^(1/alpha)\n}\n\nSimulamos de la inicial:\n\nsims_pareto_inicial <- tibble(\n  theta = rpareto(20000, 300, 1.1 ),\n  dist = \"inicial\")\n\nY con los datos de la muestra, simulamos de la posterior:\n\nsims_pareto_posterior <- tibble(\n  theta = rpareto(20000,\n                  max(c(300, muestra_loteria$numero)),\n                  nrow(muestra_loteria) + 1.1),\n  dist = \"posterior\")\nsims_theta <- bind_rows(sims_pareto_inicial, sims_pareto_posterior)\nggplot(sims_theta) +\n  geom_histogram(aes(x = theta, fill = dist),\n                 bins = 70, alpha = 0.5, position = \"identity\",\n                 boundary = max(muestra_loteria$numero))  +\n  xlim(0, 15000) + scale_y_sqrt() +\n  geom_rug(data = muestra_loteria, aes(x = numero))\n\n\n\n\n\n\n\n\nNótese que cortamos algunos valores de la inicial en la cola derecha: un defecto de esta distribución inicial, con una cola tan larga a la derecha, es que pone cierto peso en valores que son poco creíbles y la vuelve poco apropiada para este problema. Regresamos más adelante a este problema.\nSi obtenemos percentiles, obtenemos el intervalo\n\nf <- c(0.025, 0.5, 0.975)\nsims_theta |> group_by(dist) |>\n  summarise(cuantiles = quantile(theta, f) |> round(2), f = f) |>\n  pivot_wider(names_from = f, values_from = cuantiles)\n\n# A tibble: 2 × 4\n# Groups:   dist [2]\n  dist      `0.025` `0.5` `0.975`\n  <chr>       <dbl> <dbl>   <dbl>\n1 inicial      307.  569.   8449.\n2 posterior   5858. 6010.   6732.\n\n\nEstimamos entre 5.8 millones y 6.7 millones de boletos. El máximo en la muestra es de\n\nmax(muestra_loteria$numero)\n\n[1] 5851.982\n\n\nEscoger la distribución pareto como inicial es conveniente y nos permitió resolver el problema sin dificultad, pero por su forma vemos que no necesariamente es apropiada para el problema por lo que señalamos arriba. Nos gustaría, por ejemplo, poner una inicial como la siguiente\n\nqplot(rgamma(2000, 5, 0.001), geom=\"histogram\", bins = 20) +\n  scale_x_continuous(breaks = seq(1000, 15000, by = 2000))\n\n\n\n\n\n\n\n\nSin embargo, los cálculos no son tan simples en este caso, pues la posterior no tiene un forma reconocible. Tendremos que usar otras estrategias de simulación para ejemplos como este (Monte Carlo por medio de Cadenas de Markov, que veremos más adelante)."
  },
  {
    "objectID": "intro-bayesiana-1.html#probabilidad-a-priori",
    "href": "intro-bayesiana-1.html#probabilidad-a-priori",
    "title": "7  Introducción a inferencia bayesiana",
    "section": "Probabilidad a priori",
    "text": "Probabilidad a priori\nLa inferencia bayesiana es conceptualmente simple: siempre hay que calcular la posterior a partir de verosimilitud (modelo de datos) y distribución inicial o a priori. Sin embargo, una crítica usual que se hace de la inferencia bayesiana es precisamente que hay que tener esa información inicial, y que distintos analistas llegan a distintos resultados si tienen información inicial distinta.\nEso realmente no es un defecto, es una ventaja de la inferencia bayesiana. Los datos y los problemas que queremos resolver no viven en un vacío donde podemos creer que la estatura de las personas, por ejemplo, puede variar de 0 a mil kilómetros, el número de boletos de una lotería puede ir de 2 o 3 boletos o también quizá 500 millones de boletos, o la proporción de personas infectadas de una enfermedad puede ser de unos cuantos hasta miles de millones.\n\nEn todos estos casos tenemos cierta información inicial que podemos usar para informar nuestras estimaciones. Esta información debe usarse.\nAntes de tener datos, las probabilidades iniciales deben ser examinadas en términos del conocimiento de expertos.\nLas probabilidades iniciales son supuestos que hacemos acerca del problema de interés, y también están sujetas a críticas y confrontación con datos.\nPoner iniciales “no informativas” en todos los parámetros no necesariamente es buena idea. En la siguiente gráfica mostramos dos distribuciones iniciales para porcentaje de votos en modelos bayesianos para el conteo rápido. El de la derecha no usa iniciales informativas en los parámetros, lo que resulta en valores informativos para las cantidades que al final queremos estimar:\n\n\n\n\n\n\n\n\n\n\n\nEjemplo\nSupongamos que queremos estimar la estatura de los cantantes de tesitura tenor con una muestra iid de tenores de Estados Unidos. Usaremos el modelo normal de forma que \\(X_i\\sim \\mathsf{N}(\\mu, \\sigma^2)\\).\nUna vez decidido el modelo, tenemos que poner distribución inicial para los parámetros \\((\\mu, \\sigma^2)\\).\nComenzamos con \\(\\sigma^2\\). Checando fuentes oficiales, en la población general la desviación estándar es alrededor de 7 centímetros. Puede ser que la variabilidad de los tenores sea algo menor o mayor. Tenemos varias opciones para la distribución inicial. En este ejemplo, usaremos una gamma.\nSimulamos para calcular cuantiles probando con distintas iniciales. Esperamos que esta información sea consiste con el conocimiento que tenemos. Por ejemplo, una inicial como esta no tiene sentido:\n\nsigma <- rgamma(10000, 0.65, 1/20)\nquantile(sigma, c(0.05, 0.5, 0.95))\n\n        5%        50%        95% \n 0.1508418  7.4707260 46.0530573 \n\n\nporque estamos diciendo que es posible que los tenores varíen hasta en 2 *50 = 100 centímetros de estatura como población alrededor de su media. Eso es físicamente imposible.\n\nsigma_inicial <- rgamma(10000, 2, 1/4)\nquantile(sigma_inicial, c(0.05, 0.5, 0.95))\n\n       5%       50%       95% \n 1.424524  6.668293 18.786815 \n\n\nEsta distribución inicial es más razonable. Quizá puede haber el doble de variación que en población, y dudamos de que sean tan uniformes como para tener menos de 1cm de desviación estándar. Un histograma de la distribución inicial, que es informativa y consistente con lo que sabemos de los humanos en cuanto a su estatura:\n\nqplot(x = sigma_inicial, geom = \"histogram\")\n\n\n\n\n\n\n\n\nComenzamos con \\(\\mu\\). Sabemos, por ejemplo, que con alta probabilidad la media debe ser algún número entre 1.60 y 1.80. Podemos investigar: la media nacional en estados unidos está alrededor de 1.75, y el percentil 90% es 1.82. Esto es variabilidad en la población: debe ser muy poco probable, por ejemplo, que la media de tenores sea 1.82 Quizá los cantantes tienden a ser un poco más altos o bajos que la población general, así que podríamos agregar algo de dispersión.\nPodemos establecer parámetros y simular de la marginal a partir de las fórmulas de arriba para entender cómo se ve la inicial de \\(\\mu\\):\n\nmu_0 <- 175 # valor medio de inicial\ns_0 <- 5 # cuánta concentración en la inicial\nmu <- rnorm(1000, mu_0, s_0)\nquantile(mu, c(0.05, 0.5, 0.95))\n\n      5%      50%      95% \n166.5934 174.9179 183.3338 \n\n\nQue consideramos un rango en el que con alta probabilidad debe estar la media poblacional de los cantantes tenores.\nPodemos checar nuestros supuestos simulando posibles muestras usando sólo nuestra información previa:\n\nsimular_modelo_inicial <- function(n, pars){\n  mu_0 <- pars[1]\n  s_0 <- pars[2]\n  a_sigma <- pars[3]\n  b_sigma <- pars[4]\n  # simular media\n  sigma <- rgamma(1, a_sigma, b_sigma)\n  mu <- rnorm(1, mu_0, s_0)\n  # simular sigma\n  rnorm(n, mu, sigma)\n}\nsimular_parametros <- function(n, pars){\n  mu_0 <- pars[1]\n  s_0 <- pars[2]\n  a_sigma <- pars[3]\n  b_sigma <- pars[4]\n  # simular media\n  tibble(\n    mu = rnorm(n, mu_0, s_0),\n    sigma = rgamma(n, a_sigma, b_sigma)\n  )\n}\nset.seed(34161)\nm_0 <- 175\nsigma_0 <- 5\na_sigma <- 2\nb_sigma <- 1/4\nsims_tbl <- tibble(rep = 1:20) |>\n  mutate(estatura = map(rep, ~ simular_modelo_inicial(500, \n    c(mu_0, sigma_0, a_sigma, b_sigma)))) |>\n  unnest(cols = c(estatura))\nggplot(sims_tbl, aes(x = estatura)) + geom_histogram() +\n  facet_wrap(~ rep) +\n  geom_vline(xintercept = c(150, 180), colour = \"red\") + theme_light()\n\n\n\n\n\n\n\n\nPusimos líneas de referencia en 150 y 180. Vemos que nuestras iniciales no producen simulaciones totalmente fuera del contexto, y parecen cubrir apropiadamente el espacio de posiblidades para estaturas de los tenores. Quizá hay algunas realizaciones poco creíbles, pero no extremadamente. En este punto, podemos regresar y ajustar la inicial para \\(\\sigma\\), que parece tomar valores demasiado grandes (produciendo por ejemplo una simulación con estatura de 220 y 120, que deberían ser menos probables).\n\nEste paso se llama el chequeo predictivo a priori. Estamos probando que el modelo y la inicial produzca valores consistentes con el conocimiento de dominio. Este análisis puede hacerse más detallado mostrando valores de poblaciones relacionadas o datos relevantes.\n\nUna vez que estamos satisfechos con le modelo inicial, podemos considerar los datos para producir las distribuciones posteriores. En este caso, nuestra muestra es:\n\nset.seed(3413)\ncantantes <- lattice::singer |>\n  mutate(estatura_cm = round(2.54 * height)) |>\n  filter(str_detect(voice.part, \"Tenor\")) |>\n  sample_n(20)\ncantantes\n\n    height voice.part estatura_cm\n139     70    Tenor 1         178\n150     68    Tenor 2         173\n140     65    Tenor 1         165\n132     66    Tenor 1         168\n152     69    Tenor 2         175\n141     72    Tenor 1         183\n161     71    Tenor 2         180\n156     71    Tenor 2         180\n158     71    Tenor 2         180\n164     69    Tenor 2         175\n147     68    Tenor 1         173\n130     72    Tenor 1         183\n162     71    Tenor 2         180\n134     74    Tenor 1         188\n170     69    Tenor 2         175\n167     68    Tenor 2         173\n149     64    Tenor 1         163\n143     68    Tenor 1         173\n157     69    Tenor 2         175\n153     71    Tenor 2         180\n\n\n\nEn este caso usaremos métodos numéricos para simular de la posterior (métodos MCMC, o Markov Chain Monte Carlo).\n\nNuestro modelo en Stan se escribe como sigue, según nuestra discusión de arriba:\n\nlibrary(cmdstanr)\nmod_estaturas <- cmdstan_model(\"cantantes.stan\")\nprint(mod_estaturas)\n\ndata {\n  int<lower=0> N;\n  vector[N] estaturas;\n}\n\nparameters {\n  real mu;\n  real<lower=0> sigma;\n}\n\nmodel {\n  // verosimilitud\n  estaturas ~ normal(mu, sigma);\n  // iniciales\n  mu ~ normal(175, 5);\n  sigma ~ gamma(2.0, 0.5);\n}\n\ngenerated quantities {\n  vector[N] estaturas_sim;\n  // generar una muestra bajo el modelo ajustado\n  for(i in 1:N){\n    estaturas_sim[i] = normal_rng(mu, sigma);\n  }\n}\n\n\n¿Cómo se ve nuestra posterior comparada con la inicial? Podemos hacer simulaciones:\n\ny <- cantantes$estatura_cm\nN <- length(y)\ndatos_lista <- list(N = N, estaturas = y)\najuste <- mod_estaturas$sample(data = datos_lista, refresh = 1000)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.0 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 0.0 seconds.\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 0.0 seconds.\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.5 seconds.\n\najuste$summary(c(\"mu\", \"sigma\"))   \n\n# A tibble: 2 × 10\n  variable   mean median    sd   mad     q5    q95  rhat ess_bulk ess_tail\n  <chr>     <dbl>  <dbl> <dbl> <dbl>  <dbl>  <dbl> <dbl>    <dbl>    <dbl>\n1 mu       176.   176.   1.30  1.28  174.   178.    1.00    2951.    2490.\n2 sigma      6.15   6.05 0.976 0.914   4.79   7.88  1.00    2550.    2386.\n\n\nY vemos que nuestra posterior es consistente con la información inicial que usamos, hemos aprendido considerablemente de la muestra. La posterior se ve como sigue. Hemos marcado también las medias posteriores de cada parámetro: media y desviación estándar.\n\nsims_post <- ajuste$draws(c(\"mu\", \"sigma\"), format = \"df\")\nhead(sims_post)\n\n# A draws_df: 6 iterations, 1 chains, and 2 variables\n   mu sigma\n1 177   4.7\n2 174   7.8\n3 176   5.5\n4 176   5.4\n5 176   6.1\n6 172   7.9\n# ... hidden reserved variables {'.chain', '.iteration', '.draw'}\n\n\n\nsims_inicial <- \n  simular_parametros(4000, c(mu_0, sigma_0, a_sigma, b_sigma))\nsims <- bind_rows(sims_inicial |> mutate(tipo = \"inicial\"),\n                  sims_post |> mutate(tipo = \"posterior\"))\nggplot(sims, aes(x = mu, y = sigma, colour = tipo)) +\n  geom_point() + coord_equal()\n\n\n\n\n\n\n\n\nPodemos construir intervalos creíbles del 90% para estos dos parámetros, por ejemplo haciendo intervalos de percentiles\n\nf <- c(0.05, 0.5, 0.95)\nsims |>\n  pivot_longer(cols = mu:sigma, names_to = \"parametro\") |>\n  group_by(tipo, parametro) |>\n  summarise(cuantil = quantile(value, f) |> round(1), f= f) |>\n  pivot_wider(names_from = f, values_from = cuantil)\n\n# A tibble: 4 × 5\n# Groups:   tipo, parametro [4]\n  tipo      parametro `0.05` `0.5` `0.95`\n  <chr>     <chr>      <dbl> <dbl>  <dbl>\n1 inicial   mu         167   175.   183. \n2 inicial   sigma        1.4   6.7   19.4\n3 posterior mu         174.  176.   178. \n4 posterior sigma        4.8   6.1    7.9\n\n\nComo comparación, los estimadores de máxima verosimlitud son\n\nmedia_mv <- mean(cantantes$estatura_cm)\nsigma_mv <- mean((cantantes$estatura_cm - media_mv)^2) |> sqrt()\nc(media_mv, sigma_mv)\n\n[1] 176   6\n\n\nAhora solo resta checar que el modelo es razonable. Veremos más adelante cómo hacer esto, usando la distribución predictiva posterior."
  },
  {
    "objectID": "intro-bayesiana-1.html#pasos-de-un-análisis-de-datos-bayesiano",
    "href": "intro-bayesiana-1.html#pasos-de-un-análisis-de-datos-bayesiano",
    "title": "7  Introducción a inferencia bayesiana",
    "section": "Pasos de un análisis de datos bayesiano",
    "text": "Pasos de un análisis de datos bayesiano\n\n\n\nComo vimos en los ejemplos, en general un análisis de datos bayesiano sigue los siguientes pasos: - Identificar los datos releventes a nuestra pregunta de investigación, el tipo de datos que vamos a describir, que variables queremos estimar. - Definir el modelo descriptivo para los datos. La forma matemática y los parámetros deben ser apropiados para los objetivos del análisis. - Especificar la distribución inicial de los parámetros. Verificar que el modelo inicial produce datos consistentes con el conocimiento de dominio. - Utilizar inferencia bayesiana para reubicar la credibilidad a lo largo de los posibles valores de los parámetros. - Verificar que la distribución posterior replique los datos de manera razonable, de no ser el caso considerar otros modelos descriptivos para los datos."
  },
  {
    "objectID": "intro-bayesiana-1.html#verificación-predictiva-posterior",
    "href": "intro-bayesiana-1.html#verificación-predictiva-posterior",
    "title": "7  Introducción a inferencia bayesiana",
    "section": "Verificación predictiva posterior",
    "text": "Verificación predictiva posterior\nUna vez que ajustamos un modelo bayesiano, podemos simular nuevas observaciones a partir del modelo. Esto tiene dos utilidades:\n\nHacer predicciones acerca de datos no observados.\nConfirmar que nuevas producidas simuladas con el modelo son similares a las que de hecho observamos. Esto nos permite confirmar la calidad del ajuste del modelo, y se llama verificación predictiva posterior.\n\nSupongamos que tenemos la posterior \\(p(\\theta | x)\\). Podemos generar una nueva replicación de los datos como sigue:\n\n\n\nLa distribución predictiva posterior genera nuevas observaciones a partir de la información observada. La denotamos como (p(|x)). Para simular de ella: - Muestreamos un valor () de la posterior (p(|x)). - Simulamos del modelo de las observaciones ( p(|)). - Repetimos el proceso hasta obtener una muestra grande. - Usamos este método para producir, por ejemplo, intervalos de predicción para nuevos datos. Si queremos una replicación de las observaciones de la predictiva posterior, - Muestreamos un valor () de la posterior (p(|x)). - Simulamos del modelo de las observaciones (_1, _2,, _n p(|)), done (n) es el tamaño de muestra de la muestra original (x). - Usamos este método para producir conjuntos de datos simulados que comparamos con los observados para verificar nuestro modelo.\n\n\n\n\nEjemplo: estaturas de tenores\nEn este ejemplo, usaremos la posterior predictiva para checar nuestro modelo. Vamos a crear varias muestras, del mismo tamaño que la original, según nuestra predictiva posterior, y compararemos estas muestras con la observada.\n\nset.seed(82223)\nmuestras_sim <- ajuste$draws(\"estaturas_sim\", format = \"df\") |> \n  as_tibble() |> \n  pivot_longer(contains(\"estaturas_sim\"), names_to = \"variable\", \n               values_to = \"estatura\") |>\n  select(.n = .draw, estatura_cm = estatura) |> \n  filter(.n %in% sample(1:4000, 19)) |>\n  nest(c(estatura_cm)) |> \n  mutate(.n = min_rank(.n)) |> \n  unnest()\n\nPodemos simular varias muestras y hacer una prueba de lineup:\n\nlibrary(nullabor)\nset.seed(44165)\npos <- sample(1:20, 1)\nlineup_tbl <- lineup(true = cantantes |> select(estatura_cm),\n                     samples = muestras_sim, pos = pos)\nggplot(lineup_tbl, aes(x = estatura_cm)) + geom_histogram(binwidth = 2.5) +\n  facet_wrap(~.sample)\n\n\n\n\n\n\n\n\nCon este tipo de gráficas podemos checar desajustes potenciales de nuestro modelo.\n\n\n\n\n¿Puedes encontrar los datos verdaderos? ¿Cuántos seleccionaron los datos correctos?\n\n\nPrueba hacer pruebas con una gráfica de cuantiles. ¿Qué problema ves y cómo lo resolverías?\n\n\n\n\n\n\nEjemplo: modelo Poisson\nSupongamos que pensamos el modelo para las observaciones es Poisson con parámetro \\(\\lambda\\). Pondremos como inicial para \\(\\lambda\\) una exponencial con media 10.\nNótese que la posterior está dada por\n\\[p(\\lambda|x_1,\\ldots, x_n) \\propto e^{-n\\lambda}\\lambda^{\\sum_i x_i} e^{-0.1\\lambda} = \\lambda^{n\\overline{x}}e^{-\\lambda(n + 0.1)}\\]\nque es una distribución gamma con parámetros \\((n\\overline{x} + 1, n+0.1)\\)\nAhora supongamos que observamos la siguiente muestra, ajustamos nuestro modelo y hacemos replicaciones posteriores de los datos observados:\n\nx <- rnbinom(250, mu = 20, size = 3)\ncrear_sim_rep <- function(x){\n  n <- length(x)\n  suma <- sum(x)\n  sim_rep <- function(rep){\n    lambda <- rgamma(1, sum(x) + 1, n + 0.1)\n    x_rep <- rpois(n, lambda)\n    tibble(rep = rep, x_rep = x_rep)\n  }\n}\nsim_rep <- crear_sim_rep(x)\nlineup_tbl <- map(1:5, ~ sim_rep(.x)) |>\n  bind_rows() |>\n  bind_rows(tibble(rep = 6, x_rep = x))\nggplot(lineup_tbl, aes(x = x_rep)) +\n  geom_histogram(bins = 15) +\n  facet_wrap(~rep)\n\n\n\n\n\n\n\n\nY vemos claramente que nuestro modelo no explica apropiadamente la variación de los datos observados. Contrasta con:\n\nset.seed(223)\nx <- rpois(250, 15)\ncrear_sim_rep <- function(x){\n  n <- length(x)\n  suma <- sum(x)\n  sim_rep <- function(rep){\n    lambda <- rgamma(1, sum(x) + 1, n + 0.1)\n    x_rep <- rpois(n, lambda)\n    tibble(rep = rep, x_rep = x_rep)\n  }\n}\nsim_rep <- crear_sim_rep(x)\nlineup_tbl <- map(1:5, ~ sim_rep(.x)) |>\n  bind_rows() |>\n  bind_rows(tibble(rep = 6, x_rep = x))\nggplot(lineup_tbl, aes(x = x_rep)) +\n  geom_histogram(bins = 15) +\n  facet_wrap(~rep)\n\n\n\n\n\n\n\n\nY verificamos que en este caso el ajuste del modelo es apropiado."
  },
  {
    "objectID": "intro-bayesiana-1.html#predicción",
    "href": "intro-bayesiana-1.html#predicción",
    "title": "7  Introducción a inferencia bayesiana",
    "section": "Predicción",
    "text": "Predicción\nCuando queremos hacer predicciones particulares acerca de datos que observemos en el futuro, también podemos usar la posterior predictiva.\n\nEn el caso de los cantantes, lo usamos para replicar nuevas muestras y checar el desempeño del modelo.\n\n\nEjemplo: posterior predictiva de Pareto-Uniforme.\nLa posterior predictiva del modelo Pareto-Uniforme no tiene un nombre estándar, pero podemos aproximarla usando simulación. Usando los mismos datos del ejercicio de la lotería, haríamos:\n\nrpareto <- function(n, theta_0, alpha){\n  # usar el método de inverso de distribución acumulada\n  u <- runif(n, 0, 1)\n  theta_0 / (1 - u)^(1/alpha)\n}\n# Simulamos de la posterior de los parámetros\nlim_inf_post <- max(c(300, muestra_loteria$numero))\nk_posterior <- nrow(muestra_loteria) + 1.1\nsims_pareto_posterior <- tibble(\n  theta = rpareto(100000, lim_inf_post, k_posterior))\n# Simulamos una observación para cada una de las anteriores:\nsims_post_pred <- sims_pareto_posterior |>\n  mutate(x_pred = map_dbl(theta, ~ runif(1, 0, .x)))\n# Graficamos\nggplot(sims_post_pred, aes(x = x_pred)) +\n  geom_histogram(binwidth = 50) +\n  geom_vline(xintercept = lim_inf_post, colour = \"red\")\n\n\n\n\n\n\n\n\nQue es una mezcla de una uniforme con una Pareto."
  },
  {
    "objectID": "intro-bayesiana-2.html",
    "href": "intro-bayesiana-2.html",
    "title": "8  Calibración bayesiana y Regularización",
    "section": "",
    "text": "El enfoque bayesiano se puede formalizar coherentemente en términos de probabilidades subjetivas, y como vimos, esta es una fortaleza del enfoque bayesiano.\nEn la práctica, sin embargo, muchas veces puede ser difícil argumentar en términos exclusivos de probabilidad subjetiva, aunque hagamos los esfuerzos apropiados para incorporar la totalidad de información que distintos actores involucrados pueden tener.\nConsideremos, por ejemplo, que INEGI produjera un intervalo creíble del 95% para el ingreso mediano de los hogares de México. Aún cuando nuestra metodología sea transparente y correctamente informada, algunos investigadores interesados puede ser que tengan recelo en usar esta información, y quizá preferirían hacer estimaciones propias. Esto restaría valor al trabajo cuidadoso que pusimos en nuestras estimaciones oficiales.\nPor otra parte, el enfoque frecuentista provee de ciertas garantías mínimas para la utilización de las estimaciones, que no dependen de la interpretación subjetiva de la probabilidad, sino de las propiedades del muestreo. Consideremos la cobertura de los intervalos de confianza:\nLos intervalos creíbles en principio no tienen por qué cumplir esta propiedad, pero consideramos que en la práctica es una garantía mínima que deberían cumplir.\nEl enfoque resultante se llama bayesiano calibrado, Little (2011) . La idea es seguir el enfoque bayesiano usual para construir nuestras estimaciones, pero verificar hasta donde sea posible que los intervalos resultantes satisfacen alguna garantía frecuentista básica.\nObservación. checar que la cobertura real es similar a la nominal es importante en los dos enfoques: frecuentista y bayesiano. Los intervalos frecuentistas, como hemos visto, generalmente son aproximados, y por lo tanto no cumplen automáticamente esta propiedad de calibración."
  },
  {
    "objectID": "intro-bayesiana-2.html#enfoque-bayesiano-y-frecuentista",
    "href": "intro-bayesiana-2.html#enfoque-bayesiano-y-frecuentista",
    "title": "8  Calibración bayesiana y Regularización",
    "section": "Enfoque bayesiano y frecuentista",
    "text": "Enfoque bayesiano y frecuentista\nLos métodos estadísticos clásicos toman el punto de vista frecuentista y se basa en los siguientes puntos (Wasserman (2013)):\n\nLa probabilidad se interpreta como un límite de frecuencias relativas, donde las probabilidades son propiedades objetivas en el mundo real.\nEn un modelo, los parámetros son constantes fijas (desconocidas). Como consecuencia, no se pueden realizar afirmaciones probabilísticas útiles en relación a éstos.\nLos procedimientos estadísticos deben diseñarse con el objetivo de tener propiedades frecuentistas bien definidas. Por ejemplo, un intervalo de confianza del \\(95\\%\\) debe contener el verdadero valor del parámetro con frecuencia límite de al menos el \\(95\\%\\).\n\nEn contraste, el acercamiento Bayesiano muchas veces se describe por los siguientes postulados:\n\nLa probabilidad describe grados de creencia, no frecuencias limite. Como tal uno puede hacer afirmaciones probabilísticas acerca de muchas cosas y no solo datos sujetos a variabilidad aleatoria. Por ejemplo, puedo decir: “La probabilidad de que Einstein tomara una taza de té el primero de agosto de \\(1948\\)” es \\(0.35\\), esto no hace referencia a ninguna frecuencia relativa sino que refleja la certeza que yo tengo de que la proposición sea verdadera.\nPodemos hacer afirmaciones probabilísticas de parámetros.\nPodemos hacer inferencia de un parámetro \\(\\theta\\) por medio de distribuciones de probabilidad. Las inferencias como estimaciones puntuales y estimaciones de intervalos se pueden extraer de dicha distribución.\n\nFinalmente, en el enfoque bayesiano calibrado (Little (2011)):\n\nUsamos el enfoque bayesiano para modelar y hacer afirmaciones probabilísticas de los parámetros.\nBuscamos cumplir las garantías frecuentistas del inciso 3)."
  },
  {
    "objectID": "intro-bayesiana-2.html#ejemplo-estimación-de-una-proporción",
    "href": "intro-bayesiana-2.html#ejemplo-estimación-de-una-proporción",
    "title": "8  Calibración bayesiana y Regularización",
    "section": "Ejemplo: estimación de una proporción",
    "text": "Ejemplo: estimación de una proporción\nRecordamos nuestro problema de estimación de una proporcion \\(\\theta\\). Usando la distribución inicial \\(p(\\theta)\\sim \\mathsf{Beta}(2,2)\\), y la verosimilitud estándar binomial, vimos que la posterior cuando observamos \\(k\\) éxitos es \\[p(\\theta|k) \\sim \\mathsf{Beta}(k + 2, n - k + 2)\\].\nLa media posterior es\n\\[\\frac{k + 2}{n + 4} \\]\nque podemos interpretar como: agrega 2 éxitos y 2 fracasos a los datos observados y calcula la proporción de éxitos. Un intervalo posterior de credibilidad del 95% se calcula encontrando los cuantiles 0.025 y 0.975 de una \\(\\mathsf{Beta}(k + 2, n - k + 2)\\)\n\\[I_a = \\left [q_{0.025}(k+2, n+4), q_{0.975}(k+2, n+4)\\right ]\\]\nQue compararemos con el intervalo usual de Wald: si \\(\\hat{\\theta} = \\frac{k}{n}\\), entonces\n\\[I_w = \\left [\\hat{\\theta} - 2 \\sqrt{\\frac{\\hat{\\theta}(1-\\hat{\\theta})}{n}}, \\hat{\\theta} + 2 \\sqrt{\\frac{\\hat{\\theta}(1-\\hat{\\theta})}{n}}\\right]\\]\n¿Cómo podemos comparar la calibración de estos dos intervalos? Nominalmente, deben tener cobertura de 95%. Hagamos un ejercicio de simulación para distintos tamaños de muestra \\(n\\) y posibles valores \\(\\theta\\in (0,1)\\):\n\nset.seed(332)\nsimular_muestras <- function(M, n, p){\n  k = rbinom(M, n, p)\n  tibble(rep = 1:M, n = n, p = p, k = k)\n}\nintervalo_wald <- function(n, k){\n  p_hat <- k / n\n  ee_hat <- sqrt(p_hat * (1 - p_hat) / n)\n  tibble(inf = p_hat - 2 * ee_hat, sup = p_hat + 2 * ee_hat)\n}\nintervalo_bayes <- function(n, k, a = 2, b = 2){\n  a <- k + a\n  b <- n - k + b\n  tibble(inf = qbeta(0.025, a, b), sup = qbeta(0.975, a, b))\n}\nset.seed(812)\nejemplo <- simular_muestras(5, 20, 0.4)\n\n\nejemplo %>% mutate(intervalo = intervalo_wald(n, k)) %>% pull(intervalo) %>% \n  bind_cols(ejemplo) %>% select(-rep)\n\n# A tibble: 5 × 5\n     inf   sup     n     p     k\n   <dbl> <dbl> <dbl> <dbl> <int>\n1 0.0211 0.379    20   0.4     4\n2 0.228  0.672    20   0.4     9\n3 0.276  0.724    20   0.4    10\n4 0.228  0.672    20   0.4     9\n5 0.137  0.563    20   0.4     7\n\n\n\nejemplo %>% mutate(intervalo = intervalo_bayes(n, k)) %>% pull(intervalo) %>% \n  bind_cols(ejemplo) %>% select(-rep)\n\n# A tibble: 5 × 5\n    inf   sup     n     p     k\n  <dbl> <dbl> <dbl> <dbl> <int>\n1 0.102 0.437    20   0.4     4\n2 0.268 0.655    20   0.4     9\n3 0.306 0.694    20   0.4    10\n4 0.268 0.655    20   0.4     9\n5 0.197 0.573    20   0.4     7\n\n\n¿Cuáles de estos intervalos cubren al verdadero valor? Nótese que no podemos descalificar a ningún método por no cubrir una vez. Es fácil producir un intervalo con 100% de cobertura: (0,1). Pero no nos informa dónde es probable que esté el parámetro.\nSin embargo, podemos checar la cobertura frecuentista haciendo una cantidad grande de simulaciones:\n\nparametros <- crossing(n = c(5, 10, 30, 60, 100, 400), \n                       p = c(0.01, 0.015, 0.02, 0.025, 0.03, 0.035, 0.04, 0.05, 0.07, 0.1, 0.15))\nset.seed(2343)\n# simulaciones\nsimulaciones <- parametros %>% \n  mutate(muestra = map2(n, p, ~ simular_muestras(50000, .x, .y) %>% select(rep, k))) %>% \n  unnest(muestra)\n# calcular_cobertura\ncalcular_cobertura <- function(simulaciones, construir_intervalo){\n  # nombre de función\n  intervalo_nombre <- substitute(construir_intervalo) %>% as.character()\n  cobertura_tbl <- simulaciones %>% \n    mutate(intervalo  = construir_intervalo(n, k)) %>%\n    pull(intervalo) %>% \n    bind_cols(simulaciones) %>% \n    mutate(cubre = p >= inf & p <= sup) %>% \n    group_by(n, p) %>% \n    summarise(cobertura = mean(cubre), long_media = mean(sup - inf))\n  cobertura_tbl %>% mutate(tipo = intervalo_nombre)\n}\n\n\ncobertura_wald <- calcular_cobertura(simulaciones, intervalo_wald)\ncobertura_wald\n\n# A tibble: 66 × 5\n# Groups:   n [6]\n       n     p cobertura long_media tipo          \n   <dbl> <dbl>     <dbl>      <dbl> <chr>         \n 1     5 0.01     0.0483     0.0347 intervalo_wald\n 2     5 0.015    0.0733     0.0527 intervalo_wald\n 3     5 0.02     0.0954     0.0689 intervalo_wald\n 4     5 0.025    0.119      0.0862 intervalo_wald\n 5     5 0.03     0.140      0.102  intervalo_wald\n 6     5 0.035    0.165      0.120  intervalo_wald\n 7     5 0.04     0.187      0.137  intervalo_wald\n 8     5 0.05     0.227      0.167  intervalo_wald\n 9     5 0.07     0.299      0.223  intervalo_wald\n10     5 0.1      0.398      0.303  intervalo_wald\n# … with 56 more rows\n\n\n\ngraficar_cobertura <- function(cobertura_tbl){\n  ggplot(cobertura_tbl, aes(x = p, y = cobertura, colour = tipo)) +\n  geom_hline(yintercept = 0.95, colour = \"black\") +\n  geom_line() + geom_point() +\n  facet_wrap(~n) +\n  ylim(0, 1) \n}\ncobertura_wald %>% \n  graficar_cobertura()\n\n\n\n\n\n\n\n\nLa cobertura real es mucho más baja que la nominal en muchos casos, especialmente cuando la \\(p\\) es baja y \\(n\\) es chica. Pero incluso para muestras relativamente grandes (100), la cobertura es mala si \\(p\\) es chica.\nAhora probamos nuestro método alternativo:\n\ncobertura_bayes <- calcular_cobertura(simulaciones, intervalo_bayes)\n\n\nbind_rows(cobertura_wald, cobertura_bayes) %>% \n  mutate(tipo = factor(tipo, levels = c('intervalo_wald', 'intervalo_bayes'))) %>% \n  graficar_cobertura()\n\n\n\n\n\n\n\n\nY vemos que en general el intervalo de Bayes es superior al de Wald, en sentido de que su cobertura real es más cercana a la nominal. El caso donde fallan los dos es para muestras muy chicas \\(n=5, 10\\), con probabilidades de éxito chicas \\(p\\leq 0.02\\).\n\nSin embargo, si tenemos información previa acerca del tamaño de la proporción que estamos estimando, es posible obtener buena calibración con el método bayesiano.\n\nEn este caso particular, tenemos argumentos frecuentistas para utilizar el método bayesiano. Por ejemplo, si el INEGI utilizara estos intervalos creíbles, un análisis de calibración de este tipo sostendría esa decisión."
  },
  {
    "objectID": "intro-bayesiana-2.html#intervalos-de-agresti-coull",
    "href": "intro-bayesiana-2.html#intervalos-de-agresti-coull",
    "title": "8  Calibración bayesiana y Regularización",
    "section": "Intervalos de Agresti-Coull",
    "text": "Intervalos de Agresti-Coull\nUn método intermedio que se usa para obtener mejores intervalos cuando estimamos proporciones es el siguiente:\n\nAgregar dos 1’s y dos 0’s a los datos.\nUtilizar el método de Wald con estos datos modificados.\n\n\nintervalo_agresti_coull <- function(n, k){\n  p_hat <- (k + 2)/ (n + 4)\n  ee_hat <- sqrt(p_hat * (1 - p_hat) / n)\n  tibble(inf = p_hat - 2 * ee_hat, sup = p_hat + 2 * ee_hat)\n}\ncobertura_ac <- calcular_cobertura(simulaciones, intervalo_agresti_coull)\n\n\nbind_rows(cobertura_wald, cobertura_bayes, cobertura_ac) %>% \n  mutate(tipo = factor(tipo, levels = c('intervalo_wald', 'intervalo_bayes', 'intervalo_agresti_coull'))) %>% \n  graficar_cobertura()\n\n\n\n\n\n\n\n\nQue tiende a ser demasiado conservador para proporciones chicas:\n\ngraficar_cobertura(cobertura_ac) +\n  ylim(c(0.9, 1))\n\n\n\n\n\n\n\n\nConclusión 1: Los intervalos de Agresti-Coull son una buena alternativa para estimar proporciones como sustituto de los intervalos clásicos de Wald, aunque tienden a ser muy conservadores para muestras chicas\nIdealmente podemos utilizar un método bayesiano pues normalmente tenemos información inicial acerca de las proporciones que queremos estimar."
  },
  {
    "objectID": "intro-bayesiana-2.html#incorporando-información-inicial",
    "href": "intro-bayesiana-2.html#incorporando-información-inicial",
    "title": "8  Calibración bayesiana y Regularización",
    "section": "Incorporando información inicial",
    "text": "Incorporando información inicial\nNótese que generalmente tenemos información acerca de la cantidad que queremos estimar: por ejemplo, que proporción de visitantes de un sitio web compra algo (usualmente muy baja, menos de 2%), qué proporción de personas tiene diabetes tipo 1 (una proporción muy baja, menos de 1 por millar), o qué proporción de hogares tienen ingresos trimestrales mayores a 150 mil pesos (menos de %5 con alta probabilidad).\nEn este caso, tenemos que ajustar nuestra inicial. Por ejemplo, para el problema de ingresos, podríamos usar una \\(\\mathsf{Beta}(2, 100)\\), cuyos cuantiles son:\n\n# uno de cada 100\na <- 2\nb <- 100\nbeta_sims <- rbeta(5000, a, b)\nquantile(beta_sims, c(0.01, 0.05, 0.50, 0.90, 0.99)) %>% round(3)\n\n   1%    5%   50%   90%   99% \n0.001 0.004 0.016 0.039 0.067 \n\n\n\nqplot(beta_sims)\n\n\n\n\n\n\n\n\nVeamos cómo se ven los intervalos bayesianos producidos con esta inicial:\n\ncrear_intervalo_bayes <- function(a, b){\n  intervalo_fun <- function(n, k){\n    a_post <- k + a\n    b_post <- n - k + b\n    tibble(inf = qbeta(0.025, a_post, b_post), sup = qbeta(0.975, a_post, b_post))\n  }\n  intervalo_fun\n}\nintervalo_bayes_2 <- crear_intervalo_bayes(a, b)\n\n\ncobertura_bayes <- calcular_cobertura(simulaciones,\n                                      intervalo_bayes_2)\n\n\ngraficar_cobertura(bind_rows(cobertura_bayes, cobertura_ac) %>% filter(p < 0.05)) +\n  ylim(c(0.5, 1))\n\n\n\n\n\n\n\n\nY vemos que la calibración es similar. Notemos sin embargo que la longitud del del intervalo bayesiano es mucho menor que el de Agresti-Coull cuando la muestra es chica:\n\nggplot(bind_rows(cobertura_bayes, cobertura_ac), \n       aes(x = p, y = long_media, colour = tipo)) +\n  geom_point() + facet_wrap(~n) \n\n\n\n\n\n\n\n\nCuando la muestra es chica, los intervalos de bayes son similares a los iniciales, y mucho más cortos que los de Agresti-Coull. Para muestras intermedias (50-100) los intervalos bayesianos son más informativos que los de Agresti-Coull, con calibración similar, y representan aprendizaje por encima de lo que sabíamos en la inicial. Para muestras grandes, obtenemos resultados simililares.\nPor ejemplo:\n\nset.seed(2131)\nk <- rbinom(1, 50, 0.03)\nk\n\n[1] 4\n\nintervalo_agresti_coull(50, k) %>% round(3)\n\n# A tibble: 1 × 2\n    inf   sup\n  <dbl> <dbl>\n1 0.022   0.2\n\n\nes un intervalo muy grande que puede incluir valores negativos. En contraste, el intervalo bayesiano es:\n\nintervalo_bayes_2(50, k) %>% round(3)\n\n# A tibble: 1 × 2\n    inf   sup\n  <dbl> <dbl>\n1 0.015 0.076\n\n\nAún quitando valores negativos, los intervalos de Agresti-Coull son mucho más anchos. La aproximación bayesiana, entonces, utiliza información previa para dar un resultado considerablemente más informativo, con calibración similar a Agresti-Coull.\n¿Aprendimos algo? Comparemos la posterior con la inicial:\n\nbeta_sims_inicial <- tibble(prop = rbeta(5000, a, b), dist = \"inicial\")\nbeta_sims_posterior <- tibble(prop = rbeta(5000, a + k, b + 50), dist = \"posterior\")\nbind_rows(beta_sims_inicial, beta_sims_posterior) %>% \n  ggplot(aes(x = prop, fill = dist)) +\n    geom_histogram(alpha = 0.5, position = \"identity\") \n\n\n\n\n\n\n\n\nDonde vemos que no aprendimos mucho en este caso, pero nuestras creencias sí cambiaron en comparación con la inicial.\nConclusión 2: con el enfoque bayesiano podemos obtener intervalos informativos con calibración razonable, incluso con información inicial que no es muy precisa. Los intervalos de Agresti-Coull son poco informativos para muestras chicas y/o proporciones chicas.\n\nEjemplo: porporción de hogares de ingresos grandes\nUsaremos los datos de ENIGH como ejemplo (ignorando el diseño, pero es posible hacer todas las estimaciones correctamente) para estimar el porcentaje de hogares que tienen ingreso corriente de más de 150 mil pesos al trimestre. Suponemos que la muestra del enigh es la población, y tomaremos una muestra iid de esta población. Usamos la misma inicial que mostramos arriba, que es una Beta con parámetros\n\nc(a,b)\n\n[1]   2 100\n\n\n\nset.seed(2521)\nmuestra_enigh <- read_csv(\"datos/conjunto_de_datos_concentradohogar_enigh_2018_ns.csv\") %>% \n  select(ing_cor) %>% \n  sample_n(120) %>% \n  mutate(mas_150mil = ing_cor > 150000)\n\nUn intervalo de 95% es entonces\n\nk <- sum(muestra_enigh$mas_150mil)\nk\n\n[1] 3\n\nintervalo_bayes_2(120, sum(muestra_enigh$mas_150mil)) %>% round(3)\n\n# A tibble: 1 × 2\n    inf   sup\n  <dbl> <dbl>\n1 0.007 0.046\n\n\nLa media posterior es\n\nprop_post <- (a + k) / (120 + b)\nprop_post\n\n[1] 0.02272727\n\n\nEl estimador de máxima verosimilitud es\n\nk / 120\n\n[1] 0.025\n\n\n¿Cuál es la verdadera proporción?\n\nread_csv(\"datos/conjunto_de_datos_concentradohogar_enigh_2018_ns.csv\") %>% \n  select(ing_cor) %>% \n  mutate(mas_150mil = ing_cor > 150000) %>% \n  summarise(prop_pob = mean(mas_150mil))\n\n# A tibble: 1 × 1\n  prop_pob\n     <dbl>\n1   0.0277\n\n\nEn este caso, nuestro intervalo cubre a la proporción poblacional."
  },
  {
    "objectID": "intro-bayesiana-2.html#inferencia-bayesiana-y-regularización",
    "href": "intro-bayesiana-2.html#inferencia-bayesiana-y-regularización",
    "title": "8  Calibración bayesiana y Regularización",
    "section": "Inferencia bayesiana y regularización",
    "text": "Inferencia bayesiana y regularización\nComo hemos visto en análisis y modelos anteriores, la posterior que usamos para hacer inferencia combina aspectos de la inicial con la verosimilitud (los datos). Una manera de ver esta combinación y sus beneficios es pensando en término de regularización de estimaciones.\n\nEn las muestras hay variación. Algunas muestras particulares nos dan estimaciones de máxima verosimilitud pobres de los parámetros de interés (estimaciones ruidosas).\nCuando esas estimaciones pobres están en una zona de baja probabilidad de la inicial, la estimación posterior tiende a moverse (o encogerse) hacia las zonas de alta probabilidad de la inicial.\nEsto filtra ruido en las estimaciones.\nEl mecanismo resulta en una reducción del error cuadrático medio, mediante una reducción de la varianza de los estimadores (aunque quizá el sesgo aumente).\n\nEsta es una técnica poderosa, especialmente para problemas complejos donde tenemos pocos datos para cada parámetro. En general, excluímos resultados que no concuerdan con el conocimiento previo, y esto resulta en mayor precisión en las estimaciones."
  },
  {
    "objectID": "intro-bayesiana-2.html#ejemplo-modelo-normal-y-estaturas",
    "href": "intro-bayesiana-2.html#ejemplo-modelo-normal-y-estaturas",
    "title": "8  Calibración bayesiana y Regularización",
    "section": "Ejemplo: modelo normal y estaturas",
    "text": "Ejemplo: modelo normal y estaturas\nHaremos un experimento donde simularemos muestras de los datos de cantantes. Usaremos el modelo normal-gamma inverso que discutimos anteriormente, con la información inicial que elicitamos. ¿Cómo se compara la estimación de máxima verosimilitud con la media posterior?\n\n# inicial para media, ver sección anterior para discusión (normal)\nmu_0 <- 175\nn_0 <- 5\n# inicial para sigma^2 (gamma inversa)\na <- 3\nb <- 140\n\nPara este ejemplo chico, usaremos muestras de tamaño 5:\n\nset.seed(3413)\n# ver sección anterior para explicación de esta función\ncalcular_pars_posterior <- function(x, pars_inicial){\n  # iniciales\n  mu_0 <- pars_inicial[1]\n  n_0 <- pars_inicial[2]\n  a_0 <- pars_inicial[3]\n  b_0 <- pars_inicial[4]\n  # muestra\n  n <- length(x)\n  media <- mean(x)\n  S2 <- sum((x - media)^2)\n  # sigma post\n  a_1 <- a_0 + 0.5 * n\n  b_1 <- b_0 + 0.5 * S2 + 0.5 * (n * n_0) / (n + n_0) * (media - mu_0)^2\n  # posterior mu\n  mu_1 <- (n_0 * mu_0 + n * media) / (n + n_0)\n  n_1 <- n + n_0\n  c(mu_1, n_1, a_1, b_1)\n}\n\nY también de la sección anterior:\n\nsim_params <- function(m, pars){\n  mu_0 <- pars[1]\n  n_0 <- pars[2]\n  a <- pars[3]\n  b <- pars[4]\n  # simular sigmas\n  sims <- tibble(tau = rgamma(m, a, b)) %>% \n    mutate(sigma = 1 / sqrt(tau))\n  # simular mu\n  sims <- sims %>% mutate(mu = rnorm(m, mu_0, sigma / sqrt(n_0)))\n  sims\n}\n\n\n# simular muestras y calcular medias posteriores\nsimular_muestra <- function(rep, mu_0, n_0, a_0, b_0){\n  cantantes <- lattice::singer %>% \n    mutate(estatura_cm = 2.54 * height) %>% \n    filter(str_detect(voice.part, \"Tenor\")) %>% \n    sample_n(5, replace = FALSE)\n  pars_posterior <- calcular_pars_posterior(cantantes$estatura_cm,\n                                            c(mu_0, n_0, a_0, b_0))\n  medias_post <- \n    sim_params(1000, pars_posterior) %>% \n    summarise(across(everything(), mean)) %>% \n    select(mu, sigma)\n  media <- mean(cantantes$estatura_cm)\n  est_mv <- c(\"mu\" = media,\n              \"sigma\" = sqrt(mean((cantantes$estatura_cm - media)^2)))\n  bind_rows(medias_post, est_mv) %>% \n    mutate(rep = rep, tipo = c(\"media_post\", \"max_verosim\")) %>% \n    pivot_longer(mu:sigma, names_to = \"parametro\", values_to = \"estimador\")\n}\n\n\npoblacion <- lattice::singer %>% \n  mutate(estatura_cm = 2.54 * height) %>% \n  filter(str_detect(voice.part, \"Tenor\")) %>% \n  summarise(mu = mean(estatura_cm), sigma = sd(estatura_cm)) %>% \n  pivot_longer(mu:sigma, names_to = \"parametro\", values_to = \"valor_pob\")\n\n\nerrores <- map(1:2000, ~ simular_muestra(.x, mu_0, n_0, a, b)) %>%\n  bind_rows() %>% left_join(poblacion) %>% \n  mutate(error = (estimador - valor_pob))\nggplot(errores, aes(x = error, fill = tipo)) +\n  geom_histogram(bins = 20, position = \"identity\", alpha = 0.5) + facet_wrap(~parametro)\n\n\n\n\n\n\n\n\nVemos claramente que la estimación de la desviación estándar de nuestro modelo es claramente superior a la de máxima verosimilitud. En resumen:\n\nerrores %>% \n  group_by(tipo, parametro) %>% \n  summarise(recm = sqrt(mean(error^2)) %>% round(2)) %>% \n  arrange(parametro)\n\n# A tibble: 4 × 3\n# Groups:   tipo [2]\n  tipo        parametro  recm\n  <chr>       <chr>     <dbl>\n1 max_verosim mu         2.85\n2 media_post  mu         1.55\n3 max_verosim sigma      2.45\n4 media_post  sigma      1.04\n\n\nObtenemos una ganancia considerable en cuanto a la estimación de la desviación estandar de esta población. Los estimadores de la media superior son superiores a los de máxima verosimilitud en términos de error cuadrático medio.\nPodemos graficar las dos estimaciones, muestra a muestra, para entender cómo sucede esto:\n\nerrores %>% \n  select(-error) %>% \n  pivot_wider(names_from = tipo, values_from = estimador) %>% \n  filter(parametro == \"sigma\") %>% \nggplot(aes(x = max_verosim, y = media_post)) +\n  geom_abline(colour = \"red\") +\n  geom_hline(yintercept = sqrt(b/(a - 1)), lty = 2, color = 'black') + \n  geom_point() +\n  labs(subtitle = \"Estimación de sigma\") +\n  xlab(\"Estimador MV de sigma\") +\n  ylab(\"Media posterior de sigma\") +\n  coord_fixed() + \n  geom_segment(aes(x = 13, y = 11, xend = 13, yend = sqrt(b/(a - 1))), \n               colour='red', size=1, arrow =arrow(length = unit(0.5, \"cm\"))) + \n  geom_segment(aes(x = .5, y = 6, xend = .5, yend = sqrt(b/(a - 1))), \n               colour='red', size=1, arrow =arrow(length = unit(0.5, \"cm\")))\n\n\n\n\n\n\n\n\nNótese como estimaciones demasiado bajas o demasiada altas son contraídas hacia valores más consistentes con la inicial, lo cual resulta en menor error. El valor esperado de \\(\\sigma\\) bajo la distribución inicial se muestra como una horizontal punteada."
  },
  {
    "objectID": "intro-bayesiana-2.html#ejemplo-estimación-de-proporciones",
    "href": "intro-bayesiana-2.html#ejemplo-estimación-de-proporciones",
    "title": "8  Calibración bayesiana y Regularización",
    "section": "Ejemplo: estimación de proporciones",
    "text": "Ejemplo: estimación de proporciones\nAhora repetimos el ejercicio\n\n# inicial\na <- 2\nb <- 100\nqbeta(c(0.01, 0.99), a, b)\n\n[1] 0.001477084 0.063921446\n\n# datos\ndatos <- read_csv(\"datos/conjunto_de_datos_concentradohogar_enigh_2018_ns.csv\") %>% \n    select(ing_cor)\n# estimaciones\nobtener_estimados <- function(datos){\n  muestra_enigh <-  datos %>% \n    sample_n(120) %>% \n    mutate(mas_150mil = ing_cor > 150000)\n  k <- sum(muestra_enigh$mas_150mil)\n  tibble(k = k, est_mv = k/120, media_post = (a + k) / (120 + b), pob = 0.02769)\n}\nestimadores_sim <- map(1:200, ~obtener_estimados(datos)) %>% \n  bind_rows() \n# calculo de errores\nerror_cm <- estimadores_sim %>% \n  summarise(error_mv = sqrt(mean((est_mv - pob)^2)),\n         error_post = sqrt(mean((media_post - pob)^2)))\nerror_cm\n\n# A tibble: 1 × 2\n  error_mv error_post\n     <dbl>      <dbl>\n1   0.0147    0.00928\n\n\nPodemos ver claramente que las medias posteriores están encogidas hacia valores más chicos (donde la inicial tiene densidad alta) comparadas con las estimaciones de máxima verosimilitud:\n\nestimadores_sim_ag <- estimadores_sim %>% \n  group_by(k, est_mv, media_post) %>% \n  summarise(n = n())\nggplot(estimadores_sim_ag, aes(x = est_mv, media_post, size = n)) + geom_point() +\n  geom_abline()\n\n\n\n\n\n\n\n\n\n\n\n\nLittle, Roderick. 2011. «Calibrated Bayes, for Statistics in General, and Missing Data in Particular». Statist. Sci. 26 (2): 162-74. https://doi.org/10.1214/10-STS318.\n\n\nWasserman, Larry. 2013. All of statistics: a concise course in statistical inference. Springer Science & Business Media."
  }
]